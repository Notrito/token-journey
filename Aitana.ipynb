{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y hf-xet\n",
        "!pip install -U 'huggingface_hub[cli]'\n",
        "\n",
        "# Ahora descarga el modelo\n",
        "!huggingface-cli download gplsi/Aitana-2B-S"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RBi-6SV_uya",
        "outputId": "70a4d7e6-1ce7-4134-85fa-029233be78b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: hf-xet 1.1.10\n",
            "Uninstalling hf-xet-1.1.10:\n",
            "  Successfully uninstalled hf-xet-1.1.10\n",
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub[cli])\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.52)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2025.10.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.14)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, hf-xet, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 hf-xet-1.1.10 pfzy-0.3.4\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]Downloading 'config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/22de496f6cae5f5519436a54f325368abd138882.incomplete'\n",
            "Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/c205db342a8a7321a4147629d806caac1c70a8cb1cebf4c0f3636dec7a3452e0.incomplete'\n",
            "\n",
            "config.json: 100% 722/722 [00:00<00:00, 5.39MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/22de496f6cae5f5519436a54f325368abd138882\n",
            "Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/2161af68419ee601de3bf72f0fe1fb532cee2deabb71f2e745a78e75b2e22a1b.incomplete'\n",
            "Downloading 'tokenizer.model' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/ab94ddf46d14f0279254858d53770c5319c5129d47291ee2bada530271cb1292.incomplete'\n",
            "Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/72ecfeeb7e14d244c936169d2ed139eeae235ef1.incomplete'\n",
            "\n",
            "special_tokens_map.json: 100% 437/437 [00:00<00:00, 4.87MB/s]\n",
            "Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/1243416e65a219384cdd9100454c4c3076b37c00.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/72ecfeeb7e14d244c936169d2ed139eeae235ef1\n",
            "\n",
            "generation_config.json: 100% 224/224 [00:00<00:00, 2.35MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/1243416e65a219384cdd9100454c4c3076b37c00\n",
            "Downloading 'README.md' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/547a6c991ec1a933b8a3c17b83c378088bbe0c9d.incomplete'\n",
            "\n",
            "model.safetensors:   0% 0.00/4.51G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.model:   0% 0.00/4.81M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "tokenizer.json:   0% 0.00/19.1M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[ADownloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/69c8945cd78d9dad80b1f2e3fba734dfc0b3fe35.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 25.6kB [00:00, 16.2MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/69c8945cd78d9dad80b1f2e3fba734dfc0b3fe35\n",
            "Downloading '.gitattributes' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "README.md: 17.4kB [00:00, 11.2MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/547a6c991ec1a933b8a3c17b83c378088bbe0c9d\n",
            "\n",
            "\n",
            "\n",
            "\n",
            ".gitattributes: 1.57kB [00:00, 8.35MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:06,  1.31it/s]\n",
            "\n",
            "\n",
            "tokenizer.json: 100% 19.1M/19.1M [00:01<00:00, 12.6MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/c205db342a8a7321a4147629d806caac1c70a8cb1cebf4c0f3636dec7a3452e0\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 4.81M/4.81M [00:01<00:00, 3.15MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/ab94ddf46d14f0279254858d53770c5319c5129d47291ee2bada530271cb1292\n",
            "\n",
            "model.safetensors:   0% 746k/4.51G [00:03<5:17:35, 236kB/s]\u001b[A\n",
            "model.safetensors:   0% 3.07M/4.51G [00:04<1:44:54, 716kB/s]\u001b[A\n",
            "model.safetensors:   0% 4.66M/4.51G [00:05<1:05:16, 1.15MB/s]\u001b[A\n",
            "model.safetensors:   2% 71.7M/4.51G [00:09<06:40, 11.1MB/s]  \u001b[A\n",
            "model.safetensors:   3% 139M/4.51G [00:12<04:37, 15.7MB/s] \u001b[A\n",
            "model.safetensors:   5% 206M/4.51G [00:13<03:01, 23.7MB/s]\u001b[A\n",
            "model.safetensors:   6% 273M/4.51G [00:16<03:03, 23.1MB/s]\u001b[A\n",
            "model.safetensors:   8% 340M/4.51G [00:17<02:25, 28.7MB/s]\u001b[A\n",
            "model.safetensors:   9% 407M/4.51G [00:18<01:54, 35.8MB/s]\u001b[A\n",
            "model.safetensors:  11% 474M/4.51G [00:19<01:31, 44.0MB/s]\u001b[A\n",
            "model.safetensors:  12% 541M/4.51G [00:20<01:12, 54.4MB/s]\u001b[A\n",
            "model.safetensors:  13% 608M/4.51G [00:22<01:30, 43.0MB/s]\u001b[A\n",
            "model.safetensors:  15% 675M/4.51G [00:22<01:06, 57.9MB/s]\u001b[A\n",
            "model.safetensors:  16% 742M/4.51G [00:22<00:49, 76.2MB/s]\u001b[A\n",
            "model.safetensors:  18% 809M/4.51G [00:23<00:46, 80.1MB/s]\u001b[A\n",
            "model.safetensors:  19% 876M/4.51G [00:23<00:34, 105MB/s] \u001b[A\n",
            "model.safetensors:  21% 943M/4.51G [00:24<00:32, 109MB/s]\u001b[A\n",
            "model.safetensors:  22% 1.01G/4.51G [00:26<00:59, 59.0MB/s]\u001b[A\n",
            "model.safetensors:  24% 1.08G/4.51G [00:30<01:44, 32.7MB/s]\u001b[A\n",
            "model.safetensors:  25% 1.14G/4.51G [00:31<01:17, 43.5MB/s]\u001b[A\n",
            "model.safetensors:  27% 1.21G/4.51G [00:31<01:01, 53.8MB/s]\u001b[A\n",
            "model.safetensors:  28% 1.28G/4.51G [00:32<00:53, 60.3MB/s]\u001b[A\n",
            "model.safetensors:  30% 1.35G/4.51G [00:34<01:07, 46.7MB/s]\u001b[A\n",
            "model.safetensors:  31% 1.41G/4.51G [00:37<01:30, 34.3MB/s]\u001b[A\n",
            "model.safetensors:  33% 1.48G/4.51G [00:42<02:05, 24.1MB/s]\u001b[A\n",
            "model.safetensors:  34% 1.55G/4.51G [00:44<01:55, 25.6MB/s]\u001b[A\n",
            "model.safetensors:  36% 1.61G/4.51G [00:45<01:20, 35.8MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.68G/4.51G [00:46<01:12, 39.1MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.75G/4.51G [00:51<01:47, 25.7MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.81G/4.51G [00:57<02:27, 18.3MB/s]\u001b[A\n",
            "model.safetensors:  42% 1.88G/4.51G [00:57<01:42, 25.7MB/s]\u001b[A\n",
            "model.safetensors:  43% 1.95G/4.51G [00:58<01:17, 33.0MB/s]\u001b[A\n",
            "model.safetensors:  45% 2.02G/4.51G [01:03<01:52, 22.2MB/s]\u001b[A\n",
            "model.safetensors:  46% 2.08G/4.51G [01:09<02:22, 17.0MB/s]\u001b[A\n",
            "model.safetensors:  48% 2.15G/4.51G [01:09<01:38, 23.9MB/s]\u001b[A\n",
            "model.safetensors:  49% 2.22G/4.51G [01:13<01:47, 21.2MB/s]\u001b[A\n",
            "model.safetensors:  51% 2.28G/4.51G [01:13<01:14, 29.8MB/s]\u001b[A\n",
            "model.safetensors:  52% 2.35G/4.51G [01:20<01:58, 18.1MB/s]\u001b[A\n",
            "model.safetensors:  54% 2.42G/4.51G [01:25<02:08, 16.3MB/s]\u001b[A\n",
            "model.safetensors:  55% 2.49G/4.51G [01:30<02:10, 15.5MB/s]\u001b[A\n",
            "model.safetensors:  57% 2.55G/4.51G [01:32<01:42, 19.0MB/s]\u001b[A\n",
            "model.safetensors:  58% 2.62G/4.51G [01:38<01:58, 15.9MB/s]\u001b[A\n",
            "model.safetensors:  60% 2.69G/4.51G [01:38<01:21, 22.5MB/s]\u001b[A\n",
            "model.safetensors:  61% 2.75G/4.51G [01:44<01:42, 17.1MB/s]\u001b[A\n",
            "model.safetensors:  63% 2.82G/4.51G [01:44<01:11, 23.6MB/s]\u001b[A\n",
            "model.safetensors:  64% 2.90G/4.51G [01:53<01:41, 15.8MB/s]\u001b[A\n",
            "model.safetensors:  66% 2.97G/4.51G [01:56<01:33, 16.5MB/s]\u001b[A\n",
            "model.safetensors:  67% 3.03G/4.51G [02:02<01:42, 14.4MB/s]\u001b[A\n",
            "model.safetensors:  69% 3.10G/4.51G [02:02<01:09, 20.1MB/s]\u001b[A\n",
            "model.safetensors:  70% 3.17G/4.51G [02:08<01:22, 16.3MB/s]\u001b[A\n",
            "model.safetensors:  72% 3.23G/4.51G [02:09<00:55, 22.7MB/s]\u001b[A\n",
            "model.safetensors:  73% 3.30G/4.51G [02:13<00:58, 20.6MB/s]\u001b[A\n",
            "model.safetensors:  75% 3.37G/4.51G [02:13<00:40, 28.2MB/s]\u001b[A\n",
            "model.safetensors:  76% 3.44G/4.51G [02:14<00:30, 35.0MB/s]\u001b[A\n",
            "model.safetensors:  78% 3.50G/4.51G [02:14<00:21, 46.3MB/s]\u001b[A\n",
            "model.safetensors:  79% 3.57G/4.51G [02:16<00:20, 45.7MB/s]\u001b[A\n",
            "model.safetensors:  81% 3.64G/4.51G [02:16<00:14, 59.2MB/s]\u001b[A\n",
            "model.safetensors:  82% 3.70G/4.51G [02:19<00:20, 39.9MB/s]\u001b[A\n",
            "model.safetensors:  84% 3.77G/4.51G [02:19<00:14, 51.2MB/s]\u001b[A\n",
            "model.safetensors:  85% 3.84G/4.51G [02:20<00:10, 63.1MB/s]\u001b[A\n",
            "model.safetensors:  87% 3.91G/4.51G [02:27<00:25, 23.5MB/s]\u001b[A\n",
            "model.safetensors:  88% 3.97G/4.51G [02:27<00:16, 32.7MB/s]\u001b[A\n",
            "model.safetensors:  90% 4.04G/4.51G [02:33<00:22, 20.5MB/s]\u001b[A\n",
            "model.safetensors:  91% 4.10G/4.51G [02:39<00:24, 16.2MB/s]\u001b[A\n",
            "model.safetensors:  93% 4.17G/4.51G [02:39<00:14, 22.9MB/s]\u001b[A\n",
            "model.safetensors:  94% 4.24G/4.51G [02:40<00:09, 29.8MB/s]\u001b[A\n",
            "model.safetensors:  96% 4.31G/4.51G [02:45<00:09, 21.2MB/s]\u001b[A\n",
            "model.safetensors:  97% 4.37G/4.51G [02:45<00:04, 29.7MB/s]\u001b[A\n",
            "model.safetensors: 100% 4.51G/4.51G [02:49<00:00, 26.5MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/2161af68419ee601de3bf72f0fe1fb532cee2deabb71f2e745a78e75b2e22a1b\n",
            "Fetching 9 files: 100% 9/9 [02:50<00:00, 18.94s/it]\n",
            "/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/snapshots/6451cdb022d329cf575b2e7854f495af68d71812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcBVblstUONI",
        "outputId": "a5b9e692-811b-4f33-d7c7-92645712694c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"gplsi/Aitana-2B-S\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "vocab_size = len(tokenizer)\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = \"Termina la frase en una sola paraula: Açò es or...\"\n",
        "input_text = \"El nom correcte de la comunitat autonoma de valencia es...\"\n",
        "generation = generator(\n",
        "    input_text,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(f\"Result: {generation[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "4TIGZukiYlbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8e4014-0890-4edd-97a1-9a58a09e3245"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: El nom correcte de la comunitat autonoma de valencia es...\n",
            "\n",
            "(Aplaudiments) (S'interromp el senyal de televisió per raons tècniques)\n",
            "\n",
            "El senyor president:\n",
            "\n",
            "Moltes gràcies.València, 16 d'octubre de 2004. La Comunitat Valenciana és una comunitat que té un gran poder adquisitiu en relació amb les comunitats autònomes del seu entorn, i això ha suposat sempre una posició privilegiada a l'hora de negociar els acords interterritorials i també als mercats.\n",
            "\n",
            "En aquest sentit, la nostra autonomia, des de fa molts anys, està fent esforços perquè s'aconseguisca el major nombre possible d'acords bilaterals i multilaterals en matèria fiscal que permeten a esta comunitat millorar la seua competitivitat respecte al conjunt dels territoris de l'estat espanyol. I no només estem parlant de qüestions fiscals sinó de tot tipus de qüestions econòmiques que són fonamentals quan tenim unes característiques específiques.\n",
            "\n",
            "Per exemple, en termes econòmics, hem de tindre clar que en l'any 2053 la població valenciana serà més alta que en qualsevol altra comunitat autònoma espanyola, i això ens obligarà a plantejar-nos noves formes de gestió, noves estructures administratives, nous instruments de finançament, etcètera. Això suposa, evidentment, que necessitem recursos addicion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Método 1: Ver la clase exacta del tokenizer\n",
        "print(\"Clase del tokenizer:\")\n",
        "print(type(tokenizer).__name__)\n",
        "print()\n",
        "\n",
        "# Método 2: Ver el nombre completo con módulo\n",
        "print(\"Clase completa:\")\n",
        "print(type(tokenizer))\n",
        "print()\n",
        "\n",
        "# Método 3: Información del modelo base\n",
        "print(\"Información del modelo:\")\n",
        "print(f\"Model type: {tokenizer.model_max_length}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print()\n",
        "\n",
        "# Método 4: Ver archivos de configuración\n",
        "print(\"Nombre del modelo base:\")\n",
        "if hasattr(tokenizer, 'name_or_path'):\n",
        "    print(tokenizer.name_or_path)\n",
        "print()\n",
        "\n",
        "# Método 5: Inspeccionar atributos del tokenizer\n",
        "print(\"Atributos especiales:\")\n",
        "if hasattr(tokenizer, 'model_type'):\n",
        "    print(f\"Model type: {tokenizer.model_type}\")\n",
        "\n",
        "# Método 6: Ver toda la configuración\n",
        "print(\"\\nConfiguración completa:\")\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "Wjj_I0Z3b6KE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae370f5-4abe-4586-8523-a6c496c25868"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase del tokenizer:\n",
            "LlamaTokenizerFast\n",
            "\n",
            "Clase completa:\n",
            "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "\n",
            "Información del modelo:\n",
            "Model type: 1000000000000000019884624838656\n",
            "Vocab size: 256000\n",
            "\n",
            "Nombre del modelo base:\n",
            "gplsi/Aitana-2B-S\n",
            "\n",
            "Atributos especiales:\n",
            "\n",
            "Configuración completa:\n",
            "LlamaTokenizerFast(name_or_path='gplsi/Aitana-2B-S', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t4: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t5: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t6: AddedToken(\"<|reserved_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t7: AddedToken(\"<|reserved_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t8: AddedToken(\"<|reserved_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t9: AddedToken(\"<|reserved_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t10: AddedToken(\"<|reserved_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t11: AddedToken(\"<|reserved_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t12: AddedToken(\"<|reserved_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t13: AddedToken(\"<|reserved_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t14: AddedToken(\"<|reserved_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t15: AddedToken(\"<|reserved_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t16: AddedToken(\"<|reserved_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t17: AddedToken(\"<|reserved_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t18: AddedToken(\"<|reserved_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t19: AddedToken(\"<|reserved_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t20: AddedToken(\"<|reserved_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t21: AddedToken(\"<|reserved_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t22: AddedToken(\"<|reserved_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t23: AddedToken(\"<|reserved_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t24: AddedToken(\"<|reserved_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t25: AddedToken(\"<|reserved_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t26: AddedToken(\"<|reserved_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t27: AddedToken(\"<|reserved_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t28: AddedToken(\"<|reserved_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t29: AddedToken(\"<|reserved_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t30: AddedToken(\"<|reserved_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t31: AddedToken(\"<|reserved_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32: AddedToken(\"<|reserved_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t33: AddedToken(\"<|reserved_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t34: AddedToken(\"<|reserved_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t35: AddedToken(\"<|reserved_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t36: AddedToken(\"<|reserved_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t37: AddedToken(\"<|reserved_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t38: AddedToken(\"<|reserved_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t39: AddedToken(\"<|reserved_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t40: AddedToken(\"<|reserved_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t41: AddedToken(\"<|reserved_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t42: AddedToken(\"<|reserved_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t43: AddedToken(\"<|reserved_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t44: AddedToken(\"<|reserved_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t45: AddedToken(\"<|reserved_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t46: AddedToken(\"<|reserved_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t47: AddedToken(\"<|reserved_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t48: AddedToken(\"<|reserved_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t49: AddedToken(\"<|reserved_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50: AddedToken(\"<|reserved_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t51: AddedToken(\"<|reserved_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t52: AddedToken(\"<|reserved_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t53: AddedToken(\"<|reserved_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t54: AddedToken(\"<|reserved_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t55: AddedToken(\"<|reserved_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t56: AddedToken(\"<|reserved_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t57: AddedToken(\"<|reserved_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t58: AddedToken(\"<|reserved_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t59: AddedToken(\"<|reserved_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t60: AddedToken(\"<|reserved_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t61: AddedToken(\"<|reserved_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t62: AddedToken(\"<|reserved_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t63: AddedToken(\"<|reserved_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t64: AddedToken(\"<|reserved_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t65: AddedToken(\"<|reserved_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t66: AddedToken(\"<|reserved_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t67: AddedToken(\"<|reserved_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t68: AddedToken(\"<|reserved_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t69: AddedToken(\"<|reserved_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t70: AddedToken(\"<|reserved_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t71: AddedToken(\"<|reserved_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t72: AddedToken(\"<|reserved_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t73: AddedToken(\"<|reserved_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t74: AddedToken(\"<|reserved_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t75: AddedToken(\"<|reserved_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t76: AddedToken(\"<|reserved_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t77: AddedToken(\"<|reserved_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t78: AddedToken(\"<|reserved_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t79: AddedToken(\"<|reserved_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t80: AddedToken(\"<|reserved_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t81: AddedToken(\"<|reserved_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t82: AddedToken(\"<|reserved_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t83: AddedToken(\"<|reserved_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t84: AddedToken(\"<|reserved_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t85: AddedToken(\"<|reserved_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t86: AddedToken(\"<|reserved_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t87: AddedToken(\"<|reserved_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t88: AddedToken(\"<|reserved_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t89: AddedToken(\"<|reserved_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t90: AddedToken(\"<|reserved_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t91: AddedToken(\"<|reserved_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t92: AddedToken(\"<|reserved_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t93: AddedToken(\"<|reserved_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t94: AddedToken(\"<|reserved_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t95: AddedToken(\"<|reserved_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t96: AddedToken(\"<|reserved_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t97: AddedToken(\"<|reserved_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t98: AddedToken(\"<|reserved_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t99: AddedToken(\"<|reserved_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"<|reserved_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"<|reserved_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"<|reserved_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"<|reserved_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t104: AddedToken(\"\\r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t105: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t106: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t107: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t108: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t109: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t110: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t111: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t112: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t113: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t114: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t115: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t116: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t117: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t118: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t119: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t120: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t121: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t122: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t123: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t124: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t125: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t126: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t127: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t128: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t129: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t130: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t131: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t132: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t133: AddedToken(\"\n",
            "\n",
            "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t134: AddedToken(\"\n",
            "\n",
            "\n",
            "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, model_id: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m', t,\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "def show_token_ids(sentence: str, model_id: str):\n",
        "    \"\"\" Show the tokens\"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(t)"
      ],
      "metadata": {
        "id": "Kl6oAgAoao-j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_name = tokenizer.__class__.__name__\n",
        "print(tokenizer_name)"
      ],
      "metadata": {
        "id": "mA4Uqo9jbIeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3608cec0-d1b7-4b7e-9b19-922ad804e17c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura del modelo"
      ],
      "metadata": {
        "id": "R87mRMwmhrfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del Modelo\n",
        "Antes de empezar, verifiquemos los detalles arquitectónicos:"
      ],
      "metadata": {
        "id": "buY4dwGeRg6s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d365db13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e2e9d2-af0d-4711-e80e-9f60368f8194"
      },
      "source": [
        "# Load the model again to inspect its architecture\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=5440, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=5440, bias=False)\n",
            "          (down_proj): Linear(in_features=5440, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama la atención el tamaño del embedding:\n",
        "Embedding(256000, 2048)\n",
        "\n",
        "esto implica un vocabulario de 256000 tokens y una dimensión de 2048 (que se mantendrá a lo largo de las transformaciones en el transformer)."
      ],
      "metadata": {
        "id": "VHI6q6Whh1bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"CONFIGURACIÓN DEL MODELO AITANA-2B-S\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Número de capas: {model.config.num_hidden_layers}\")\n",
        "print(f\"Dimensión oculta: {model.config.hidden_size}\")\n",
        "print(f\"Attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"KV heads: {model.config.num_key_value_heads}\")\n",
        "print(f\"Vocabulario: {len(tokenizer)}\")\n",
        "print(f\"Intermediate size (MLP): {model.config.intermediate_size}\")\n",
        "\n",
        "# Verificar RoPE\n",
        "if hasattr(model.config, 'rope_theta'):\n",
        "    print(f\"✅ Usa RoPE con theta: {model.config.rope_theta}\")\n",
        "else:\n",
        "    print(\"❌ No usa RoPE\")\n",
        "\n",
        "print(f\"\\nParámetros totales: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdpzqpyHRoXn",
        "outputId": "f7a255c4-8d4a-4b0c-95f7-b2a287ca24b2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIGURACIÓN DEL MODELO AITANA-2B-S\n",
            "================================================================================\n",
            "Número de capas: 24\n",
            "Dimensión oculta: 2048\n",
            "Attention heads: 16\n",
            "KV heads: 16\n",
            "Vocabulario: 256000\n",
            "Intermediate size (MLP): 5440\n",
            "✅ Usa RoPE con theta: 10000.0\n",
            "\n",
            "Parámetros totales: 2,253,490,176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionamos el método forward para asegurarnos del orden correcto de las capas."
      ],
      "metadata": {
        "id": "eORW6URcnjqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir el método forward\n",
        "import inspect\n",
        "print(inspect.getsource(model.model.layers[0].__class__.forward))"
      ],
      "metadata": {
        "id": "yj6ZdhtjnJLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e28ebd-a220-45ff-e120-8fe16cf3b7f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        attention_mask: Optional[torch.Tensor] = None,\n",
            "        position_ids: Optional[torch.LongTensor] = None,\n",
            "        past_key_values: Optional[Cache] = None,\n",
            "        use_cache: Optional[bool] = False,\n",
            "        cache_position: Optional[torch.LongTensor] = None,\n",
            "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
            "        **kwargs: Unpack[TransformersKwargs],\n",
            "    ) -> torch.Tensor:\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.input_layernorm(hidden_states)\n",
            "        # Self Attention\n",
            "        hidden_states, _ = self.self_attn(\n",
            "            hidden_states=hidden_states,\n",
            "            attention_mask=attention_mask,\n",
            "            position_ids=position_ids,\n",
            "            past_key_values=past_key_values,\n",
            "            use_cache=use_cache,\n",
            "            cache_position=cache_position,\n",
            "            position_embeddings=position_embeddings,\n",
            "            **kwargs,\n",
            "        )\n",
            "        hidden_states = residual + hidden_states\n",
            "\n",
            "        # Fully Connected\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
            "        hidden_states = self.mlp(hidden_states)\n",
            "        hidden_states = residual + hidden_states\n",
            "        return hidden_states\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos (as standard in Llama models):\n",
        "\n",
        "1-Layer Normalization\n",
        "2-Self Attention\n",
        "3-Layer Normalization\n",
        "4-Multilayer Perceptron (Feed forward layer)\n"
      ],
      "metadata": {
        "id": "i9wUF3tsnysA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicio del viaje predictivo.\n",
        "Vamos a analizar por todas las transformaciones que pasa nuestro texto al meterle al modelo la frase:\n",
        "\n",
        "açò es or..."
      ],
      "metadata": {
        "id": "-9u0THzgkwn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"açò es or...\""
      ],
      "metadata": {
        "id": "nYezkiPglopZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_token_ids(text, model_id)"
      ],
      "metadata": {
        "id": "R8McFWchlP_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f3ba677-26c0-4659-9366-87384df6a16e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "1\n",
            "125647\n",
            "594\n",
            "785\n",
            "889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "de manera más visual:"
      ],
      "metadata": {
        "id": "1YER1PLbl-j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show_tokens(text, model_id)"
      ],
      "metadata": {
        "id": "wtLjBx7saxcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c20b1e-9eb8-4412-cf82-2894d647bb1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "\u001b[0;30;48;2;102;194;165m<s>\u001b[0m 1 \u001b[0;30;48;2;252;141;98maçò\u001b[0m 125647 \u001b[0;30;48;2;141;160;203mes\u001b[0m 594 \u001b[0;30;48;2;231;138;195mor\u001b[0m 785 \u001b[0;30;48;2;166;216;84m...\u001b[0m 889 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lo que le pasamos al modelo:"
      ],
      "metadata": {
        "id": "6OFRhyk4mBJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer(text).input_ids\n",
        "print(token_ids)\n",
        "print(len(token_ids))"
      ],
      "metadata": {
        "id": "RINLsjg_l6WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93f58bab-9e07-438c-bec0-7fe688cea189"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 125647, 594, 785, 889]\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver representación de grafias específicas del valenciano"
      ],
      "metadata": {
        "id": "Tewi4sGmim5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"anxova així caparaçò col·legi l'horta mascletà perquè\"\n",
        "show_tokens(text, model_id)"
      ],
      "metadata": {
        "id": "msa19ZmbaxZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e536b44-ae04-45f2-8852-ef21fe6b515b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "\u001b[0;30;48;2;102;194;165m<s>\u001b[0m 1 \u001b[0;30;48;2;252;141;98manx\u001b[0m 126722 \u001b[0;30;48;2;141;160;203mova\u001b[0m 1255 \u001b[0;30;48;2;231;138;195maixí\u001b[0m 18268 \u001b[0;30;48;2;166;216;84mcap\u001b[0m 2153 \u001b[0;30;48;2;255;217;47mara\u001b[0m 925 \u001b[0;30;48;2;102;194;165mçò\u001b[0m 14720 \u001b[0;30;48;2;252;141;98mcol\u001b[0m 1896 \u001b[0;30;48;2;141;160;203m·\u001b[0m 255740 \u001b[0;30;48;2;231;138;195mlegi\u001b[0m 39451 \u001b[0;30;48;2;166;216;84ml\u001b[0m 414 \u001b[0;30;48;2;255;217;47m'\u001b[0m 255723 \u001b[0;30;48;2;102;194;165mhorta\u001b[0m 163861 \u001b[0;30;48;2;252;141;98mmasc\u001b[0m 170285 \u001b[0;30;48;2;141;160;203mlet\u001b[0m 736 \u001b[0;30;48;2;231;138;195mà\u001b[0m 255804 \u001b[0;30;48;2;166;216;84mperquè\u001b[0m 11203 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontramos que \"·\", \"'\" y \"à\" son tokens únicos y que están al final del vocabulario muy cerca del 256000. Vamos a imprimir por curiosidad los 100 últimos tokens"
      ],
      "metadata": {
        "id": "iyRmIFPwjOV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in range(vocab_size - 100, vocab_size):\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "    print(f\"{token_id}: {repr(token_text)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i5FP2OY2axWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f4d65b9-f03c-4420-d2f7-6bb428264826"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255900: 'Ν'\n",
            "255901: '•'\n",
            "255902: 'Δ'\n",
            "255903: 'љ'\n",
            "255904: 'ń'\n",
            "255905: 'ß'\n",
            "255906: 'Ž'\n",
            "255907: 'Ф'\n",
            "255908: 'É'\n",
            "255909: 'Γ'\n",
            "255910: 'Ч'\n",
            "255911: 'Х'\n",
            "255912: 'Ρ'\n",
            "255913: 'Λ'\n",
            "255914: 'І'\n",
            "255915: 'ψ'\n",
            "255916: 'ű'\n",
            "255917: 'Υ'\n",
            "255918: 'ķ'\n",
            "255919: '‘'\n",
            "255920: 'Ú'\n",
            "255921: 'Ц'\n",
            "255922: 'ģ'\n",
            "255923: 'Я'\n",
            "255924: 'Á'\n",
            "255925: 'Ü'\n",
            "255926: 'ď'\n",
            "255927: '‑'\n",
            "255928: 'ù'\n",
            "255929: 'Β'\n",
            "255930: 'Ш'\n",
            "255931: '│'\n",
            "255932: 'ђ'\n",
            "255933: 'ï'\n",
            "255934: '€'\n",
            "255935: 'Î'\n",
            "255936: 'Ä'\n",
            "255937: 'Χ'\n",
            "255938: 'Θ'\n",
            "255939: 'ź'\n",
            "255940: 'Φ'\n",
            "255941: 'Ø'\n",
            "255942: 'Ġ'\n",
            "255943: 'Ó'\n",
            "255944: 'Í'\n",
            "255945: 'Ω'\n",
            "255946: '°'\n",
            "255947: 'ё'\n",
            "255948: 'Ö'\n",
            "255949: 'º'\n",
            "255950: 'ë'\n",
            "255951: 'Ж'\n",
            "255952: 'Э'\n",
            "255953: 'Å'\n",
            "255954: '^'\n",
            "255955: '~'\n",
            "255956: '┼'\n",
            "255957: '№'\n",
            "255958: 'ì'\n",
            "255959: 'Έ'\n",
            "255960: 'Щ'\n",
            "255961: 'ϊ'\n",
            "255962: 'Ј'\n",
            "255963: 'Ю'\n",
            "255964: '¿'\n",
            "255965: 'Ś'\n",
            "255966: 'Ż'\n",
            "255967: 'Ã'\n",
            "255968: 'È'\n",
            "255969: 'Ħ'\n",
            "255970: 'Ό'\n",
            "255971: '\\u200b'\n",
            "255972: '²'\n",
            "255973: 'Ş'\n",
            "255974: 'Й'\n",
            "255975: 'ð'\n",
            "255976: '©'\n",
            "255977: 'Ά'\n",
            "255978: 'ا'\n",
            "255979: 'Į'\n",
            "255980: 'Õ'\n",
            "255981: 'Є'\n",
            "255982: 'ŵ'\n",
            "255983: 'Ā'\n",
            "255984: 'Ζ'\n",
            "255985: 'Ċ'\n",
            "255986: 'À'\n",
            "255987: 'Ė'\n",
            "255988: 'Ξ'\n",
            "255989: 'Ř'\n",
            "255990: 'Ò'\n",
            "255991: '®'\n",
            "255992: 'œ'\n",
            "255993: 'Ł'\n",
            "255994: '→'\n",
            "255995: '´'\n",
            "255996: 'Ī'\n",
            "255997: '的'\n",
            "255998: '×'\n",
            "255999: '，'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "por tanto le pasamos al modelo la secuencia\n",
        "[1, 125647, 594, 785, 889]\n",
        "\n",
        "La primera capa es el embedding, que (consultando en una tabla de (256000, 2048)nos va a devolver un vector de dimensión 2048 para cada token, es decir un tensor (5, 2048)"
      ],
      "metadata": {
        "id": "i1sDn-jivtZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.tensor([[1, 125647, 594, 785, 889]])  # Shape: (1, 5)\n",
        "\n",
        "# Paso 1: Embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = model.model.embed_tokens(tokens)\n",
        "\n",
        "print(\"Después de embed_tokens:\")\n",
        "print(f\"Shape: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "YCmfnM8mjNBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9561437e-bf15-4c22-d64f-7d0c744d29e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de embed_tokens:\n",
            "Shape: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualizamos los primeros 10 dígitos de cada uno."
      ],
      "metadata": {
        "id": "gZZWzX3nwoKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for embedding in embeddings[0][:]:\n",
        "    print(embedding[:8])"
      ],
      "metadata": {
        "id": "HFPZYstiwmRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fa63e3-c44c-4eb9-a565-b3c97c0022fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0004, -0.0002, -0.0007, -0.0016, -0.0005,  0.0002,  0.0071,  0.0006])\n",
            "tensor([-0.0081, -0.0030, -0.0015,  0.0126,  0.0275, -0.0022,  0.0211, -0.0004])\n",
            "tensor([-0.0039,  0.0030,  0.0129,  0.0031,  0.0062, -0.0005, -0.0036, -0.0053])\n",
            "tensor([-0.0063,  0.0043, -0.0093,  0.0012,  0.0033,  0.0076, -0.0021,  0.0039])\n",
            "tensor([-0.0011,  0.0011,  0.0017,  0.0077,  0.0106,  0.0103, -0.0118, -0.0073])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "de manera más visual:"
      ],
      "metadata": {
        "id": "ebbWiTN6yIDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embeddings - Shape: (1, 5, 2048)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "for i in range(5):\n",
        "    row = embeddings[0][i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"Token {i} ({tokens_names[i]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "8GF80cknw1D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330b7b3e-d954-4bab-c9af-8ef16a2e3505"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings - Shape: (1, 5, 2048)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0004, -0.0002, -0.0007, -0.0016, -0.0005, ..., -0.0009, 0.0008, -0.0027, 0.0003, -0.0008]  (2048 dims)\n",
            "Token 1 ( açò): [-0.0081, -0.0030, -0.0015, 0.0126, 0.0275, ..., -0.0092, -0.0146, -0.0013, -0.0075, 0.0078]  (2048 dims)\n",
            "Token 2 (  es): [-0.0039, 0.0030, 0.0129, 0.0031, 0.0062, ..., -0.0020, 0.0165, 0.0102, -0.0143, -0.0113]  (2048 dims)\n",
            "Token 3 (  or): [-0.0063, 0.0043, -0.0093, 0.0012, 0.0033, ..., 0.0062, 0.0043, 0.0063, 0.0019, 0.0030]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0011, 0.0011, 0.0017, 0.0077, 0.0106, ..., -0.0179, 0.0047, -0.0026, 0.0027, 0.0149]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se normaliza a lo largo de las filas. En este caso se hace la RMSNorm, que calcula la raíz cuadrada del promedio de los cuadrados, divide cada valor por él y multiplica por un valor gamma aprendible. Esto es una transformación matemática que no altera la dimensión."
      ],
      "metadata": {
        "id": "UM4NQEBqyj0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized = model.model.layers[0].input_layernorm(embeddings)\n",
        "print(\"\\nDespués de input_layernorm (primera capa):\")\n",
        "print(f\"Shape: {normalized.shape}\")"
      ],
      "metadata": {
        "id": "rbw0xj95xp09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05300395-ecd5-43c2-e157-f9ba21a69a85"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de input_layernorm (primera capa):\n",
            "Shape: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Después de input_layernorm - Shape: (1, 5, 2048)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "for i in range(5):\n",
        "    row = normalized[0, i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "i6sWT1zly6mH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830c5190-2f85-4f80-8e76-b335de467321"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de input_layernorm - Shape: (1, 5, 2048)\n",
            "================================================================================\n",
            "[-0.0077, -0.0051, -0.0163, -0.0330, -0.0092, ..., -0.0267, 0.0137, -0.0585, 0.0046, -0.0162]  (2048 dims)\n",
            "[-0.1861, -0.0883, -0.0411, 0.2961, 0.5605, ..., -0.2989, -0.2932, -0.0325, -0.1344, 0.1704]  (2048 dims)\n",
            "[-0.1271, 0.1251, 0.5070, 0.1055, 0.1801, ..., -0.0951, 0.4700, 0.3511, -0.3674, -0.3536]  (2048 dims)\n",
            "[-0.2106, 0.1875, -0.3764, 0.0404, 0.0988, ..., 0.2971, 0.1256, 0.2221, 0.0514, 0.0960]  (2048 dims)\n",
            "[-0.0328, 0.0419, 0.0626, 0.2359, 0.2824, ..., -0.7634, 0.1218, -0.0827, 0.0622, 0.4264]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso es ya la famosa capa de atención. Para saber exactamente qué operaciones se aplican hemos de averiguar qué tipo de atención se usa en esta arquitectura MHA estándar (todos los heads son iguales) o GQA (K y V comparten heads entre grupos de Q)."
      ],
      "metadata": {
        "id": "-SIsz65m1Rjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orden de operaciones en Attention\n",
        "\n",
        "1. **Proyectar Q, K, V** (usando q_proj, k_proj, v_proj)\n",
        "2. **Aplicar RoPE a Q y K** ← Codifica posición mediante rotación\n",
        "3. **Calcular scores** Q @ K.T\n",
        "4. **Aplicar máscara causal** ← Bloquea tokens futuros\n",
        "5. **Softmax** ← Convierte a probabilidades\n",
        "6. **Multiplicar por V** ← Combina información"
      ],
      "metadata": {
        "id": "y0k4VV8XTwGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proyectar Q, K, V"
      ],
      "metadata": {
        "id": "Xx_yylV3T9eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"num_attention_heads: {model.config.num_attention_heads}\")\n",
        "print(f\"num_key_value_heads: {model.config.num_key_value_heads}\")"
      ],
      "metadata": {
        "id": "9cxJ6cA5zUyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144a5f9d-293a-4fca-c212-ce2f4d5ec044"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_attention_heads: 16\n",
            "num_key_value_heads: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos 16 heads. Esto implica que cada head procesa 2048/16 = 128 dimensiones. Por tanto tenemos 16 Queries, 16 Keys y 16 Values (QKV)"
      ],
      "metadata": {
        "id": "j6doSCGf1v_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente operación en la transformación lineal Q, donde cada uno de nuestros vectores de 2048 dims se multiplica por una matriz (2048,2048), lo que nos devuelve otro vector de dim 2048. Al tener 5 de ellos no hemos sufrido ninguna transformación en las dimensiones."
      ],
      "metadata": {
        "id": "ZTvh02rd4PfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = model.model.layers[0].self_attn.q_proj(normalized)  # Linear(2048 → 2048)\n",
        "Q.shape"
      ],
      "metadata": {
        "id": "iL-QQE9X1lQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95b68eb-4725-4b6b-b535-a8ae6fdeb1e9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí tenemos la matriz de pesos (adquirida durante el entrenamiento) W_q."
      ],
      "metadata": {
        "id": "6f7aABsGnkjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_q = model.model.layers[0].self_attn.q_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos W_q: {W_q.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_q[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_q[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "5p5_N36pnGuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afef839-02f9-449b-91cf-17b4a11f4826"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_q: torch.Size([2048, 2048])\n",
            "[-0.0308, 0.0061, 0.0210, -0.0179, 0.0299, ..., -0.0145, 0.0255, -0.0206, -0.0233, 0.0396]  (2048 dims)\n",
            "[-0.0152, -0.0139, 0.0317, -0.0513, 0.0030, ..., -0.0072, 0.0476, 0.0098, -0.0405, 0.0171]  (2048 dims)\n",
            "[0.0388, 0.0219, -0.0245, 0.0486, 0.0085, ..., -0.0151, -0.0471, 0.0023, 0.1196, -0.0045]  (2048 dims)\n",
            "[-0.0228, -0.0154, 0.0121, -0.0112, 0.0056, ..., -0.0258, 0.0459, -0.0466, -0.1104, -0.0054]  (2048 dims)\n",
            "[0.0168, 0.0243, -0.0151, -0.0011, -0.0320, ..., -0.0091, -0.0430, 0.0151, 0.0596, 0.0203]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0304, -0.0062, -0.0016, 0.0026, 0.0040, ..., -0.0054, -0.0045, -0.0267, 0.0132, 0.0014]  (2048 dims)\n",
            "[0.0266, 0.0106, -0.0063, 0.0041, 0.0048, ..., 0.0184, -0.0197, -0.0083, 0.0146, -0.0020]  (2048 dims)\n",
            "[-0.0182, 0.0386, 0.0042, -0.0300, -0.0192, ..., 0.0228, -0.0113, 0.0557, 0.0055, 0.0194]  (2048 dims)\n",
            "[-0.0027, -0.0091, 0.0171, 0.0221, -0.0170, ..., 0.0284, 0.0065, -0.0208, 0.0110, -0.0172]  (2048 dims)\n",
            "[-0.0284, 0.0109, 0.0142, 0.0014, -0.0170, ..., 0.0074, 0.0157, 0.0231, -0.0172, -0.0089]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora multiplicamos por nuestro tensor con los embeddings normalizados. Lo que nos dará Q con dimensión (5, 2048)"
      ],
      "metadata": {
        "id": "XOwe26nZnr0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "Q = all_tokens @ W_q.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#Q = model.model.layers[0].self_attn.q_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_q: {Q.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = Q[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "8CyOPXHL2nar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36da884c-0b8a-4bf4-d6ed-4e8dfcc47496"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_q: torch.Size([5, 2048])\n",
            "[0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (2048 dims)\n",
            "[-1.3560, -1.4548, 0.6690, -0.2317, -0.0264, ..., 0.0732, -0.0682, 0.6090, 0.3939, 0.7310]  (2048 dims)\n",
            "[-0.5737, -0.6154, 0.4625, -0.4684, 0.2226, ..., -0.1939, -0.4065, 0.4861, 0.9461, 0.8626]  (2048 dims)\n",
            "[-0.2960, -0.2892, 0.1795, -0.1695, 0.5004, ..., -0.2480, -0.3824, 0.9402, 0.8782, 0.7316]  (2048 dims)\n",
            "[-0.4210, -0.3547, 0.4076, -0.6729, 0.3303, ..., -0.0021, -0.2761, 0.7703, 0.5679, 0.7159]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual forma tenemos la matriz de pesos de las Keys W_k (2048, 2048)"
      ],
      "metadata": {
        "id": "XocLHbrMoJGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_k = model.model.layers[0].self_attn.k_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos K_q: {W_k.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_k[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_k[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Wrjh3yD_n_Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b90cb611-61db-44d6-eb2d-c7513728b7ba"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos K_q: torch.Size([2048, 2048])\n",
            "[0.0065, -0.0530, 0.0045, -0.0178, 0.0410, ..., 0.0369, -0.0128, -0.0762, 0.0039, 0.0542]  (2048 dims)\n",
            "[0.0063, -0.0322, -0.0334, 0.0020, 0.0530, ..., 0.0255, -0.0110, -0.0854, -0.0042, 0.0698]  (2048 dims)\n",
            "[0.0193, 0.0154, 0.0361, 0.0168, -0.0483, ..., -0.0197, -0.0229, 0.0845, -0.0253, -0.0654]  (2048 dims)\n",
            "[-0.0138, 0.0010, -0.0510, -0.0288, 0.0559, ..., 0.0312, 0.0060, -0.0564, 0.0214, 0.0698]  (2048 dims)\n",
            "[0.0752, -0.0315, 0.0464, 0.0620, -0.0082, ..., -0.0330, -0.0435, 0.0381, 0.0037, -0.0459]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0214, -0.0229, -0.0177, -0.0608, 0.0096, ..., -0.0325, -0.0142, 0.0327, 0.0022, 0.0579]  (2048 dims)\n",
            "[-0.0092, -0.0461, 0.0295, -0.0160, 0.0105, ..., -0.0299, 0.0010, -0.0148, -0.0047, -0.0344]  (2048 dims)\n",
            "[0.0247, -0.0118, -0.0250, 0.0125, 0.0134, ..., -0.0035, 0.0085, 0.0037, -0.0204, -0.0115]  (2048 dims)\n",
            "[0.0204, -0.0157, -0.0304, 0.0057, 0.0120, ..., 0.0227, -0.0157, 0.0079, -0.0383, 0.0131]  (2048 dims)\n",
            "[0.0059, 0.0596, -0.0825, -0.0283, -0.0165, ..., 0.0270, -0.0203, 0.0188, -0.0281, -0.0076]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "K = all_tokens @ W_k.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#K_all = model.model.layers[0].self_attn.k_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_k: {K.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = K[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "42m_rweFoFqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf3677b-4cc4-4bae-b67b-31eb414319fc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_k: torch.Size([5, 2048])\n",
            "[0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (2048 dims)\n",
            "[-0.1193, -0.3046, 0.0923, -0.8474, -0.1341, ..., -0.0361, -0.6353, -0.0747, 0.4616, 0.5062]  (2048 dims)\n",
            "[-0.2077, -1.0465, 0.9611, -1.6995, 0.1201, ..., -1.6739, -0.4848, -0.7525, -0.1444, 0.1011]  (2048 dims)\n",
            "[-0.0302, -1.1034, 0.7296, -1.7983, -0.1476, ..., -2.2730, -0.1709, -0.7411, -0.2895, -0.0452]  (2048 dims)\n",
            "[1.1368, -0.8047, 1.3983, -3.4482, 4.9817, ..., -1.8034, -0.8704, -1.1216, -1.0419, -0.5522]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y por último y de igual manera los Values, con su W_v y su V"
      ],
      "metadata": {
        "id": "B6naGVQB_-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_v = model.model.layers[0].self_attn.v_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos W_v: {W_v.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_v[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_v[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "wCGsBSLLp_-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf23f0c-25ae-4fbe-f2dd-5ea023969470"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_v: torch.Size([2048, 2048])\n",
            "[0.0008, 0.0009, 0.0023, -0.0012, 0.0022, ..., 0.0002, -0.0023, -0.0014, -0.0016, 0.0017]  (2048 dims)\n",
            "[-0.0010, 0.0024, 0.0036, -0.0007, -0.0024, ..., -0.0002, 0.0001, 0.0014, -0.0007, -0.0011]  (2048 dims)\n",
            "[0.0038, 0.0025, 0.0021, 0.0014, -0.0022, ..., -0.0003, 0.0001, 0.0002, -0.0019, 0.0010]  (2048 dims)\n",
            "[0.0030, -0.0009, 0.0023, -0.0026, 0.0020, ..., 0.0018, 0.0016, 0.0031, 0.0013, 0.0003]  (2048 dims)\n",
            "[-0.0011, 0.0017, 0.0050, 0.0012, -0.0045, ..., -0.0006, -0.0021, 0.0018, 0.0022, 0.0012]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0025, 0.0011, 0.0030, -0.0005, -0.0038, ..., -0.0032, 0.0005, 0.0024, -0.0002, 0.0018]  (2048 dims)\n",
            "[0.0001, -0.0010, 0.0001, 0.0009, -0.0016, ..., 0.0021, 0.0011, -0.0018, 0.0049, 0.0009]  (2048 dims)\n",
            "[-0.0003, -0.0006, -0.0012, 0.0032, 0.0029, ..., 0.0008, -0.0005, 0.0010, 0.0030, 0.0008]  (2048 dims)\n",
            "[-0.0029, -0.0015, 0.0002, -0.0017, -0.0024, ..., -0.0031, 0.0009, -0.0015, 0.0003, 0.0007]  (2048 dims)\n",
            "[0.0024, 0.0012, 0.0001, -0.0009, -0.0018, ..., -0.0011, 0.0006, -0.0020, -0.0029, -0.0003]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "V = all_tokens @ W_v.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#V = model.model.layers[0].self_attn.v_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_v: {V.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = V[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "hPfI1MtSqABM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fcd2d4-f2d5-438f-f6a6-0ddc2fe3e3c4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_v: torch.Size([5, 2048])\n",
            "[0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (2048 dims)\n",
            "[-0.0194, 0.0367, 0.0695, 0.0036, 0.0111, ..., -0.0239, -0.0067, -0.0332, 0.0050, -0.0243]  (2048 dims)\n",
            "[0.0151, 0.0223, -0.0127, 0.0137, 0.0391, ..., -0.0464, -0.0095, -0.0084, -0.0192, 0.0126]  (2048 dims)\n",
            "[0.0186, 0.0353, -0.0046, -0.0044, 0.0186, ..., -0.0092, -0.0029, -0.0215, -0.0029, 0.0147]  (2048 dims)\n",
            "[-0.0066, 0.0324, 0.0575, -0.0022, 0.0066, ..., 0.0185, -0.0176, 0.0189, -0.0021, 0.0139]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividimos los 2048 valores en 16 heads de 128 dimensiones cada uno.\n",
        "\n",
        "En este modelo cada head tiene su propia Q, K, V completamente independiente. Los 16 heads trabajan en paralelo y luego se concatenan.\n",
        "\n",
        "Así hay más capacidad en el modelo, 16 proyecciones lineales independientes en paralelo dan más flexibilidad (en otras arquitecturas se tiene un solo Q, K y V, aunque ultimamente se agrupan por grupos como punto medio).\n",
        "\n",
        "Habrás oído decir que cada head aprende conceptos diferentes como semántica, sintaxis... pero en realidad no se sabe qué aprende cada head ni siquiera si son redundantes unos con otros."
      ],
      "metadata": {
        "id": "CpaaQbmrrf3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La operació que se realiza es simplemente dividir secuancialmente los 2048 en 16 trozos de 128."
      ],
      "metadata": {
        "id": "od1lnuu9tULL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración\n",
        "num_heads = 16\n",
        "head_dim = 2048 // 16  # = 128\n",
        "print(f'Número de heads: {num_heads}')\n",
        "print(f'Dimensión de cada head {head_dim}')\n",
        "# Reshape: dividir 2048 en (16 heads × 128 dims)\n",
        "Q_reshaped = Q.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "K_reshaped = K.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "V_reshaped = V.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "\n",
        "print(f\"\\nDespués del reshape:\")\n",
        "print(f\"Q: {Q_reshaped.shape}  # (tokens, heads, head_dim)\")\n",
        "print(f\"K: {K_reshaped.shape}\")\n",
        "print(f\"V: {V_reshaped.shape}\")"
      ],
      "metadata": {
        "id": "Ug64bDMOp_70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1e4a9c-73f4-46bc-ec47-2d1735bdb9cd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de heads: 16\n",
            "Dimensión de cada head 128\n",
            "\n",
            "Después del reshape:\n",
            "Q: torch.Size([5, 16, 128])  # (tokens, heads, head_dim)\n",
            "K: torch.Size([5, 16, 128])\n",
            "V: torch.Size([5, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transponemos de (tokens, heads, dim) a (heads, tokens, dim).\n",
        "\n",
        "Esto nos permite que cada uno de los 16 heads procese su secuencia completa de forma independiente y en paralelo. Es decir, head 0 procesa sus 5 tokens, head 1 procesa sus 5 tokens, etc."
      ],
      "metadata": {
        "id": "Pywqse0vvBXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose: intercambiar dimensiones (tokens, heads) → (heads, tokens)\n",
        "Q_heads = Q_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "K_heads = K_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "V_heads = V_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "\n",
        "print(f\"\\nDespués del transpose:\")\n",
        "print(f\"Q: {Q_heads.shape}  # (heads, tokens, head_dim)\")\n",
        "print(f\"K: {K_heads.shape}\")\n",
        "print(f\"V: {V_heads.shape}\")"
      ],
      "metadata": {
        "id": "r3kioIDtp_1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9405944e-0f4b-41fa-9134-520cb94d5287"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después del transpose:\n",
            "Q: torch.Size([16, 5, 128])  # (heads, tokens, head_dim)\n",
            "K: torch.Size([16, 5, 128])\n",
            "V: torch.Size([16, 5, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"16 heads, cada uno con 5 tokens × 128 dimensiones:\\n\")\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "print(f\"Q después del reshape: {Q_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = Q_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "ntWJwipLvLWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "594c0120-c107-4ae1-eb89-058caf299783"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 heads, cada uno con 5 tokens × 128 dimensiones:\n",
            "\n",
            "Q después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., -2.6492, 1.4339, -1.5661, 1.1632, -0.4373]  (128 dims)\n",
            "Token 1 ( açò): [1.8248, 1.5445, -0.5783, -0.1684, 0.6500, ..., -0.7560, 0.2730, -1.6083, 1.4793, 0.1333]  (128 dims)\n",
            "Token 2 (  es): [-0.0330, -0.6712, 0.3220, 0.0009, -0.5072, ..., -1.1545, -2.2204, 1.5710, -1.4540, 1.1891]  (128 dims)\n",
            "Token 3 (  or): [-0.3294, 0.2933, 0.0419, -0.4579, 0.6117, ..., -0.2443, -0.8531, 0.8565, -1.6399, 0.8491]  (128 dims)\n",
            "Token 4 ( ...): [0.3782, -0.1435, 0.2127, 0.4223, -0.2753, ..., -0.1828, -1.2649, -0.1196, -0.1121, 0.6006]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0463, 0.0001, -0.1137, 0.2507, 0.3485, ..., 0.0239, 1.2501, 0.0091, -1.1293, 0.5304]  (128 dims)\n",
            "Token 1 ( açò): [-0.0678, 0.0212, 0.1633, -0.5076, -0.3456, ..., -0.5869, 0.5336, -0.1438, -0.2584, -0.3560]  (128 dims)\n",
            "Token 2 (  es): [-0.0952, -0.1902, -0.4246, 0.9607, 0.4994, ..., -0.0599, 0.6324, 0.0925, -0.2681, -0.0036]  (128 dims)\n",
            "Token 3 (  or): [-0.0037, 0.0955, 0.3246, -0.2741, -0.4474, ..., -0.2307, 0.6501, -0.1520, 0.2410, -0.1348]  (128 dims)\n",
            "Token 4 ( ...): [0.2357, -0.1221, -0.6921, 0.1914, 0.1647, ..., 0.3253, 1.4929, 0.4698, -0.1429, 0.3723]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1706, -0.0658, -0.0793, 0.2997, -0.3046, ..., 0.7790, -0.1864, -0.0850, 0.0032, -0.0692]  (128 dims)\n",
            "Token 1 ( açò): [-0.2265, -0.6422, 0.0545, 0.5570, -0.1794, ..., 1.0372, 0.8095, -0.7266, 0.1974, -0.1934]  (128 dims)\n",
            "Token 2 (  es): [0.3361, 0.2575, 0.7406, 0.0751, -0.0637, ..., 2.0267, -0.5804, -0.3756, 0.1945, -0.2174]  (128 dims)\n",
            "Token 3 (  or): [-0.2193, 0.0724, -0.2764, 0.0435, -0.2737, ..., 0.9312, -0.3561, -0.2507, -0.1018, 0.1735]  (128 dims)\n",
            "Token 4 ( ...): [-0.1403, 0.0686, 0.1529, -0.2592, 0.3956, ..., 1.1983, 0.1541, -0.2936, 0.0375, 0.0328]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2368, 0.0941, 0.4004, -0.7508, -0.3325, ..., -2.3852, 2.1380, 1.3856, 1.8737, 1.7969]  (128 dims)\n",
            "Token 1 ( açò): [-0.3839, 0.1791, 0.3685, -0.3582, 0.0443, ..., -2.3755, 2.1573, 1.1992, 1.8477, 1.7532]  (128 dims)\n",
            "Token 2 (  es): [0.8295, -0.8050, 0.4069, -1.3075, -0.8144, ..., -1.3638, 1.2229, 0.6125, 1.0110, 0.9349]  (128 dims)\n",
            "Token 3 (  or): [-1.6822, 0.7983, -0.2109, 1.1316, 1.2373, ..., -1.4358, 1.2668, 0.6345, 1.0600, 0.9840]  (128 dims)\n",
            "Token 4 ( ...): [0.7916, -0.1446, 0.5280, -0.8374, -0.1132, ..., -1.5793, 1.4596, 0.9966, 1.2346, 1.1444]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7162, -0.2681, -0.7130, 0.0637, 0.1057, ..., -1.9417, 0.2538, 0.2032, -0.0186, 0.1673]  (128 dims)\n",
            "Token 1 ( açò): [0.3781, 0.1559, 0.1734, 0.0401, 0.2239, ..., -1.3422, -0.1519, -0.1309, 0.2114, -0.0700]  (128 dims)\n",
            "Token 2 (  es): [0.1463, -0.4681, 0.3836, 0.1440, 0.2287, ..., -0.8290, 0.4554, 0.3656, -0.1669, 0.4058]  (128 dims)\n",
            "Token 3 (  or): [0.3729, -0.0944, 0.2051, -0.0928, 0.0787, ..., -1.1985, 0.6022, 0.4389, -0.2640, 0.4956]  (128 dims)\n",
            "Token 4 ( ...): [-0.2791, 0.0694, 0.0886, 0.3482, -0.2416, ..., -1.1045, 0.1561, 0.0105, 0.1079, 0.0469]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0136, 0.2143, 0.0703, 0.2861, -0.2265, ..., -0.4979, -0.4998, -0.4526, 0.7031, 0.5648]  (128 dims)\n",
            "Token 1 ( açò): [0.5404, -0.6967, -0.0233, -0.1696, 0.3782, ..., -0.2275, 0.1538, -0.3084, 0.8759, -0.0168]  (128 dims)\n",
            "Token 2 (  es): [-0.4816, 0.4783, -0.1417, 0.4928, -0.2925, ..., 0.3936, 0.3927, 0.0567, 0.9555, -0.5115]  (128 dims)\n",
            "Token 3 (  or): [0.4272, -0.2658, 0.1673, -0.0494, -0.3090, ..., -0.0564, 0.2699, 0.2222, 0.4069, -0.0401]  (128 dims)\n",
            "Token 4 ( ...): [-0.1101, 0.0363, -0.0446, -0.0269, 0.3741, ..., 0.3131, 0.1933, -0.0790, 0.6762, -0.1910]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4356, 0.3842, 0.4366, -0.0392, -0.6289, ..., 0.3766, 0.0753, -0.0762, 0.2500, 0.2201]  (128 dims)\n",
            "Token 1 ( açò): [-0.1261, 0.2138, -0.1219, 0.1246, 0.2104, ..., 0.2324, 0.5596, 0.0502, 0.3005, -0.1350]  (128 dims)\n",
            "Token 2 (  es): [0.7951, 0.3712, 0.3620, 0.2888, -0.0242, ..., 0.8335, 0.1068, -0.3831, -0.1308, 0.5708]  (128 dims)\n",
            "Token 3 (  or): [-0.6682, -0.6734, -0.6790, -0.5111, -0.3867, ..., 0.5548, 0.0384, -0.0471, 0.0597, 0.4024]  (128 dims)\n",
            "Token 4 ( ...): [0.2483, 0.4944, 0.2735, -0.0251, 0.5204, ..., 0.6874, -0.0749, -0.6521, -0.0194, 0.4921]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3952, -0.0236, -1.0263, -0.1699, -0.1340, ..., -0.0407, -0.4909, 0.6510, -0.6418, 0.1959]  (128 dims)\n",
            "Token 1 ( açò): [0.4655, -0.1644, 0.5330, -0.2112, -1.0051, ..., -0.6447, -0.9231, 0.4158, -0.5965, 0.1688]  (128 dims)\n",
            "Token 2 (  es): [-0.8972, -0.6351, -1.8786, 0.6740, 2.2149, ..., -0.3015, -0.9045, 0.6345, -0.3708, 0.6216]  (128 dims)\n",
            "Token 3 (  or): [-0.0221, 0.1344, 1.6989, -0.1682, -0.7082, ..., -0.3799, -1.0173, 0.7634, -0.1049, 0.6471]  (128 dims)\n",
            "Token 4 ( ...): [-0.0546, -0.1184, -1.7576, -0.1427, -0.0608, ..., 0.2700, -0.1350, 0.5089, 0.2167, 0.3279]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3521, -0.4142, 0.0211, -0.3359, -0.0088, ..., -0.1404, 0.3456, 0.1993, -0.1943, -0.0363]  (128 dims)\n",
            "Token 1 ( açò): [0.3861, -0.5999, 0.4485, 0.1675, -0.1348, ..., -0.2342, 0.1050, 0.2735, -0.1040, -0.2153]  (128 dims)\n",
            "Token 2 (  es): [-0.1909, 0.5919, -0.3370, -0.8841, 0.1125, ..., -0.1782, -0.2320, 0.0280, -0.0639, -0.6009]  (128 dims)\n",
            "Token 3 (  or): [0.1286, -0.2518, 0.5868, 0.2489, -0.6469, ..., 0.0425, -0.0710, -0.0715, -0.4833, -0.5340]  (128 dims)\n",
            "Token 4 ( ...): [-0.4909, 0.7146, -0.2031, 0.1951, -0.5548, ..., 0.4684, -0.4117, -0.3180, -0.9572, -0.7467]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5972, -0.9489, 0.6698, -0.5446, -0.0196, ..., 0.5141, -0.4475, 0.1800, -0.7482, -0.2691]  (128 dims)\n",
            "Token 1 ( açò): [-0.2298, 0.0815, -0.4322, -0.1390, 0.4386, ..., -0.4145, -0.9009, 0.6052, -0.1834, 0.2828]  (128 dims)\n",
            "Token 2 (  es): [0.7328, -0.7330, 0.4384, 0.3690, 0.3225, ..., 0.4342, -0.0287, 0.5014, -0.2085, -0.0126]  (128 dims)\n",
            "Token 3 (  or): [-1.1316, 1.5047, -1.3762, -0.3298, 0.4305, ..., 0.4012, 0.3921, 0.2512, -0.5374, 0.0454]  (128 dims)\n",
            "Token 4 ( ...): [0.3708, -0.8788, 0.4431, 0.7906, 0.2628, ..., 0.0450, -0.2284, 1.5338, 0.7457, -0.3762]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.3898, 0.5608, -0.3478, -0.7937, -0.1874, ..., -6.1992, 0.7628, -0.9464, -2.9338, -1.4260]  (128 dims)\n",
            "Token 1 ( açò): [-0.6673, -1.0153, 0.2218, 0.3894, 0.6455, ..., -4.9787, 0.5735, 0.2173, -0.8115, -1.6427]  (128 dims)\n",
            "Token 2 (  es): [0.3415, -0.3062, -0.5262, -0.4598, -0.7543, ..., -1.7644, 1.4710, 0.4276, 1.0407, 0.2479]  (128 dims)\n",
            "Token 3 (  or): [-0.5770, -0.2307, 0.7096, -0.1202, 0.6154, ..., -2.3003, 1.8679, 0.0037, 0.7812, -0.0634]  (128 dims)\n",
            "Token 4 ( ...): [-0.0327, -0.2048, -0.5370, -0.0723, -0.6052, ..., -3.0527, 1.5606, -0.6601, -0.8882, -1.0182]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0608, 0.0167, -0.1336, -0.4315, 0.2209, ..., -0.0606, -0.7908, -0.1831, -0.5492, -0.3523]  (128 dims)\n",
            "Token 1 ( açò): [-0.5180, 0.3226, -0.0726, -0.3531, -0.1277, ..., -0.0856, -0.3392, -0.4314, -0.3726, 0.0073]  (128 dims)\n",
            "Token 2 (  es): [0.2565, -0.3319, -0.2928, -0.9512, -1.2323, ..., 0.3268, -0.0858, -0.2757, -0.7710, 0.2864]  (128 dims)\n",
            "Token 3 (  or): [-0.7342, 0.4365, 0.2097, 0.9972, 0.6842, ..., 0.1171, -0.3086, -0.2630, -0.3489, 0.0648]  (128 dims)\n",
            "Token 4 ( ...): [1.0788, -0.7635, -0.2226, -0.8765, 0.1732, ..., 0.7332, -0.2393, -0.6599, -0.5926, 0.5154]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0172, -0.2092, -0.3587, 0.3502, -0.2233, ..., 0.5501, -0.1378, -1.5415, 1.7725, 0.4384]  (128 dims)\n",
            "Token 1 ( açò): [-0.4731, -0.5063, 0.3211, -0.5662, 0.3967, ..., 0.6026, 0.3417, -1.1409, 1.8243, 0.3757]  (128 dims)\n",
            "Token 2 (  es): [0.3963, 1.2535, -1.3616, 0.2631, -0.8149, ..., -0.2083, 0.0482, -1.5576, 2.3082, 1.9916]  (128 dims)\n",
            "Token 3 (  or): [0.3645, -0.6735, 0.7016, -0.4345, -0.0705, ..., 0.1167, 0.2951, -2.1790, 2.6701, 1.0848]  (128 dims)\n",
            "Token 4 ( ...): [-0.6157, 0.7128, -0.6992, 0.5642, 0.4343, ..., -0.3786, 0.7434, -3.0881, 3.3234, 1.0332]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.6629, 0.4068, -0.1787, -1.1420, -0.5572, ..., -0.5138, 0.3562, 1.0724, 0.4084, 0.2699]  (128 dims)\n",
            "Token 1 ( açò): [-0.3005, -0.2346, 1.0412, 1.4295, -0.0230, ..., -0.4145, 0.2586, 0.6176, 0.4214, 0.2792]  (128 dims)\n",
            "Token 2 (  es): [0.8212, 0.9099, 0.0385, -1.2518, 1.2982, ..., -0.4150, 0.4176, 0.5711, 0.3691, 0.4123]  (128 dims)\n",
            "Token 3 (  or): [1.2070, -0.2959, -0.3636, 0.2568, -0.9351, ..., -0.6966, 0.7172, 0.8284, 0.6971, 0.6879]  (128 dims)\n",
            "Token 4 ( ...): [-1.1720, -0.6635, -0.0240, 0.3582, 0.1363, ..., -0.3872, 0.3628, 0.8055, 0.2949, 0.3160]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1084, -0.6354, 0.1776, -0.3193, 0.2784, ..., -0.5032, 0.6485, 0.9932, -0.7111, 0.6745]  (128 dims)\n",
            "Token 1 ( açò): [-0.2733, 0.0306, -0.3371, 0.5749, -0.4432, ..., -0.2288, 0.0737, 1.2006, -0.9586, 0.8340]  (128 dims)\n",
            "Token 2 (  es): [0.4464, -0.3278, 0.0376, -0.3283, 0.6782, ..., -0.5068, 0.3855, 0.5419, -0.9952, 0.9043]  (128 dims)\n",
            "Token 3 (  or): [-0.0486, -0.2113, -0.3776, -0.0247, -0.4162, ..., 0.2442, 0.3383, 0.0379, -0.6897, 0.6517]  (128 dims)\n",
            "Token 4 ( ...): [-0.2624, 0.2730, -0.3187, 0.0242, -0.6024, ..., 0.1082, 1.3626, 0.3655, -1.2784, 1.2904]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0436, 0.1843, -0.1697, 0.2867, -0.0651, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (128 dims)\n",
            "Token 1 ( açò): [0.2625, -0.2437, 0.2955, 0.0787, 0.1374, ..., 0.0736, -0.0681, 0.6089, 0.3940, 0.7308]  (128 dims)\n",
            "Token 2 (  es): [0.0601, -0.0625, -0.6281, 0.5323, 0.2201, ..., -0.1933, -0.4057, 0.4847, 0.9464, 0.8628]  (128 dims)\n",
            "Token 3 (  or): [0.0729, -0.0511, 0.1755, -0.6256, 0.2138, ..., -0.2485, -0.3816, 0.9384, 0.8779, 0.7314]  (128 dims)\n",
            "Token 4 ( ...): [0.0388, 0.1500, -0.2517, 0.1334, -0.4867, ..., -0.0022, -0.2755, 0.7693, 0.5681, 0.7157]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lo mismo se hace para K:"
      ],
      "metadata": {
        "id": "EfTRxrW3drS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"K después del reshape: {K_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = K_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ByVhNxdysp",
        "outputId": "ca6337e5-81af-46bc-b299-526b59cab583"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., 0.8779, 0.0834, -0.0709, 0.5904, -0.6143]  (128 dims)\n",
            "Token 1 ( açò): [-0.1016, 0.2246, -0.0739, 0.8160, 0.5713, ..., 1.1051, -1.9500, -0.2813, 0.2008, -0.7693]  (128 dims)\n",
            "Token 2 (  es): [-1.3314, -1.8256, 0.5817, -0.1037, -0.5556, ..., 0.7090, -0.9583, 0.0256, 0.4607, 0.7339]  (128 dims)\n",
            "Token 3 (  or): [0.8912, 1.9277, -0.2274, -0.6683, -0.0046, ..., 1.1949, -0.9730, 0.8337, 0.8412, 0.6157]  (128 dims)\n",
            "Token 4 ( ...): [-0.6050, -2.8337, -0.1328, 2.6048, -4.1525, ..., 1.2153, -0.5793, -1.4445, 2.4347, -0.0150]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4833, -0.3272, 0.1366, -0.1066, -0.4100, ..., 1.3725, 1.4919, -0.0843, -1.8912, 0.7427]  (128 dims)\n",
            "Token 1 ( açò): [0.5312, 0.8743, 0.1248, -0.0281, 0.4151, ..., -0.5029, 0.4558, 0.9583, -0.0562, 1.7710]  (128 dims)\n",
            "Token 2 (  es): [0.0372, 0.4510, 0.0066, 0.0108, 0.2158, ..., 0.2197, 0.0143, 1.3846, -0.4279, 1.6441]  (128 dims)\n",
            "Token 3 (  or): [0.1103, -0.0854, -0.1406, -0.4540, 0.0804, ..., 0.8813, -0.0961, 2.2490, 0.4245, 0.5848]  (128 dims)\n",
            "Token 4 ( ...): [0.3241, -0.2392, -0.5368, 0.1509, 0.1352, ..., 1.2765, 0.1822, 0.8957, -0.6264, 1.9425]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [1.6238, -2.1105, -0.3979, 0.6275, 0.1373, ..., 0.2037, 1.9504, -0.6677, 2.1918, -1.5709]  (128 dims)\n",
            "Token 1 ( açò): [-0.2451, 0.7183, 0.2188, -0.0430, -0.9188, ..., -0.4237, -0.8903, 0.4161, -2.4319, 0.4101]  (128 dims)\n",
            "Token 2 (  es): [-1.3096, -0.2214, 0.7616, 1.2296, 0.7686, ..., -0.0837, 0.4272, 2.3719, 0.0050, -0.1883]  (128 dims)\n",
            "Token 3 (  or): [-0.0624, -0.1482, 0.2155, 0.2019, 0.4139, ..., -0.6681, 0.5824, 0.2608, 0.0928, 1.8671]  (128 dims)\n",
            "Token 4 ( ...): [-0.5049, -0.7794, 0.8232, -0.2226, 0.0146, ..., -0.6867, 1.1887, 1.8938, 1.8025, 0.8327]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0515, 0.0201, -0.0272, 0.0114, 0.0579, ..., -2.8270, 2.8352, 2.2631, 2.7978, 2.7694]  (128 dims)\n",
            "Token 1 ( açò): [0.6304, 0.6898, -0.1606, 0.6275, 0.9082, ..., -0.3249, 0.2405, -0.6897, 0.1076, 0.0463]  (128 dims)\n",
            "Token 2 (  es): [-0.0661, -0.0983, 0.4547, -0.7227, -0.1320, ..., -1.1486, 1.0429, 0.6906, 0.9707, 0.9594]  (128 dims)\n",
            "Token 3 (  or): [0.3118, 0.4632, -0.7506, 0.7282, -0.0390, ..., -0.8657, 0.8484, 0.1496, 0.8096, 0.7892]  (128 dims)\n",
            "Token 4 ( ...): [0.2246, -0.3837, 0.2841, -0.1582, -0.4299, ..., -0.4286, 0.2719, -1.1595, 0.0955, 0.0393]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1220, 0.3785, -0.1131, -0.5678, 0.6471, ..., 0.2490, 0.8485, 1.3179, -1.8046, 1.0987]  (128 dims)\n",
            "Token 1 ( açò): [0.1914, -0.2421, 0.4797, 0.0315, 0.2931, ..., -1.1763, -0.5668, -1.1389, 1.0695, -0.8755]  (128 dims)\n",
            "Token 2 (  es): [-0.0477, 0.0234, -0.5137, 0.6056, -0.4572, ..., 1.3647, -0.8588, -0.4872, 0.1388, -1.2541]  (128 dims)\n",
            "Token 3 (  or): [0.7227, -0.3674, 0.4878, -0.4236, 0.2761, ..., 1.2733, -1.0216, -0.7051, 0.7682, -1.5164]  (128 dims)\n",
            "Token 4 ( ...): [0.0754, 0.2113, -0.1464, 0.3637, 0.2135, ..., 0.8532, -1.7782, -1.0842, -0.0238, -1.9262]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7487, 0.6855, 0.4504, 0.3863, -0.1420, ..., -2.3786, -0.9175, -1.4701, 0.3072, 0.4352]  (128 dims)\n",
            "Token 1 ( açò): [0.0028, -0.0528, -0.4741, -0.2245, -0.2700, ..., -0.3987, -1.4212, 0.6782, 0.6851, 0.0632]  (128 dims)\n",
            "Token 2 (  es): [0.5912, 0.0474, 0.1207, 0.2126, 0.0724, ..., -0.4594, -1.3940, 1.4630, 0.1750, -1.7183]  (128 dims)\n",
            "Token 3 (  or): [-0.5854, -0.2852, -0.6361, -0.0235, 0.0863, ..., -0.2120, -3.4975, 0.7702, 0.8716, 2.8818]  (128 dims)\n",
            "Token 4 ( ...): [0.3332, 0.0520, 0.2143, 0.4009, -0.2760, ..., 0.4657, -1.7529, 0.9540, -0.2581, -2.1383]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5317, 0.1277, 0.2778, 0.0549, -0.3009, ..., 1.0020, -0.5405, -1.2072, -0.0631, 1.1323]  (128 dims)\n",
            "Token 1 ( açò): [0.0345, 0.1599, 0.2010, 0.5531, 0.1546, ..., 0.0743, 2.3088, -0.3130, 1.0311, -0.5152]  (128 dims)\n",
            "Token 2 (  es): [0.1380, -0.0725, 0.3443, 0.3626, -0.1731, ..., -0.3434, 0.4526, -0.5516, 0.8111, -0.5100]  (128 dims)\n",
            "Token 3 (  or): [-0.0956, -0.2161, -0.5929, -0.4264, -0.1226, ..., -0.5480, 1.8225, -1.8400, 1.3272, -0.4146]  (128 dims)\n",
            "Token 4 ( ...): [0.1354, 0.0285, 0.9298, 0.3653, 0.7767, ..., -0.4265, 0.2204, -1.4258, 1.4335, -0.1583]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0247, -0.3167, -0.0929, -0.2560, -0.1773, ..., -0.7128, -2.8245, 1.3088, -1.4605, 1.0619]  (128 dims)\n",
            "Token 1 ( açò): [0.2122, 0.2962, 0.3352, -0.6600, 0.3573, ..., 0.3200, -0.3037, -0.0852, 0.6982, 0.7971]  (128 dims)\n",
            "Token 2 (  es): [-0.6828, -2.3507, 0.0626, 1.1933, 0.3841, ..., 0.9818, -1.9419, -0.1737, 1.4250, 0.3966]  (128 dims)\n",
            "Token 3 (  or): [0.6072, 0.8552, 0.1151, -0.1495, -0.0570, ..., -0.4545, -1.2716, 1.0400, 1.5095, -0.1437]  (128 dims)\n",
            "Token 4 ( ...): [-0.8149, -0.4780, -0.8051, -0.0399, -0.5052, ..., -0.9461, -0.3107, -1.6995, 0.8716, -0.1181]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1430, -0.2671, 0.2044, -0.0357, 0.1421, ..., -0.2843, 0.7301, 0.2891, 0.4284, 0.0749]  (128 dims)\n",
            "Token 1 ( açò): [-0.0519, 0.0675, 0.0634, -0.0093, 0.0689, ..., -0.2268, 0.8002, 0.7047, 0.8372, 0.0577]  (128 dims)\n",
            "Token 2 (  es): [-0.0596, -0.2477, 0.5313, 0.0864, -0.2184, ..., -0.0497, -1.0718, -0.1084, 1.4080, -0.8774]  (128 dims)\n",
            "Token 3 (  or): [0.2037, 0.1340, -0.4119, -0.0044, 0.1656, ..., 0.4834, -0.8425, -0.8408, 0.5783, -0.5453]  (128 dims)\n",
            "Token 4 ( ...): [0.1863, -0.1383, 0.1654, -0.4510, -0.2670, ..., 0.5292, -1.4703, -1.2482, 1.9447, -0.0659]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8143, -1.2586, 0.4210, -1.8082, -2.6416, ..., -0.3086, 0.7203, 0.4455, 0.2583, -0.7831]  (128 dims)\n",
            "Token 1 ( açò): [-0.4242, 0.4227, 0.0699, -0.1361, 0.5490, ..., -0.8333, 0.1749, -0.5771, 0.7942, -0.8797]  (128 dims)\n",
            "Token 2 (  es): [0.6524, -0.1383, -0.2649, -0.8973, -0.3629, ..., -1.1023, -0.4354, -0.2108, -0.3338, -0.0032]  (128 dims)\n",
            "Token 3 (  or): [-0.0906, 0.6085, -0.4664, 0.1581, -1.0070, ..., -1.3722, -0.4048, 0.3373, -0.2194, 0.1047]  (128 dims)\n",
            "Token 4 ( ...): [-1.0321, 0.3380, 2.1548, 1.1586, 3.0817, ..., -0.0782, 0.4186, 0.1855, 0.3351, -1.8325]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1141, 0.0147, 0.2306, 0.0312, -0.1102, ..., -0.4191, 1.2161, 0.5313, 0.9569, -0.1863]  (128 dims)\n",
            "Token 1 ( açò): [-0.0626, -0.1629, -0.0357, 0.1074, -0.0703, ..., 0.5938, 0.8049, 1.2462, 2.1894, 0.4079]  (128 dims)\n",
            "Token 2 (  es): [0.6054, 0.2522, -0.9695, -0.2041, -0.8891, ..., 1.2149, -0.5078, 1.8112, 1.9427, 1.8350]  (128 dims)\n",
            "Token 3 (  or): [-0.1481, 0.0852, 0.9688, 0.1043, 0.3827, ..., 0.9909, -0.7568, 1.6146, 1.2028, 2.4526]  (128 dims)\n",
            "Token 4 ( ...): [-0.2215, 0.0372, -0.2650, 0.2819, 0.3325, ..., 0.5849, 1.1333, 0.9238, 1.0887, 2.2401]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.2609, -0.2117, 0.4495, 0.1503, 0.2674, ..., -0.4155, -0.3622, -0.2970, -0.0813, -0.1502]  (128 dims)\n",
            "Token 1 ( açò): [0.2257, -0.4350, 0.1395, 0.2924, 0.4226, ..., -0.2560, -0.0121, 0.1954, 0.1556, 1.6544]  (128 dims)\n",
            "Token 2 (  es): [-0.3313, -0.2240, 0.1297, -0.1254, 0.3050, ..., 0.0770, -0.0906, 0.6055, -0.4251, 2.1312]  (128 dims)\n",
            "Token 3 (  or): [0.0777, -0.3345, -0.5978, 0.1790, 0.1033, ..., -2.0161, 0.0425, 0.3927, 0.4521, 1.5922]  (128 dims)\n",
            "Token 4 ( ...): [-0.4585, 0.7728, 0.6825, 0.5927, -0.1042, ..., -1.1302, 0.4805, 0.9536, 0.3927, 1.3267]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1495, 0.2181, -0.2070, -0.1809, -0.1314, ..., 1.1907, -2.8027, -0.6817, 0.5220, -0.8140]  (128 dims)\n",
            "Token 1 ( açò): [-0.9060, -0.6845, 0.6847, 0.6593, 0.6486, ..., 1.5745, -2.6731, -0.8837, 0.7214, -1.0012]  (128 dims)\n",
            "Token 2 (  es): [0.6419, 0.7360, -0.8159, 0.6620, -0.2170, ..., 0.6489, -0.5654, 0.5717, -0.7819, -0.5677]  (128 dims)\n",
            "Token 3 (  or): [-0.0679, -0.6155, 0.8432, -0.8017, -0.1340, ..., -0.1520, -2.8569, -0.0696, -0.3681, -0.7334]  (128 dims)\n",
            "Token 4 ( ...): [-0.4549, 1.8215, -1.4481, 1.7549, 2.1875, ..., -0.1054, -1.9019, 0.3074, -0.5237, -0.6556]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4624, -0.1438, 0.5297, 0.3217, 0.5069, ..., -0.7191, 0.7023, 0.2106, 0.3088, 0.4109]  (128 dims)\n",
            "Token 1 ( açò): [-0.1317, -0.0174, -0.8673, -0.8122, -0.3214, ..., 0.1797, -0.2020, -0.4274, -0.9015, -0.8953]  (128 dims)\n",
            "Token 2 (  es): [0.3496, 0.1353, -0.1683, -0.2381, 0.7356, ..., -2.0417, 2.2300, 0.2500, 1.2086, 1.9691]  (128 dims)\n",
            "Token 3 (  or): [0.3565, 0.1333, -0.2500, 0.1146, -0.6161, ..., -2.0303, 2.0644, 0.3191, 1.2834, 1.7251]  (128 dims)\n",
            "Token 4 ( ...): [-1.0352, -0.3384, 0.2985, 0.0436, -0.4783, ..., -1.4647, 1.4464, -0.5570, 0.1502, 0.6561]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2740, -0.0506, 0.4008, -0.1798, 0.0627, ..., 1.1809, -0.2412, -0.8631, -0.6208, 0.6555]  (128 dims)\n",
            "Token 1 ( açò): [-0.1912, 0.1285, 0.0406, 0.2929, -0.0425, ..., -0.2111, -0.6323, 0.8501, 0.3801, -0.4111]  (128 dims)\n",
            "Token 2 (  es): [0.1033, -0.0220, 0.0327, -0.4313, 0.2558, ..., 1.0503, -0.8605, -1.1373, 1.0758, -1.0259]  (128 dims)\n",
            "Token 3 (  or): [0.3152, 0.0959, -0.1321, 0.4906, -0.3217, ..., 0.3542, -1.6046, -0.4638, 0.3473, -0.3991]  (128 dims)\n",
            "Token 4 ( ...): [-0.3863, -0.0211, 0.0948, -0.1853, -0.0408, ..., 1.9919, -1.7386, -1.2319, 0.6787, -0.6202]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8710, -1.0364, -0.1889, -0.3356, 0.0606, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (128 dims)\n",
            "Token 1 ( açò): [0.1092, 0.3405, -0.1620, -0.3379, -0.1355, ..., -0.0360, -0.6338, -0.0742, 0.4613, 0.5065]  (128 dims)\n",
            "Token 2 (  es): [0.1942, 0.0635, 0.0911, 0.3123, 0.2469, ..., -1.6743, -0.4812, -0.7509, -0.1428, 0.1033]  (128 dims)\n",
            "Token 3 (  or): [-0.0284, 0.0153, -0.1884, -0.1263, -0.1753, ..., -2.2731, -0.1676, -0.7398, -0.2897, -0.0441]  (128 dims)\n",
            "Token 4 ( ...): [0.2155, -0.5425, -0.4284, 0.5324, -0.1978, ..., -1.8042, -0.8629, -1.1168, -1.0369, -0.5487]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y para V:"
      ],
      "metadata": {
        "id": "dHrAUw9gd8ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"V después del reshape: {V_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = V_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Letc7Biqd90t",
        "outputId": "e21a4f4e-393d-45b1-df64-b07141a2882a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0101, 0.0171, 0.0520, 0.0119, -0.0049]  (128 dims)\n",
            "Token 1 ( açò): [-0.0194, 0.0367, 0.0695, 0.0036, 0.0111, ..., 0.0025, -0.0039, -0.0263, -0.0170, -0.0196]  (128 dims)\n",
            "Token 2 (  es): [0.0151, 0.0223, -0.0127, 0.0137, 0.0391, ..., -0.0199, -0.0072, -0.0610, -0.0246, -0.0376]  (128 dims)\n",
            "Token 3 (  or): [0.0186, 0.0353, -0.0046, -0.0044, 0.0186, ..., 0.0247, 0.0039, -0.0278, -0.0000, -0.0036]  (128 dims)\n",
            "Token 4 ( ...): [-0.0066, 0.0324, 0.0575, -0.0022, 0.0066, ..., 0.0131, 0.0214, -0.0935, -0.0029, -0.0114]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0012, 0.1335, -0.0075, -0.0087, -0.0006, ..., 0.0052, -0.0055, 0.0015, 0.0052, 0.0133]  (128 dims)\n",
            "Token 1 ( açò): [0.0426, 0.0277, -0.0045, -0.0126, -0.0293, ..., 0.0589, -0.0051, 0.0251, 0.0028, -0.0148]  (128 dims)\n",
            "Token 2 (  es): [-0.0094, 0.0100, 0.0084, -0.0117, 0.0130, ..., -0.0241, -0.0061, -0.0019, 0.0277, 0.0257]  (128 dims)\n",
            "Token 3 (  or): [-0.0295, 0.0242, 0.0070, -0.0241, 0.0527, ..., -0.0092, -0.0038, 0.0096, 0.0276, 0.0201]  (128 dims)\n",
            "Token 4 ( ...): [-0.0058, -0.0625, -0.0261, -0.0466, 0.0628, ..., 0.0187, -0.0110, 0.0326, -0.0018, 0.0205]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0236, 0.0308, 0.0247, 0.0003, -0.0147, ..., -0.0319, -0.0092, -0.0265, -0.0062, 0.0050]  (128 dims)\n",
            "Token 1 ( açò): [-0.0027, -0.0061, -0.0405, 0.0461, -0.0422, ..., -0.0023, 0.0749, -0.0437, 0.0127, -0.0384]  (128 dims)\n",
            "Token 2 (  es): [0.0366, 0.0692, -0.0059, -0.0177, 0.0376, ..., 0.0167, -0.0588, 0.0036, 0.0036, 0.0340]  (128 dims)\n",
            "Token 3 (  or): [-0.0004, 0.0122, -0.0074, -0.0076, 0.0054, ..., 0.0240, -0.0061, -0.0007, -0.0063, -0.0144]  (128 dims)\n",
            "Token 4 ( ...): [0.0005, 0.0141, -0.0053, -0.0012, 0.0151, ..., 0.0161, -0.0032, 0.0260, 0.0005, 0.0045]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, ..., 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]  (128 dims)\n",
            "Token 1 ( açò): [0.0124, 0.0801, -0.0509, -0.0666, -0.1353, ..., 0.1175, -0.0220, 0.0734, 0.0029, 0.0229]  (128 dims)\n",
            "Token 2 (  es): [0.0061, -0.0187, 0.0600, -0.0887, 0.0140, ..., 0.0280, -0.0018, -0.0147, 0.0127, -0.0417]  (128 dims)\n",
            "Token 3 (  or): [-0.0441, -0.0376, -0.0135, 0.0945, 0.0198, ..., 0.0044, -0.0414, 0.1110, -0.0328, 0.0965]  (128 dims)\n",
            "Token 4 ( ...): [-0.0306, 0.1822, 0.0339, 0.0769, 0.0520, ..., -0.1586, 0.0068, 0.1036, 0.0005, -0.0832]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0221, -0.0290, 0.0401, 0.0479, 0.0276, ..., -0.0478, 0.0607, 0.0734, -0.0399, -0.0458]  (128 dims)\n",
            "Token 1 ( açò): [0.0096, 0.0368, 0.0156, 0.0479, -0.0254, ..., 0.2872, -0.0892, 0.0060, 0.0457, -0.0251]  (128 dims)\n",
            "Token 2 (  es): [0.0231, -0.0055, 0.0283, -0.0364, 0.0028, ..., 0.1219, 0.0114, -0.0420, -0.0282, -0.0114]  (128 dims)\n",
            "Token 3 (  or): [0.0437, -0.0278, 0.0092, -0.0403, 0.0063, ..., 0.1447, 0.0263, -0.0003, 0.0165, -0.0184]  (128 dims)\n",
            "Token 4 ( ...): [-0.0476, 0.0527, -0.0094, -0.0297, 0.0155, ..., -0.0083, -0.0393, -0.0186, 0.0095, 0.0267]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0429, -0.0179, -0.0109, 0.0065, -0.0191, ..., 0.0514, -0.0193, -0.0056, 0.0338, -0.0442]  (128 dims)\n",
            "Token 1 ( açò): [-0.0294, 0.0166, 0.0486, 0.0059, 0.0568, ..., -0.0196, 0.0131, -0.0123, 0.0156, -0.0071]  (128 dims)\n",
            "Token 2 (  es): [-0.0114, 0.0198, 0.0339, 0.0035, -0.0074, ..., -0.0129, 0.0233, -0.0073, -0.0093, -0.0188]  (128 dims)\n",
            "Token 3 (  or): [0.0054, 0.0167, 0.0031, 0.0001, 0.0260, ..., -0.0164, 0.0192, 0.0012, 0.0150, -0.0149]  (128 dims)\n",
            "Token 4 ( ...): [0.0368, -0.0021, 0.0069, -0.0106, 0.0356, ..., -0.0075, -0.0344, 0.0321, -0.0099, -0.0200]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0137, 0.0080, -0.0237, 0.0203, 0.0457, ..., -0.0295, -0.0021, -0.0045, -0.0225, 0.0009]  (128 dims)\n",
            "Token 1 ( açò): [0.0214, -0.0035, 0.0208, 0.0165, -0.0303, ..., -0.0156, -0.0377, 0.0177, -0.0271, 0.0035]  (128 dims)\n",
            "Token 2 (  es): [0.0166, 0.0083, -0.0105, -0.0016, 0.0297, ..., 0.0168, -0.0145, 0.0046, -0.0102, -0.0135]  (128 dims)\n",
            "Token 3 (  or): [-0.0126, -0.0075, 0.0158, -0.0152, 0.0139, ..., -0.0061, -0.0067, 0.0099, -0.0027, 0.0025]  (128 dims)\n",
            "Token 4 ( ...): [-0.0277, 0.0269, 0.0029, 0.0682, 0.0398, ..., -0.0310, -0.0324, -0.0141, -0.0079, -0.0239]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0055, 0.0139, -0.0012, -0.0055, 0.0251, ..., -0.0057, -0.0133, 0.0034, 0.0156, -0.0053]  (128 dims)\n",
            "Token 1 ( açò): [0.0439, -0.0256, -0.0133, 0.2030, 0.0612, ..., 0.0385, 0.0356, 0.0546, 0.0192, 0.0171]  (128 dims)\n",
            "Token 2 (  es): [-0.0409, -0.0017, -0.0221, -0.0363, 0.0533, ..., -0.0140, 0.0627, 0.0329, -0.0070, 0.0292]  (128 dims)\n",
            "Token 3 (  or): [-0.0369, -0.0000, -0.0337, 0.0020, 0.0267, ..., -0.0130, 0.0426, -0.0443, 0.0010, 0.0367]  (128 dims)\n",
            "Token 4 ( ...): [0.0719, 0.0152, 0.0941, -0.2367, -0.0310, ..., -0.0138, -0.2049, 0.0382, 0.0516, 0.0021]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0426, -0.0096, -0.0343, 0.0244, -0.0478, ..., 0.0463, -0.0073, -0.0316, 0.0095, 0.0244]  (128 dims)\n",
            "Token 1 ( açò): [0.0009, -0.0040, 0.0368, -0.0228, 0.0197, ..., 0.0210, -0.0071, -0.0095, -0.0007, 0.0064]  (128 dims)\n",
            "Token 2 (  es): [0.0050, 0.0272, -0.0084, 0.0115, 0.0296, ..., -0.0079, -0.0025, 0.0208, -0.0253, 0.0013]  (128 dims)\n",
            "Token 3 (  or): [-0.0353, 0.0153, -0.0183, 0.0017, 0.0001, ..., 0.0235, 0.0279, 0.0039, -0.0091, -0.0165]  (128 dims)\n",
            "Token 4 ( ...): [-0.0212, 0.0092, 0.0090, 0.0367, -0.0870, ..., -0.0468, 0.0262, -0.0201, 0.0186, -0.0431]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0082, 0.0044, -0.0388, 0.0688, -0.0155, ..., -0.0285, 0.0363, -0.0448, 0.0119, -0.0096]  (128 dims)\n",
            "Token 1 ( açò): [-0.0056, 0.0459, 0.0434, -0.0685, -0.0240, ..., 0.0780, 0.0057, -0.0755, -0.0033, 0.0095]  (128 dims)\n",
            "Token 2 (  es): [-0.0282, -0.0311, -0.0211, 0.0410, 0.0116, ..., -0.0050, -0.0147, 0.0178, -0.0063, -0.0120]  (128 dims)\n",
            "Token 3 (  or): [-0.0517, -0.0338, 0.0242, -0.0501, 0.0267, ..., -0.0091, 0.0373, 0.0824, 0.0130, 0.0101]  (128 dims)\n",
            "Token 4 ( ...): [-0.0351, 0.0232, -0.0426, -0.0012, -0.0061, ..., 0.0995, 0.0101, -0.1281, 0.0316, 0.0101]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, ..., -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]  (128 dims)\n",
            "Token 1 ( açò): [-0.0541, -0.0032, 0.0071, -0.0467, 0.0658, ..., 0.0618, 0.0090, 0.0202, -0.0576, 0.0039]  (128 dims)\n",
            "Token 2 (  es): [0.0586, -0.0087, 0.0153, -0.0023, -0.0177, ..., 0.0125, 0.0367, -0.0037, -0.0138, 0.0284]  (128 dims)\n",
            "Token 3 (  or): [0.0310, -0.0210, 0.0042, 0.0028, -0.0091, ..., 0.0202, 0.0172, 0.0222, -0.0011, 0.0154]  (128 dims)\n",
            "Token 4 ( ...): [0.1293, -0.0294, 0.0033, 0.1233, -0.0271, ..., 0.0506, -0.0043, 0.0095, -0.0196, -0.0010]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0407, 0.0040, -0.0497, -0.0331, 0.0371, ..., -0.0325, 0.0308, -0.0027, 0.0419, 0.0007]  (128 dims)\n",
            "Token 1 ( açò): [0.0029, -0.0077, -0.0035, 0.0170, 0.0249, ..., 0.0330, 0.0383, -0.0220, 0.0484, 0.0010]  (128 dims)\n",
            "Token 2 (  es): [-0.0018, -0.0358, 0.0079, 0.0293, 0.0126, ..., 0.0183, 0.0110, -0.0291, 0.0352, 0.0065]  (128 dims)\n",
            "Token 3 (  or): [0.0062, -0.0349, -0.0165, 0.0050, -0.0084, ..., -0.0046, -0.0042, 0.0011, 0.0251, -0.0360]  (128 dims)\n",
            "Token 4 ( ...): [-0.0380, -0.0100, 0.0140, 0.0530, -0.0513, ..., -0.0103, -0.0077, 0.0163, -0.0063, -0.0085]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0182, 0.0278, -0.0238, 0.0184, -0.0484, ..., -0.0921, 0.0130, -0.0359, 0.0489, -0.0099]  (128 dims)\n",
            "Token 1 ( açò): [-0.0119, -0.0057, -0.1055, -0.0029, 0.0033, ..., -0.0280, -0.0004, -0.0273, -0.0478, -0.0256]  (128 dims)\n",
            "Token 2 (  es): [-0.0196, -0.0070, -0.0155, 0.0050, -0.0083, ..., -0.0304, 0.0145, -0.0046, 0.0535, -0.0190]  (128 dims)\n",
            "Token 3 (  or): [0.0039, 0.0184, 0.0117, 0.0163, 0.0446, ..., 0.0288, -0.0302, 0.0397, -0.0756, 0.0071]  (128 dims)\n",
            "Token 4 ( ...): [0.0231, -0.0227, -0.0038, 0.0707, 0.0955, ..., 0.0040, -0.0115, -0.0261, 0.0004, 0.0255]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0174, -0.1290, -0.0275, 0.0390, -0.0243, ..., 0.0371, -0.0154, -0.0116, 0.0631, -0.0049]  (128 dims)\n",
            "Token 1 ( açò): [-0.0758, 0.0054, 0.0508, 0.0180, 0.0769, ..., 0.0187, 0.0278, -0.0476, 0.0130, -0.1136]  (128 dims)\n",
            "Token 2 (  es): [-0.0312, -0.0069, -0.0010, 0.0012, -0.0187, ..., -0.0060, -0.0030, -0.0247, -0.0279, -0.0743]  (128 dims)\n",
            "Token 3 (  or): [0.0037, 0.0041, 0.0107, -0.0344, 0.0404, ..., 0.0165, -0.0253, 0.0303, -0.0198, -0.0638]  (128 dims)\n",
            "Token 4 ( ...): [-0.0218, 0.0483, 0.0183, -0.0697, 0.0166, ..., 0.0565, -0.0097, 0.0232, 0.0310, -0.1542]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0047, -0.0257, 0.0017, -0.0183, 0.0019, ..., -0.0218, -0.0255, -0.0075, 0.0050, -0.0041]  (128 dims)\n",
            "Token 1 ( açò): [-0.0450, -0.0349, -0.0102, -0.0333, 0.0143, ..., -0.0554, 0.0109, -0.0386, -0.0383, -0.0504]  (128 dims)\n",
            "Token 2 (  es): [-0.0329, 0.0189, -0.0070, 0.0014, -0.0092, ..., 0.0283, -0.0124, -0.0035, 0.0228, -0.0141]  (128 dims)\n",
            "Token 3 (  or): [0.0076, 0.0523, -0.0147, 0.0218, 0.0518, ..., -0.0375, 0.0235, 0.0123, 0.0201, -0.0323]  (128 dims)\n",
            "Token 4 ( ...): [-0.0232, 0.0332, -0.0303, 0.0170, 0.0164, ..., -0.0340, 0.0043, 0.0434, 0.0100, 0.0008]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0001, 0.0307, 0.0271, -0.0246, -0.0394, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (128 dims)\n",
            "Token 1 ( açò): [-0.0465, 0.0419, 0.0015, 0.0234, 0.0394, ..., -0.0239, -0.0067, -0.0332, 0.0050, -0.0243]  (128 dims)\n",
            "Token 2 (  es): [-0.0306, -0.0032, 0.0080, -0.0363, 0.0078, ..., -0.0464, -0.0095, -0.0084, -0.0192, 0.0126]  (128 dims)\n",
            "Token 3 (  or): [-0.0078, 0.0034, 0.0153, -0.0016, -0.0157, ..., -0.0092, -0.0029, -0.0215, -0.0029, 0.0147]  (128 dims)\n",
            "Token 4 ( ...): [-0.0578, 0.0029, 0.0008, -0.0237, 0.0056, ..., 0.0185, -0.0176, 0.0189, -0.0021, 0.0139]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transponemos K para poder hacer la multiplicación matricial Q @ K^T.\n",
        "\n",
        "Queremos comparar cada query (128 dims) con cada key (128 dims) mediante producto escalar. Al transponer K de (16, 5, 128) a (16, 128, 5), la multiplicación Q @ K^T nos da una matriz (16, 5, 5) donde cada posición (i,j) es el score de cuánto el token i atiende al token j.\n",
        "\n",
        "De esta manera tenemos 16 matrices 5x5"
      ],
      "metadata": {
        "id": "FVRFCB9MuTSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_vec.shape"
      ],
      "metadata": {
        "id": "LT4TPoZrDbAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74aa1678-e9aa-46cd-bed0-16813a67087e"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2048])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicar RoPE a Q y K"
      ],
      "metadata": {
        "id": "2NRw1mJGUIdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de implementar la máscara aplicamos un positional encoding. En esta arquitectura se usa RoPE (Rotatory Positional Encoding) que usa una fórmula basada en seno y coseno para aplicar una rotación a cada vector de la matriz. Sólo se aplica a Q y a K. Inspeccionamos el método forward para confirmar que se aplica RoPE y en qué momento."
      ],
      "metadata": {
        "id": "6okZMCSAXpCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "print(inspect.getsource(attention_layer.forward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj6ZF3dHZ1lR",
        "outputId": "954eaf1d-edc7-4e15-e783-a64d9f513495"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
            "        attention_mask: Optional[torch.Tensor],\n",
            "        past_key_values: Optional[Cache] = None,\n",
            "        cache_position: Optional[torch.LongTensor] = None,\n",
            "        **kwargs: Unpack[TransformersKwargs],\n",
            "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
            "        input_shape = hidden_states.shape[:-1]\n",
            "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
            "\n",
            "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "\n",
            "        cos, sin = position_embeddings\n",
            "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
            "\n",
            "        if past_key_values is not None:\n",
            "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
            "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
            "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
            "\n",
            "        attention_interface: Callable = eager_attention_forward\n",
            "        if self.config._attn_implementation != \"eager\":\n",
            "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
            "\n",
            "        attn_output, attn_weights = attention_interface(\n",
            "            self,\n",
            "            query_states,\n",
            "            key_states,\n",
            "            value_states,\n",
            "            attention_mask,\n",
            "            dropout=0.0 if not self.training else self.attention_dropout,\n",
            "            scaling=self.scaling,\n",
            "            **kwargs,\n",
            "        )\n",
            "\n",
            "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
            "        attn_output = self.o_proj(attn_output)\n",
            "        return attn_output, attn_weights\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
        "\n",
        "Q_antes_rope = Q_reshaped.clone()\n",
        "K_antes_rope = K_reshaped.clone()\n",
        "\n",
        "print(\"Dimensiones ANTES de RoPE:\")\n",
        "print(f\"Shape: {Q_antes_rope.shape}\")  # (5, 16, 128)\n",
        "\n",
        "\n",
        "Q_transposed = Q_reshaped.unsqueeze(0).transpose(1, 2)  # (1, 16, 5, 128)\n",
        "K_transposed = K_reshaped.unsqueeze(0).transpose(1, 2)\n",
        "\n",
        "positions_ids = torch.arange(5, device=Q_reshaped.device).unsqueeze(0)\n",
        "\n",
        "cos_sin_cache = model.model.rotary_emb(K_transposed, positions_ids)\n",
        "\n",
        "Q_rope, K_rope = apply_rotary_pos_emb(Q_transposed, K_transposed, *cos_sin_cache)\n",
        "\n",
        "Q_rope_reshaped = Q_rope.squeeze(0).transpose(0, 1)  # (5, 16, 128)\n",
        "K_rope_reshaped = K_rope.squeeze(0).transpose(0, 1)\n",
        "\n",
        "print(\"\\nDimensiones DESPUÉS de RoPE:\")\n",
        "print(f\"Shape: {Q_reshaped.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYz3BA9IUxg5",
        "outputId": "d561936b-86cf-438d-dc6b-e42a9fb6f80e"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones ANTES de RoPE:\n",
            "Shape: torch.Size([5, 16, 128])\n",
            "\n",
            "Dimensiones DESPUÉS de RoPE:\n",
            "Shape: torch.Size([5, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_heads = Q_rope_reshaped.transpose(0, 1)\n",
        "\n",
        "print(f\"Q después de RoPE shape: {Q_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = Q_heads[head_idx, token_idx, :]\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tiMjNm3beUS",
        "outputId": "041e783e-274a-4d66-dcad-9f767bf2e6a0"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q después de RoPE shape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., -2.6492, 1.4339, -1.5661, 1.1632, -0.4373]  (128 dims)\n",
            "Token 1 ( açò): [1.8248, 1.5445, -0.5783, -0.1684, 0.6500, ..., -0.7560, 0.2730, -1.6083, 1.4793, 0.1333]  (128 dims)\n",
            "Token 2 (  es): [-0.0330, -0.6712, 0.3220, 0.0009, -0.5072, ..., -1.1545, -2.2204, 1.5710, -1.4540, 1.1891]  (128 dims)\n",
            "Token 3 (  or): [-0.3294, 0.2933, 0.0419, -0.4579, 0.6117, ..., -0.2443, -0.8531, 0.8565, -1.6399, 0.8491]  (128 dims)\n",
            "Token 4 ( ...): [0.3782, -0.1435, 0.2127, 0.4223, -0.2753, ..., -0.1828, -1.2649, -0.1196, -0.1121, 0.6006]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0463, 0.0001, -0.1137, 0.2507, 0.3485, ..., 0.0239, 1.2501, 0.0091, -1.1293, 0.5304]  (128 dims)\n",
            "Token 1 ( açò): [-0.0678, 0.0212, 0.1633, -0.5076, -0.3456, ..., -0.5869, 0.5336, -0.1438, -0.2584, -0.3560]  (128 dims)\n",
            "Token 2 (  es): [-0.0952, -0.1902, -0.4246, 0.9607, 0.4994, ..., -0.0599, 0.6324, 0.0925, -0.2681, -0.0036]  (128 dims)\n",
            "Token 3 (  or): [-0.0037, 0.0955, 0.3246, -0.2741, -0.4474, ..., -0.2307, 0.6501, -0.1520, 0.2410, -0.1348]  (128 dims)\n",
            "Token 4 ( ...): [0.2357, -0.1221, -0.6921, 0.1914, 0.1647, ..., 0.3253, 1.4929, 0.4698, -0.1429, 0.3723]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1706, -0.0658, -0.0793, 0.2997, -0.3046, ..., 0.7790, -0.1864, -0.0850, 0.0032, -0.0692]  (128 dims)\n",
            "Token 1 ( açò): [-0.2265, -0.6422, 0.0545, 0.5570, -0.1794, ..., 1.0372, 0.8095, -0.7266, 0.1974, -0.1934]  (128 dims)\n",
            "Token 2 (  es): [0.3361, 0.2575, 0.7406, 0.0751, -0.0637, ..., 2.0267, -0.5804, -0.3756, 0.1945, -0.2174]  (128 dims)\n",
            "Token 3 (  or): [-0.2193, 0.0724, -0.2764, 0.0435, -0.2737, ..., 0.9312, -0.3561, -0.2507, -0.1018, 0.1735]  (128 dims)\n",
            "Token 4 ( ...): [-0.1403, 0.0686, 0.1529, -0.2592, 0.3956, ..., 1.1983, 0.1541, -0.2936, 0.0375, 0.0328]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2368, 0.0941, 0.4004, -0.7508, -0.3325, ..., -2.3852, 2.1380, 1.3856, 1.8737, 1.7969]  (128 dims)\n",
            "Token 1 ( açò): [-0.3839, 0.1791, 0.3685, -0.3582, 0.0443, ..., -2.3755, 2.1573, 1.1992, 1.8477, 1.7532]  (128 dims)\n",
            "Token 2 (  es): [0.8295, -0.8050, 0.4069, -1.3075, -0.8144, ..., -1.3638, 1.2229, 0.6125, 1.0110, 0.9349]  (128 dims)\n",
            "Token 3 (  or): [-1.6822, 0.7983, -0.2109, 1.1316, 1.2373, ..., -1.4358, 1.2668, 0.6345, 1.0600, 0.9840]  (128 dims)\n",
            "Token 4 ( ...): [0.7916, -0.1446, 0.5280, -0.8374, -0.1132, ..., -1.5793, 1.4596, 0.9966, 1.2346, 1.1444]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7162, -0.2681, -0.7130, 0.0637, 0.1057, ..., -1.9417, 0.2538, 0.2032, -0.0186, 0.1673]  (128 dims)\n",
            "Token 1 ( açò): [0.3781, 0.1559, 0.1734, 0.0401, 0.2239, ..., -1.3422, -0.1519, -0.1309, 0.2114, -0.0700]  (128 dims)\n",
            "Token 2 (  es): [0.1463, -0.4681, 0.3836, 0.1440, 0.2287, ..., -0.8290, 0.4554, 0.3656, -0.1669, 0.4058]  (128 dims)\n",
            "Token 3 (  or): [0.3729, -0.0944, 0.2051, -0.0928, 0.0787, ..., -1.1985, 0.6022, 0.4389, -0.2640, 0.4956]  (128 dims)\n",
            "Token 4 ( ...): [-0.2791, 0.0694, 0.0886, 0.3482, -0.2416, ..., -1.1045, 0.1561, 0.0105, 0.1079, 0.0469]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0136, 0.2143, 0.0703, 0.2861, -0.2265, ..., -0.4979, -0.4998, -0.4526, 0.7031, 0.5648]  (128 dims)\n",
            "Token 1 ( açò): [0.5404, -0.6967, -0.0233, -0.1696, 0.3782, ..., -0.2275, 0.1538, -0.3084, 0.8759, -0.0168]  (128 dims)\n",
            "Token 2 (  es): [-0.4816, 0.4783, -0.1417, 0.4928, -0.2925, ..., 0.3936, 0.3927, 0.0567, 0.9555, -0.5115]  (128 dims)\n",
            "Token 3 (  or): [0.4272, -0.2658, 0.1673, -0.0494, -0.3090, ..., -0.0564, 0.2699, 0.2222, 0.4069, -0.0401]  (128 dims)\n",
            "Token 4 ( ...): [-0.1101, 0.0363, -0.0446, -0.0269, 0.3741, ..., 0.3131, 0.1933, -0.0790, 0.6762, -0.1910]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4356, 0.3842, 0.4366, -0.0392, -0.6289, ..., 0.3766, 0.0753, -0.0762, 0.2500, 0.2201]  (128 dims)\n",
            "Token 1 ( açò): [-0.1261, 0.2138, -0.1219, 0.1246, 0.2104, ..., 0.2324, 0.5596, 0.0502, 0.3005, -0.1350]  (128 dims)\n",
            "Token 2 (  es): [0.7951, 0.3712, 0.3620, 0.2888, -0.0242, ..., 0.8335, 0.1068, -0.3831, -0.1308, 0.5708]  (128 dims)\n",
            "Token 3 (  or): [-0.6682, -0.6734, -0.6790, -0.5111, -0.3867, ..., 0.5548, 0.0384, -0.0471, 0.0597, 0.4024]  (128 dims)\n",
            "Token 4 ( ...): [0.2483, 0.4944, 0.2735, -0.0251, 0.5204, ..., 0.6874, -0.0749, -0.6521, -0.0194, 0.4921]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3952, -0.0236, -1.0263, -0.1699, -0.1340, ..., -0.0407, -0.4909, 0.6510, -0.6418, 0.1959]  (128 dims)\n",
            "Token 1 ( açò): [0.4655, -0.1644, 0.5330, -0.2112, -1.0051, ..., -0.6447, -0.9231, 0.4158, -0.5965, 0.1688]  (128 dims)\n",
            "Token 2 (  es): [-0.8972, -0.6351, -1.8786, 0.6740, 2.2149, ..., -0.3015, -0.9045, 0.6345, -0.3708, 0.6216]  (128 dims)\n",
            "Token 3 (  or): [-0.0221, 0.1344, 1.6989, -0.1682, -0.7082, ..., -0.3799, -1.0173, 0.7634, -0.1049, 0.6471]  (128 dims)\n",
            "Token 4 ( ...): [-0.0546, -0.1184, -1.7576, -0.1427, -0.0608, ..., 0.2700, -0.1350, 0.5089, 0.2167, 0.3279]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3521, -0.4142, 0.0211, -0.3359, -0.0088, ..., -0.1404, 0.3456, 0.1993, -0.1943, -0.0363]  (128 dims)\n",
            "Token 1 ( açò): [0.3861, -0.5999, 0.4485, 0.1675, -0.1348, ..., -0.2342, 0.1050, 0.2735, -0.1040, -0.2153]  (128 dims)\n",
            "Token 2 (  es): [-0.1909, 0.5919, -0.3370, -0.8841, 0.1125, ..., -0.1782, -0.2320, 0.0280, -0.0639, -0.6009]  (128 dims)\n",
            "Token 3 (  or): [0.1286, -0.2518, 0.5868, 0.2489, -0.6469, ..., 0.0425, -0.0710, -0.0715, -0.4833, -0.5340]  (128 dims)\n",
            "Token 4 ( ...): [-0.4909, 0.7146, -0.2031, 0.1951, -0.5548, ..., 0.4684, -0.4117, -0.3180, -0.9572, -0.7467]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5972, -0.9489, 0.6698, -0.5446, -0.0196, ..., 0.5141, -0.4475, 0.1800, -0.7482, -0.2691]  (128 dims)\n",
            "Token 1 ( açò): [-0.2298, 0.0815, -0.4322, -0.1390, 0.4386, ..., -0.4145, -0.9009, 0.6052, -0.1834, 0.2828]  (128 dims)\n",
            "Token 2 (  es): [0.7328, -0.7330, 0.4384, 0.3690, 0.3225, ..., 0.4342, -0.0287, 0.5014, -0.2085, -0.0126]  (128 dims)\n",
            "Token 3 (  or): [-1.1316, 1.5047, -1.3762, -0.3298, 0.4305, ..., 0.4012, 0.3921, 0.2512, -0.5374, 0.0454]  (128 dims)\n",
            "Token 4 ( ...): [0.3708, -0.8788, 0.4431, 0.7906, 0.2628, ..., 0.0450, -0.2284, 1.5338, 0.7457, -0.3762]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.3898, 0.5608, -0.3478, -0.7937, -0.1874, ..., -6.1992, 0.7628, -0.9464, -2.9338, -1.4260]  (128 dims)\n",
            "Token 1 ( açò): [-0.6673, -1.0153, 0.2218, 0.3894, 0.6455, ..., -4.9787, 0.5735, 0.2173, -0.8115, -1.6427]  (128 dims)\n",
            "Token 2 (  es): [0.3415, -0.3062, -0.5262, -0.4598, -0.7543, ..., -1.7644, 1.4710, 0.4276, 1.0407, 0.2479]  (128 dims)\n",
            "Token 3 (  or): [-0.5770, -0.2307, 0.7096, -0.1202, 0.6154, ..., -2.3003, 1.8679, 0.0037, 0.7812, -0.0634]  (128 dims)\n",
            "Token 4 ( ...): [-0.0327, -0.2048, -0.5370, -0.0723, -0.6052, ..., -3.0527, 1.5606, -0.6601, -0.8882, -1.0182]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0608, 0.0167, -0.1336, -0.4315, 0.2209, ..., -0.0606, -0.7908, -0.1831, -0.5492, -0.3523]  (128 dims)\n",
            "Token 1 ( açò): [-0.5180, 0.3226, -0.0726, -0.3531, -0.1277, ..., -0.0856, -0.3392, -0.4314, -0.3726, 0.0073]  (128 dims)\n",
            "Token 2 (  es): [0.2565, -0.3319, -0.2928, -0.9512, -1.2323, ..., 0.3268, -0.0858, -0.2757, -0.7710, 0.2864]  (128 dims)\n",
            "Token 3 (  or): [-0.7342, 0.4365, 0.2097, 0.9972, 0.6842, ..., 0.1171, -0.3086, -0.2630, -0.3489, 0.0648]  (128 dims)\n",
            "Token 4 ( ...): [1.0788, -0.7635, -0.2226, -0.8765, 0.1732, ..., 0.7332, -0.2393, -0.6599, -0.5926, 0.5154]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0172, -0.2092, -0.3587, 0.3502, -0.2233, ..., 0.5501, -0.1378, -1.5415, 1.7725, 0.4384]  (128 dims)\n",
            "Token 1 ( açò): [-0.4731, -0.5063, 0.3211, -0.5662, 0.3967, ..., 0.6026, 0.3417, -1.1409, 1.8243, 0.3757]  (128 dims)\n",
            "Token 2 (  es): [0.3963, 1.2535, -1.3616, 0.2631, -0.8149, ..., -0.2083, 0.0482, -1.5576, 2.3082, 1.9916]  (128 dims)\n",
            "Token 3 (  or): [0.3645, -0.6735, 0.7016, -0.4345, -0.0705, ..., 0.1167, 0.2951, -2.1790, 2.6701, 1.0848]  (128 dims)\n",
            "Token 4 ( ...): [-0.6157, 0.7128, -0.6992, 0.5642, 0.4343, ..., -0.3786, 0.7434, -3.0881, 3.3234, 1.0332]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.6629, 0.4068, -0.1787, -1.1420, -0.5572, ..., -0.5138, 0.3562, 1.0724, 0.4084, 0.2699]  (128 dims)\n",
            "Token 1 ( açò): [-0.3005, -0.2346, 1.0412, 1.4295, -0.0230, ..., -0.4145, 0.2586, 0.6176, 0.4214, 0.2792]  (128 dims)\n",
            "Token 2 (  es): [0.8212, 0.9099, 0.0385, -1.2518, 1.2982, ..., -0.4150, 0.4176, 0.5711, 0.3691, 0.4123]  (128 dims)\n",
            "Token 3 (  or): [1.2070, -0.2959, -0.3636, 0.2568, -0.9351, ..., -0.6966, 0.7172, 0.8284, 0.6971, 0.6879]  (128 dims)\n",
            "Token 4 ( ...): [-1.1720, -0.6635, -0.0240, 0.3582, 0.1363, ..., -0.3872, 0.3628, 0.8055, 0.2949, 0.3160]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1084, -0.6354, 0.1776, -0.3193, 0.2784, ..., -0.5032, 0.6485, 0.9932, -0.7111, 0.6745]  (128 dims)\n",
            "Token 1 ( açò): [-0.2733, 0.0306, -0.3371, 0.5749, -0.4432, ..., -0.2288, 0.0737, 1.2006, -0.9586, 0.8340]  (128 dims)\n",
            "Token 2 (  es): [0.4464, -0.3278, 0.0376, -0.3283, 0.6782, ..., -0.5068, 0.3855, 0.5419, -0.9952, 0.9043]  (128 dims)\n",
            "Token 3 (  or): [-0.0486, -0.2113, -0.3776, -0.0247, -0.4162, ..., 0.2442, 0.3383, 0.0379, -0.6897, 0.6517]  (128 dims)\n",
            "Token 4 ( ...): [-0.2624, 0.2730, -0.3187, 0.0242, -0.6024, ..., 0.1082, 1.3626, 0.3655, -1.2784, 1.2904]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0436, 0.1843, -0.1697, 0.2867, -0.0651, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (128 dims)\n",
            "Token 1 ( açò): [0.2625, -0.2437, 0.2955, 0.0787, 0.1374, ..., 0.0736, -0.0681, 0.6089, 0.3940, 0.7308]  (128 dims)\n",
            "Token 2 (  es): [0.0601, -0.0625, -0.6281, 0.5323, 0.2201, ..., -0.1933, -0.4057, 0.4847, 0.9464, 0.8628]  (128 dims)\n",
            "Token 3 (  or): [0.0729, -0.0511, 0.1755, -0.6256, 0.2138, ..., -0.2485, -0.3816, 0.9384, 0.8779, 0.7314]  (128 dims)\n",
            "Token 4 ( ...): [0.0388, 0.1500, -0.2517, 0.1334, -0.4867, ..., -0.0022, -0.2755, 0.7693, 0.5681, 0.7157]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K_heads = K_rope_reshaped.transpose(0, 1)\n",
        "\n",
        "print(f\"Q después de RoPE shape: {K_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = K_heads[head_idx, token_idx, :]\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjZsGWl0dCh9",
        "outputId": "2e71de34-340d-48eb-f40c-3e953facb2a0"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q después de RoPE shape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., 0.8779, 0.0834, -0.0709, 0.5904, -0.6143]  (128 dims)\n",
            "Token 1 ( açò): [-0.1016, 0.2246, -0.0739, 0.8160, 0.5713, ..., 1.1051, -1.9500, -0.2813, 0.2008, -0.7693]  (128 dims)\n",
            "Token 2 (  es): [-1.3314, -1.8256, 0.5817, -0.1037, -0.5556, ..., 0.7090, -0.9583, 0.0256, 0.4607, 0.7339]  (128 dims)\n",
            "Token 3 (  or): [0.8912, 1.9277, -0.2274, -0.6683, -0.0046, ..., 1.1949, -0.9730, 0.8337, 0.8412, 0.6157]  (128 dims)\n",
            "Token 4 ( ...): [-0.6050, -2.8337, -0.1328, 2.6048, -4.1525, ..., 1.2153, -0.5793, -1.4445, 2.4347, -0.0150]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4833, -0.3272, 0.1366, -0.1066, -0.4100, ..., 1.3725, 1.4919, -0.0843, -1.8912, 0.7427]  (128 dims)\n",
            "Token 1 ( açò): [0.5312, 0.8743, 0.1248, -0.0281, 0.4151, ..., -0.5029, 0.4558, 0.9583, -0.0562, 1.7710]  (128 dims)\n",
            "Token 2 (  es): [0.0372, 0.4510, 0.0066, 0.0108, 0.2158, ..., 0.2197, 0.0143, 1.3846, -0.4279, 1.6441]  (128 dims)\n",
            "Token 3 (  or): [0.1103, -0.0854, -0.1406, -0.4540, 0.0804, ..., 0.8813, -0.0961, 2.2490, 0.4245, 0.5848]  (128 dims)\n",
            "Token 4 ( ...): [0.3241, -0.2392, -0.5368, 0.1509, 0.1352, ..., 1.2765, 0.1822, 0.8957, -0.6264, 1.9425]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [1.6238, -2.1105, -0.3979, 0.6275, 0.1373, ..., 0.2037, 1.9504, -0.6677, 2.1918, -1.5709]  (128 dims)\n",
            "Token 1 ( açò): [-0.2451, 0.7183, 0.2188, -0.0430, -0.9188, ..., -0.4237, -0.8903, 0.4161, -2.4319, 0.4101]  (128 dims)\n",
            "Token 2 (  es): [-1.3096, -0.2214, 0.7616, 1.2296, 0.7686, ..., -0.0837, 0.4272, 2.3719, 0.0050, -0.1883]  (128 dims)\n",
            "Token 3 (  or): [-0.0624, -0.1482, 0.2155, 0.2019, 0.4139, ..., -0.6681, 0.5824, 0.2608, 0.0928, 1.8671]  (128 dims)\n",
            "Token 4 ( ...): [-0.5049, -0.7794, 0.8232, -0.2226, 0.0146, ..., -0.6867, 1.1887, 1.8938, 1.8025, 0.8327]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0515, 0.0201, -0.0272, 0.0114, 0.0579, ..., -2.8270, 2.8352, 2.2631, 2.7978, 2.7694]  (128 dims)\n",
            "Token 1 ( açò): [0.6304, 0.6898, -0.1606, 0.6275, 0.9082, ..., -0.3249, 0.2405, -0.6897, 0.1076, 0.0463]  (128 dims)\n",
            "Token 2 (  es): [-0.0661, -0.0983, 0.4547, -0.7227, -0.1320, ..., -1.1486, 1.0429, 0.6906, 0.9707, 0.9594]  (128 dims)\n",
            "Token 3 (  or): [0.3118, 0.4632, -0.7506, 0.7282, -0.0390, ..., -0.8657, 0.8484, 0.1496, 0.8096, 0.7892]  (128 dims)\n",
            "Token 4 ( ...): [0.2246, -0.3837, 0.2841, -0.1582, -0.4299, ..., -0.4286, 0.2719, -1.1595, 0.0955, 0.0393]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1220, 0.3785, -0.1131, -0.5678, 0.6471, ..., 0.2490, 0.8485, 1.3179, -1.8046, 1.0987]  (128 dims)\n",
            "Token 1 ( açò): [0.1914, -0.2421, 0.4797, 0.0315, 0.2931, ..., -1.1763, -0.5668, -1.1389, 1.0695, -0.8755]  (128 dims)\n",
            "Token 2 (  es): [-0.0477, 0.0234, -0.5137, 0.6056, -0.4572, ..., 1.3647, -0.8588, -0.4872, 0.1388, -1.2541]  (128 dims)\n",
            "Token 3 (  or): [0.7227, -0.3674, 0.4878, -0.4236, 0.2761, ..., 1.2733, -1.0216, -0.7051, 0.7682, -1.5164]  (128 dims)\n",
            "Token 4 ( ...): [0.0754, 0.2113, -0.1464, 0.3637, 0.2135, ..., 0.8532, -1.7782, -1.0842, -0.0238, -1.9262]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7487, 0.6855, 0.4504, 0.3863, -0.1420, ..., -2.3786, -0.9175, -1.4701, 0.3072, 0.4352]  (128 dims)\n",
            "Token 1 ( açò): [0.0028, -0.0528, -0.4741, -0.2245, -0.2700, ..., -0.3987, -1.4212, 0.6782, 0.6851, 0.0632]  (128 dims)\n",
            "Token 2 (  es): [0.5912, 0.0474, 0.1207, 0.2126, 0.0724, ..., -0.4594, -1.3940, 1.4630, 0.1750, -1.7183]  (128 dims)\n",
            "Token 3 (  or): [-0.5854, -0.2852, -0.6361, -0.0235, 0.0863, ..., -0.2120, -3.4975, 0.7702, 0.8716, 2.8818]  (128 dims)\n",
            "Token 4 ( ...): [0.3332, 0.0520, 0.2143, 0.4009, -0.2760, ..., 0.4657, -1.7529, 0.9540, -0.2581, -2.1383]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5317, 0.1277, 0.2778, 0.0549, -0.3009, ..., 1.0020, -0.5405, -1.2072, -0.0631, 1.1323]  (128 dims)\n",
            "Token 1 ( açò): [0.0345, 0.1599, 0.2010, 0.5531, 0.1546, ..., 0.0743, 2.3088, -0.3130, 1.0311, -0.5152]  (128 dims)\n",
            "Token 2 (  es): [0.1380, -0.0725, 0.3443, 0.3626, -0.1731, ..., -0.3434, 0.4526, -0.5516, 0.8111, -0.5100]  (128 dims)\n",
            "Token 3 (  or): [-0.0956, -0.2161, -0.5929, -0.4264, -0.1226, ..., -0.5480, 1.8225, -1.8400, 1.3272, -0.4146]  (128 dims)\n",
            "Token 4 ( ...): [0.1354, 0.0285, 0.9298, 0.3653, 0.7767, ..., -0.4265, 0.2204, -1.4258, 1.4335, -0.1583]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0247, -0.3167, -0.0929, -0.2560, -0.1773, ..., -0.7128, -2.8245, 1.3088, -1.4605, 1.0619]  (128 dims)\n",
            "Token 1 ( açò): [0.2122, 0.2962, 0.3352, -0.6600, 0.3573, ..., 0.3200, -0.3037, -0.0852, 0.6982, 0.7971]  (128 dims)\n",
            "Token 2 (  es): [-0.6828, -2.3507, 0.0626, 1.1933, 0.3841, ..., 0.9818, -1.9419, -0.1737, 1.4250, 0.3966]  (128 dims)\n",
            "Token 3 (  or): [0.6072, 0.8552, 0.1151, -0.1495, -0.0570, ..., -0.4545, -1.2716, 1.0400, 1.5095, -0.1437]  (128 dims)\n",
            "Token 4 ( ...): [-0.8149, -0.4780, -0.8051, -0.0399, -0.5052, ..., -0.9461, -0.3107, -1.6995, 0.8716, -0.1181]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1430, -0.2671, 0.2044, -0.0357, 0.1421, ..., -0.2843, 0.7301, 0.2891, 0.4284, 0.0749]  (128 dims)\n",
            "Token 1 ( açò): [-0.0519, 0.0675, 0.0634, -0.0093, 0.0689, ..., -0.2268, 0.8002, 0.7047, 0.8372, 0.0577]  (128 dims)\n",
            "Token 2 (  es): [-0.0596, -0.2477, 0.5313, 0.0864, -0.2184, ..., -0.0497, -1.0718, -0.1084, 1.4080, -0.8774]  (128 dims)\n",
            "Token 3 (  or): [0.2037, 0.1340, -0.4119, -0.0044, 0.1656, ..., 0.4834, -0.8425, -0.8408, 0.5783, -0.5453]  (128 dims)\n",
            "Token 4 ( ...): [0.1863, -0.1383, 0.1654, -0.4510, -0.2670, ..., 0.5292, -1.4703, -1.2482, 1.9447, -0.0659]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8143, -1.2586, 0.4210, -1.8082, -2.6416, ..., -0.3086, 0.7203, 0.4455, 0.2583, -0.7831]  (128 dims)\n",
            "Token 1 ( açò): [-0.4242, 0.4227, 0.0699, -0.1361, 0.5490, ..., -0.8333, 0.1749, -0.5771, 0.7942, -0.8797]  (128 dims)\n",
            "Token 2 (  es): [0.6524, -0.1383, -0.2649, -0.8973, -0.3629, ..., -1.1023, -0.4354, -0.2108, -0.3338, -0.0032]  (128 dims)\n",
            "Token 3 (  or): [-0.0906, 0.6085, -0.4664, 0.1581, -1.0070, ..., -1.3722, -0.4048, 0.3373, -0.2194, 0.1047]  (128 dims)\n",
            "Token 4 ( ...): [-1.0321, 0.3380, 2.1548, 1.1586, 3.0817, ..., -0.0782, 0.4186, 0.1855, 0.3351, -1.8325]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1141, 0.0147, 0.2306, 0.0312, -0.1102, ..., -0.4191, 1.2161, 0.5313, 0.9569, -0.1863]  (128 dims)\n",
            "Token 1 ( açò): [-0.0626, -0.1629, -0.0357, 0.1074, -0.0703, ..., 0.5938, 0.8049, 1.2462, 2.1894, 0.4079]  (128 dims)\n",
            "Token 2 (  es): [0.6054, 0.2522, -0.9695, -0.2041, -0.8891, ..., 1.2149, -0.5078, 1.8112, 1.9427, 1.8350]  (128 dims)\n",
            "Token 3 (  or): [-0.1481, 0.0852, 0.9688, 0.1043, 0.3827, ..., 0.9909, -0.7568, 1.6146, 1.2028, 2.4526]  (128 dims)\n",
            "Token 4 ( ...): [-0.2215, 0.0372, -0.2650, 0.2819, 0.3325, ..., 0.5849, 1.1333, 0.9238, 1.0887, 2.2401]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.2609, -0.2117, 0.4495, 0.1503, 0.2674, ..., -0.4155, -0.3622, -0.2970, -0.0813, -0.1502]  (128 dims)\n",
            "Token 1 ( açò): [0.2257, -0.4350, 0.1395, 0.2924, 0.4226, ..., -0.2560, -0.0121, 0.1954, 0.1556, 1.6544]  (128 dims)\n",
            "Token 2 (  es): [-0.3313, -0.2240, 0.1297, -0.1254, 0.3050, ..., 0.0770, -0.0906, 0.6055, -0.4251, 2.1312]  (128 dims)\n",
            "Token 3 (  or): [0.0777, -0.3345, -0.5978, 0.1790, 0.1033, ..., -2.0161, 0.0425, 0.3927, 0.4521, 1.5922]  (128 dims)\n",
            "Token 4 ( ...): [-0.4585, 0.7728, 0.6825, 0.5927, -0.1042, ..., -1.1302, 0.4805, 0.9536, 0.3927, 1.3267]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1495, 0.2181, -0.2070, -0.1809, -0.1314, ..., 1.1907, -2.8027, -0.6817, 0.5220, -0.8140]  (128 dims)\n",
            "Token 1 ( açò): [-0.9060, -0.6845, 0.6847, 0.6593, 0.6486, ..., 1.5745, -2.6731, -0.8837, 0.7214, -1.0012]  (128 dims)\n",
            "Token 2 (  es): [0.6419, 0.7360, -0.8159, 0.6620, -0.2170, ..., 0.6489, -0.5654, 0.5717, -0.7819, -0.5677]  (128 dims)\n",
            "Token 3 (  or): [-0.0679, -0.6155, 0.8432, -0.8017, -0.1340, ..., -0.1520, -2.8569, -0.0696, -0.3681, -0.7334]  (128 dims)\n",
            "Token 4 ( ...): [-0.4549, 1.8215, -1.4481, 1.7549, 2.1875, ..., -0.1054, -1.9019, 0.3074, -0.5237, -0.6556]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4624, -0.1438, 0.5297, 0.3217, 0.5069, ..., -0.7191, 0.7023, 0.2106, 0.3088, 0.4109]  (128 dims)\n",
            "Token 1 ( açò): [-0.1317, -0.0174, -0.8673, -0.8122, -0.3214, ..., 0.1797, -0.2020, -0.4274, -0.9015, -0.8953]  (128 dims)\n",
            "Token 2 (  es): [0.3496, 0.1353, -0.1683, -0.2381, 0.7356, ..., -2.0417, 2.2300, 0.2500, 1.2086, 1.9691]  (128 dims)\n",
            "Token 3 (  or): [0.3565, 0.1333, -0.2500, 0.1146, -0.6161, ..., -2.0303, 2.0644, 0.3191, 1.2834, 1.7251]  (128 dims)\n",
            "Token 4 ( ...): [-1.0352, -0.3384, 0.2985, 0.0436, -0.4783, ..., -1.4647, 1.4464, -0.5570, 0.1502, 0.6561]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2740, -0.0506, 0.4008, -0.1798, 0.0627, ..., 1.1809, -0.2412, -0.8631, -0.6208, 0.6555]  (128 dims)\n",
            "Token 1 ( açò): [-0.1912, 0.1285, 0.0406, 0.2929, -0.0425, ..., -0.2111, -0.6323, 0.8501, 0.3801, -0.4111]  (128 dims)\n",
            "Token 2 (  es): [0.1033, -0.0220, 0.0327, -0.4313, 0.2558, ..., 1.0503, -0.8605, -1.1373, 1.0758, -1.0259]  (128 dims)\n",
            "Token 3 (  or): [0.3152, 0.0959, -0.1321, 0.4906, -0.3217, ..., 0.3542, -1.6046, -0.4638, 0.3473, -0.3991]  (128 dims)\n",
            "Token 4 ( ...): [-0.3863, -0.0211, 0.0948, -0.1853, -0.0408, ..., 1.9919, -1.7386, -1.2319, 0.6787, -0.6202]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8710, -1.0364, -0.1889, -0.3356, 0.0606, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (128 dims)\n",
            "Token 1 ( açò): [0.1092, 0.3405, -0.1620, -0.3379, -0.1355, ..., -0.0360, -0.6338, -0.0742, 0.4613, 0.5065]  (128 dims)\n",
            "Token 2 (  es): [0.1942, 0.0635, 0.0911, 0.3123, 0.2469, ..., -1.6743, -0.4812, -0.7509, -0.1428, 0.1033]  (128 dims)\n",
            "Token 3 (  or): [-0.0284, 0.0153, -0.1884, -0.1263, -0.1753, ..., -2.2731, -0.1676, -0.7398, -0.2897, -0.0441]  (128 dims)\n",
            "Token 4 ( ...): [0.2155, -0.5425, -0.4284, 0.5324, -0.1978, ..., -1.8042, -0.8629, -1.1168, -1.0369, -0.5487]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcular scores Q @ K.T"
      ],
      "metadata": {
        "id": "MT7P0uQmUV3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada head, multiplicar Q por K transpuesta\n",
        "attention_scores = Q_heads @ K_heads.transpose(-2, -1)  # (16, 5, 5)\n",
        "\n",
        "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]  (5 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "ea04cJYtyQjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b693dda-c473-495d-eac3-950a0d8b47c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [57.2693, 4.0844, -9.4224, -14.8397, -18.5172]  (5 dims)\n",
            "Token 1( açò): [62.3817, 2.9718, 18.5524, 24.2383, 6.4989]  (5 dims)\n",
            "Token 2(  es): [73.3409, 44.6725, 23.7828, 36.9749, 5.7986]  (5 dims)\n",
            "Token 3(  or): [71.0612, 24.8639, 19.2836, 28.0697, -9.0856]  (5 dims)\n",
            "Token 4( ...): [68.0888, 40.8990, 25.5791, 39.4041, -0.0167]  (5 dims)\n",
            "\n",
            "HEAD 1  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [89.0011, 58.2322, 45.6948, 52.0309, 51.2896]  (5 dims)\n",
            "Token 1( açò): [58.9285, 49.7864, 43.2922, 48.4167, 42.8550]  (5 dims)\n",
            "Token 2(  es): [66.4509, 61.2570, 52.2892, 54.9544, 46.1953]  (5 dims)\n",
            "Token 3(  or): [62.0267, 56.2284, 45.5185, 49.4880, 44.3160]  (5 dims)\n",
            "Token 4( ...): [87.7182, 68.5692, 52.6794, 61.2063, 52.9171]  (5 dims)\n",
            "\n",
            "HEAD 2  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0198, 47.3683, 35.5929, 34.7480, 33.3485]  (5 dims)\n",
            "Token 1( açò): [57.2958, 44.6003, 40.5282, 37.6567, 36.9815]  (5 dims)\n",
            "Token 2(  es): [68.4102, 62.7087, 57.4797, 45.3387, 50.8343]  (5 dims)\n",
            "Token 3(  or): [60.1698, 64.4240, 59.7353, 43.9086, 47.5078]  (5 dims)\n",
            "Token 4( ...): [75.3204, 74.0881, 70.7314, 55.2754, 56.7187]  (5 dims)\n",
            "\n",
            "HEAD 3  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [192.7018, 46.2175, 54.1872, 42.5847, 24.8486]  (5 dims)\n",
            "Token 1( açò): [174.6678, 56.0152, 39.9743, 38.5285, 32.0594]  (5 dims)\n",
            "Token 2(  es): [146.3916, 49.8872, 42.6596, 39.3752, 31.2299]  (5 dims)\n",
            "Token 3(  or): [147.4012, 49.5959, 37.0693, 31.4962, 30.4229]  (5 dims)\n",
            "Token 4( ...): [158.6954, 52.8505, 36.6701, 39.6083, 27.7998]  (5 dims)\n",
            "\n",
            "HEAD 4  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [19.3079, 15.5362, 11.3888, 6.9574, 8.4965]  (5 dims)\n",
            "Token 1( açò): [25.8710, 17.9020, 15.7039, 19.2054, 13.7928]  (5 dims)\n",
            "Token 2(  es): [27.9442, 11.2421, 12.4854, 12.5371, 5.1395]  (5 dims)\n",
            "Token 3(  or): [32.3935, 14.0204, 16.7815, 13.3945, 11.3831]  (5 dims)\n",
            "Token 4( ...): [37.5264, 24.0449, 25.9515, 25.2607, 16.1474]  (5 dims)\n",
            "\n",
            "HEAD 5  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [64.3610, 43.4759, 35.2100, 49.5548, 36.7112]  (5 dims)\n",
            "Token 1( açò): [70.2455, 50.5220, 46.6491, 51.6754, 47.0283]  (5 dims)\n",
            "Token 2(  es): [67.1061, 58.8277, 53.9371, 62.2031, 52.6380]  (5 dims)\n",
            "Token 3(  or): [64.9627, 49.3161, 41.0803, 49.8743, 41.8471]  (5 dims)\n",
            "Token 4( ...): [83.5496, 64.4049, 59.8927, 72.9963, 60.2403]  (5 dims)\n",
            "\n",
            "HEAD 6  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [31.0864, 19.0140, 19.1544, 24.2765, 24.0765]  (5 dims)\n",
            "Token 1( açò): [27.9635, 34.9156, 33.5089, 39.3130, 32.0312]  (5 dims)\n",
            "Token 2(  es): [43.5563, 31.5193, 37.2235, 40.0899, 37.3651]  (5 dims)\n",
            "Token 3(  or): [38.7311, 30.8874, 33.7755, 40.6773, 36.2215]  (5 dims)\n",
            "Token 4( ...): [61.2094, 40.7339, 47.6118, 53.4397, 50.0786]  (5 dims)\n",
            "\n",
            "HEAD 7  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [98.8943, 46.8317, 37.2407, 48.1962, 43.5809]  (5 dims)\n",
            "Token 1( açò): [89.5423, 65.6321, 45.2683, 50.6553, 53.0234]  (5 dims)\n",
            "Token 2(  es): [99.8503, 124.9106, 75.7443, 62.0599, 62.6455]  (5 dims)\n",
            "Token 3(  or): [98.1324, 81.1627, 49.1447, 50.1396, 45.9536]  (5 dims)\n",
            "Token 4( ...): [137.2454, 85.5963, 59.1651, 73.1376, 68.1102]  (5 dims)\n",
            "\n",
            "HEAD 8  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [62.6750, 31.7756, 22.9188, 20.0483, 21.0756]  (5 dims)\n",
            "Token 1( açò): [57.1317, 33.5090, 27.0342, 26.2528, 22.1907]  (5 dims)\n",
            "Token 2(  es): [58.3787, 29.3589, 27.8470, 26.3126, 24.3434]  (5 dims)\n",
            "Token 3(  or): [59.9496, 29.0159, 26.6046, 21.5567, 24.2451]  (5 dims)\n",
            "Token 4( ...): [73.8823, 37.5890, 37.1205, 32.9069, 41.4608]  (5 dims)\n",
            "\n",
            "HEAD 9  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0327, 37.0048, 41.2929, 44.1884, 37.8114]  (5 dims)\n",
            "Token 1( açò): [34.2015, 32.3804, 34.7513, 45.7432, 17.6369]  (5 dims)\n",
            "Token 2(  es): [60.5970, 35.5433, 39.9387, 47.4170, 33.9813]  (5 dims)\n",
            "Token 3(  or): [37.4668, 32.6115, 30.9289, 31.8253, 9.1577]  (5 dims)\n",
            "Token 4( ...): [68.7625, 23.6547, 25.2738, 14.5654, 29.2106]  (5 dims)\n",
            "\n",
            "HEAD 10  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [114.2219, -18.4964, -20.9610, -5.2164, 7.9187]  (5 dims)\n",
            "Token 1( açò): [130.4529, 15.6294, 18.5961, 15.1015, 5.4374]  (5 dims)\n",
            "Token 2(  es): [137.7183, 31.8623, 38.6148, 43.5129, 22.7078]  (5 dims)\n",
            "Token 3(  or): [136.0320, 31.7640, 35.1223, 35.6799, 22.7088]  (5 dims)\n",
            "Token 4( ...): [128.5079, 18.5697, 10.0177, 25.0371, 1.2952]  (5 dims)\n",
            "\n",
            "HEAD 11  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [30.0089, 23.0851, 26.9385, 22.2061, 24.4449]  (5 dims)\n",
            "Token 1( açò): [30.7042, 35.4326, 38.1047, 36.0789, 37.9300]  (5 dims)\n",
            "Token 2(  es): [42.8850, 50.6712, 44.9214, 43.1952, 32.4115]  (5 dims)\n",
            "Token 3(  or): [38.7679, 44.4305, 39.5565, 35.6568, 27.0812]  (5 dims)\n",
            "Token 4( ...): [53.8817, 59.8622, 58.0731, 48.4317, 45.8525]  (5 dims)\n",
            "\n",
            "HEAD 12  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [78.7889, 65.2274, 51.6862, 57.9437, 59.6320]  (5 dims)\n",
            "Token 1( açò): [75.8869, 70.8005, 57.3812, 64.7267, 64.6501]  (5 dims)\n",
            "Token 2(  es): [55.9906, 60.2760, 46.0080, 49.3511, 54.0680]  (5 dims)\n",
            "Token 3(  or): [68.3117, 61.6951, 52.9362, 53.0188, 53.8306]  (5 dims)\n",
            "Token 4( ...): [59.3584, 54.0663, 40.7564, 49.3788, 36.4958]  (5 dims)\n",
            "\n",
            "HEAD 13  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.1164, -35.8433, 50.0453, 53.2557, 48.7467]  (5 dims)\n",
            "Token 1( açò): [6.5927, -45.6459, 48.2167, 54.3720, 53.5238]  (5 dims)\n",
            "Token 2(  es): [6.7477, -34.8079, 54.7064, 56.0127, 57.5352]  (5 dims)\n",
            "Token 3(  or): [3.9585, -46.9168, 61.6459, 61.5716, 63.9329]  (5 dims)\n",
            "Token 4( ...): [12.0720, -34.1936, 53.2774, 57.5709, 57.6692]  (5 dims)\n",
            "\n",
            "HEAD 14  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [71.2615, 43.5506, 37.6230, 32.6451, 33.7035]  (5 dims)\n",
            "Token 1( açò): [74.2994, 52.6590, 43.6845, 43.1587, 46.8193]  (5 dims)\n",
            "Token 2(  es): [88.3702, 53.7185, 47.9821, 41.5071, 50.2896]  (5 dims)\n",
            "Token 3(  or): [80.9951, 47.4701, 48.4800, 28.5561, 47.0015]  (5 dims)\n",
            "Token 4( ...): [93.2179, 54.1828, 49.1582, 41.3337, 53.4734]  (5 dims)\n",
            "\n",
            "HEAD 15  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [46.4097, 34.3569, 28.5577, 29.9083, 30.6156]  (5 dims)\n",
            "Token 1( açò): [67.0418, 47.6794, 41.7038, 41.7728, 45.7506]  (5 dims)\n",
            "Token 2(  es): [72.9953, 57.8119, 53.9741, 49.8576, 53.5584]  (5 dims)\n",
            "Token 3(  or): [63.6866, 48.8952, 45.1899, 43.9889, 44.5126]  (5 dims)\n",
            "Token 4( ...): [79.3272, 57.0103, 54.9858, 50.6597, 52.4121]  (5 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicar máscara causal"
      ],
      "metadata": {
        "id": "4i8DuK1OUc2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos una máscara a los attention scores para que los tokens no tengan información de los tokens siguientes.\n",
        "\n",
        "Así cada secuencia sirve para entrenar el modelo a predecir el siguiente token (sin hacer trampa viendo el futuro)."
      ],
      "metadata": {
        "id": "XAgpSCi013l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear máscara triangular inferior (True = permitido, False = bloqueado)\n",
        "causal_mask = torch.tril(torch.ones(5, 5)).bool()\n",
        "\n",
        "print(\"Máscara causal (5×5):\")\n",
        "print(causal_mask.int())  # 1 = puede atender, 0 = bloqueado"
      ],
      "metadata": {
        "id": "6MV1eAbWuRJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a004f60-9dd8-4270-cf92-53abb734c191"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Máscara causal (5×5):\n",
            "tensor([[1, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ponemos `-inf` donde no queremos que el token atienda. Tras el softmax que haremos después, `-inf` se convierte en 0 (peso nulo)."
      ],
      "metadata": {
        "id": "7H2NHTaX3sbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar máscara: poner -inf donde no puede atender\n",
        "attention_scores_masked = attention_scores.masked_fill(~causal_mask, float('-inf'))\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_scores_masked.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores_masked[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "21l9-65XyKrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238a2930-ee27-4e9d-8dd1-19de179b61d9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [57.2693, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [62.3817, 2.9718, -inf, -inf, -inf]\n",
            "Token 2(  es): [73.3409, 44.6725, 23.7828, -inf, -inf]\n",
            "Token 3(  or): [71.0612, 24.8639, 19.2836, 28.0697, -inf]\n",
            "Token 4( ...): [68.0888, 40.8990, 25.5791, 39.4041, -0.0167]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [89.0011, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [58.9285, 49.7864, -inf, -inf, -inf]\n",
            "Token 2(  es): [66.4509, 61.2570, 52.2892, -inf, -inf]\n",
            "Token 3(  or): [62.0267, 56.2284, 45.5185, 49.4880, -inf]\n",
            "Token 4( ...): [87.7182, 68.5692, 52.6794, 61.2063, 52.9171]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0198, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [57.2958, 44.6003, -inf, -inf, -inf]\n",
            "Token 2(  es): [68.4102, 62.7087, 57.4797, -inf, -inf]\n",
            "Token 3(  or): [60.1698, 64.4240, 59.7353, 43.9086, -inf]\n",
            "Token 4( ...): [75.3204, 74.0881, 70.7314, 55.2754, 56.7187]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [192.7018, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [174.6678, 56.0152, -inf, -inf, -inf]\n",
            "Token 2(  es): [146.3916, 49.8872, 42.6596, -inf, -inf]\n",
            "Token 3(  or): [147.4012, 49.5959, 37.0693, 31.4962, -inf]\n",
            "Token 4( ...): [158.6954, 52.8505, 36.6701, 39.6083, 27.7998]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [19.3079, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [25.8710, 17.9020, -inf, -inf, -inf]\n",
            "Token 2(  es): [27.9442, 11.2421, 12.4854, -inf, -inf]\n",
            "Token 3(  or): [32.3935, 14.0204, 16.7815, 13.3945, -inf]\n",
            "Token 4( ...): [37.5264, 24.0449, 25.9515, 25.2607, 16.1474]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [64.3610, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [70.2455, 50.5220, -inf, -inf, -inf]\n",
            "Token 2(  es): [67.1061, 58.8277, 53.9371, -inf, -inf]\n",
            "Token 3(  or): [64.9627, 49.3161, 41.0803, 49.8743, -inf]\n",
            "Token 4( ...): [83.5496, 64.4049, 59.8927, 72.9963, 60.2403]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [31.0864, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [27.9635, 34.9156, -inf, -inf, -inf]\n",
            "Token 2(  es): [43.5563, 31.5193, 37.2235, -inf, -inf]\n",
            "Token 3(  or): [38.7311, 30.8874, 33.7755, 40.6773, -inf]\n",
            "Token 4( ...): [61.2094, 40.7339, 47.6118, 53.4397, 50.0786]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [98.8943, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [89.5423, 65.6321, -inf, -inf, -inf]\n",
            "Token 2(  es): [99.8503, 124.9106, 75.7443, -inf, -inf]\n",
            "Token 3(  or): [98.1324, 81.1627, 49.1447, 50.1396, -inf]\n",
            "Token 4( ...): [137.2454, 85.5963, 59.1651, 73.1376, 68.1102]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [62.6750, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [57.1317, 33.5090, -inf, -inf, -inf]\n",
            "Token 2(  es): [58.3787, 29.3589, 27.8470, -inf, -inf]\n",
            "Token 3(  or): [59.9496, 29.0159, 26.6046, 21.5567, -inf]\n",
            "Token 4( ...): [73.8823, 37.5890, 37.1205, 32.9069, 41.4608]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0327, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [34.2015, 32.3804, -inf, -inf, -inf]\n",
            "Token 2(  es): [60.5970, 35.5433, 39.9387, -inf, -inf]\n",
            "Token 3(  or): [37.4668, 32.6115, 30.9289, 31.8253, -inf]\n",
            "Token 4( ...): [68.7625, 23.6547, 25.2738, 14.5654, 29.2106]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [114.2219, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [130.4529, 15.6294, -inf, -inf, -inf]\n",
            "Token 2(  es): [137.7183, 31.8623, 38.6148, -inf, -inf]\n",
            "Token 3(  or): [136.0320, 31.7640, 35.1223, 35.6799, -inf]\n",
            "Token 4( ...): [128.5079, 18.5697, 10.0177, 25.0371, 1.2952]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [30.0089, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [30.7042, 35.4326, -inf, -inf, -inf]\n",
            "Token 2(  es): [42.8850, 50.6712, 44.9214, -inf, -inf]\n",
            "Token 3(  or): [38.7679, 44.4305, 39.5565, 35.6568, -inf]\n",
            "Token 4( ...): [53.8817, 59.8622, 58.0731, 48.4317, 45.8525]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [78.7889, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [75.8869, 70.8005, -inf, -inf, -inf]\n",
            "Token 2(  es): [55.9906, 60.2760, 46.0080, -inf, -inf]\n",
            "Token 3(  or): [68.3117, 61.6951, 52.9362, 53.0188, -inf]\n",
            "Token 4( ...): [59.3584, 54.0663, 40.7564, 49.3788, 36.4958]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.1164, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.5927, -45.6459, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.7477, -34.8079, 54.7064, -inf, -inf]\n",
            "Token 3(  or): [3.9585, -46.9168, 61.6459, 61.5716, -inf]\n",
            "Token 4( ...): [12.0720, -34.1936, 53.2774, 57.5709, 57.6692]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [71.2615, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [74.2994, 52.6590, -inf, -inf, -inf]\n",
            "Token 2(  es): [88.3702, 53.7185, 47.9821, -inf, -inf]\n",
            "Token 3(  or): [80.9951, 47.4701, 48.4800, 28.5561, -inf]\n",
            "Token 4( ...): [93.2179, 54.1828, 49.1582, 41.3337, 53.4734]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [46.4097, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [67.0418, 47.6794, -inf, -inf, -inf]\n",
            "Token 2(  es): [72.9953, 57.8119, 53.9741, -inf, -inf]\n",
            "Token 3(  or): [63.6866, 48.8952, 45.1899, 43.9889, -inf]\n",
            "Token 4( ...): [79.3272, 57.0103, 54.9858, 50.6597, 52.4121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos observar tenemos valores grandes en las matrices (de hasta >100 en head 10).\n",
        "El siguiente paso es un escalado se divididen los scores por √(head_dim) para estabilizar los gradientes. Podemos observar valores mucho más moderados en todos los heads."
      ],
      "metadata": {
        "id": "nt1jri6_3p2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Escalar los scores\n",
        "scale = math.sqrt(head_dim)  # sqrt(128) ≈ 11.31\n",
        "attention_scores_scaled = attention_scores_masked / scale\n",
        "\n",
        "print(f\"Escalado por √{head_dim} = {scale:.2f}\")\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_scores_scaled.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores_scaled[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "5lfUGDYR4Rxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2da2b1b-75d4-4f23-e3f2-8d2f33cd6dd5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Escalado por √128 = 11.31\n",
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.0619, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.5138, 0.2627, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.4825, 3.9485, 2.1021, -inf, -inf]\n",
            "Token 3(  or): [6.2810, 2.1977, 1.7044, 2.4810, -inf]\n",
            "Token 4( ...): [6.0183, 3.6150, 2.2609, 3.4829, -0.0015]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [7.8667, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.2086, 4.4005, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.8735, 5.4144, 4.6218, -inf, -inf]\n",
            "Token 3(  or): [5.4824, 4.9699, 4.0233, 4.3742, -inf]\n",
            "Token 4( ...): [7.7533, 6.0607, 4.6562, 5.4099, 4.6773]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.5096, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.0643, 3.9421, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.0467, 5.5427, 5.0805, -inf, -inf]\n",
            "Token 3(  or): [5.3183, 5.6943, 5.2799, 3.8810, -inf]\n",
            "Token 4( ...): [6.6574, 6.5485, 6.2518, 4.8857, 5.0133]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [17.0326, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [15.4386, 4.9511, -inf, -inf, -inf]\n",
            "Token 2(  es): [12.9393, 4.4094, 3.7706, -inf, -inf]\n",
            "Token 3(  or): [13.0285, 4.3837, 3.2765, 2.7839, -inf]\n",
            "Token 4( ...): [14.0268, 4.6714, 3.2412, 3.5009, 2.4572]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.7066, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.2867, 1.5823, -inf, -inf, -inf]\n",
            "Token 2(  es): [2.4699, 0.9937, 1.1036, -inf, -inf]\n",
            "Token 3(  or): [2.8632, 1.2392, 1.4833, 1.1839, -inf]\n",
            "Token 4( ...): [3.3169, 2.1253, 2.2938, 2.2328, 1.4272]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.6888, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.2089, 4.4656, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.9314, 5.1997, 4.7674, -inf, -inf]\n",
            "Token 3(  or): [5.7419, 4.3590, 3.6310, 4.4083, -inf]\n",
            "Token 4( ...): [7.3848, 5.6926, 5.2938, 6.4520, 5.3245]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [2.7477, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.4716, 3.0861, -inf, -inf, -inf]\n",
            "Token 2(  es): [3.8499, 2.7859, 3.2901, -inf, -inf]\n",
            "Token 3(  or): [3.4234, 2.7301, 2.9854, 3.5954, -inf]\n",
            "Token 4( ...): [5.4102, 3.6004, 4.2083, 4.7234, 4.4264]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [8.7411, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [7.9145, 5.8011, -inf, -inf, -inf]\n",
            "Token 2(  es): [8.8256, 11.0406, 6.6949, -inf, -inf]\n",
            "Token 3(  or): [8.6738, 7.1738, 4.3438, 4.4318, -inf]\n",
            "Token 4( ...): [12.1309, 7.5657, 5.2295, 6.4645, 6.0201]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.5397, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.0498, 2.9618, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.1600, 2.5950, 2.4614, -inf, -inf]\n",
            "Token 3(  or): [5.2989, 2.5647, 2.3515, 1.9054, -inf]\n",
            "Token 4( ...): [6.5303, 3.3224, 3.2810, 2.9086, 3.6647]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.5107, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [3.0230, 2.8620, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.3561, 3.1416, 3.5301, -inf, -inf]\n",
            "Token 3(  or): [3.3116, 2.8825, 2.7338, 2.8130, -inf]\n",
            "Token 4( ...): [6.0778, 2.0908, 2.2339, 1.2874, 2.5819]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.0959, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [11.5305, 1.3815, -inf, -inf, -inf]\n",
            "Token 2(  es): [12.1727, 2.8163, 3.4131, -inf, -inf]\n",
            "Token 3(  or): [12.0236, 2.8076, 3.1044, 3.1537, -inf]\n",
            "Token 4( ...): [11.3586, 1.6413, 0.8854, 2.2130, 0.1145]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [2.6524, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.7139, 3.1318, -inf, -inf, -inf]\n",
            "Token 2(  es): [3.7905, 4.4787, 3.9705, -inf, -inf]\n",
            "Token 3(  or): [3.4266, 3.9271, 3.4963, 3.1516, -inf]\n",
            "Token 4( ...): [4.7625, 5.2911, 5.1330, 4.2808, 4.0528]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [6.9640, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.7075, 6.2579, -inf, -inf, -inf]\n",
            "Token 2(  es): [4.9489, 5.3277, 4.0666, -inf, -inf]\n",
            "Token 3(  or): [6.0380, 5.4531, 4.6789, 4.6862, -inf]\n",
            "Token 4( ...): [5.2466, 4.7788, 3.6024, 4.3645, 3.2258]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.8942, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [0.5827, -4.0346, -inf, -inf, -inf]\n",
            "Token 2(  es): [0.5964, -3.0766, 4.8354, -inf, -inf]\n",
            "Token 3(  or): [0.3499, -4.1469, 5.4488, 5.4422, -inf]\n",
            "Token 4( ...): [1.0670, -3.0223, 4.7091, 5.0886, 5.0973]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [6.2987, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.5672, 4.6544, -inf, -inf, -inf]\n",
            "Token 2(  es): [7.8109, 4.7481, 4.2411, -inf, -inf]\n",
            "Token 3(  or): [7.1590, 4.1958, 4.2851, 2.5240, -inf]\n",
            "Token 4( ...): [8.2394, 4.7891, 4.3450, 3.6534, 4.7264]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.1021, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.9257, 4.2143, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.4519, 5.1099, 4.7707, -inf, -inf]\n",
            "Token 3(  or): [5.6291, 4.3218, 3.9943, 3.8881, -inf]\n",
            "Token 4( ...): [7.0116, 5.0390, 4.8601, 4.4777, 4.6326]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax"
      ],
      "metadata": {
        "id": "wbcuGfqvUlf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la función softmax para transformar los scores en distribuciones de probabilidad.\n",
        "\n",
        "Cada fila representa cuánto atiende ese token a cada posición (suma 1). Los `-inf` se convierten en 0 (no atiende a tokens futuros).\n"
      ],
      "metadata": {
        "id": "3gVcmiOlQvXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar softmax a lo largo de la última dimensión (sobre los keys)\n",
        "attention_weights = torch.softmax(attention_scores_scaled, dim=-1)\n",
        "# se puede acceder también haciendo un forward directo con el parámetro output_attentions = True\n",
        "# outputs = model(\n",
        "    # input_ids=tokens,\n",
        "    # output_attentions=True)\n",
        "\n",
        "# #Acceder a los pesos de atención de la primera capa\n",
        "# attention_weights_from_model = outputs.attentions[0]\n",
        "\n",
        "# print(f\"Attention weights shape: {attention_weights.shape}  # (16, 5, 5)\")\n",
        "# for i in range(16):\n",
        "#     print(f\"\\nHead {i} - Atenciones:\")\n",
        "#     print(attention_weights[i])\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_weights.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_weights[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "KstYHJsD4Thw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2147649-f15f-46d1-c7b4-d0818ae86992"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.9948, 0.0052, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9159, 0.0727, 0.0115, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.9528, 0.0161, 0.0098, 0.0213, 0.0000]\n",
            "Token 4( ...): [0.8365, 0.0756, 0.0195, 0.0663, 0.0020]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.6917, 0.3083, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5214, 0.3295, 0.1491, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.4626, 0.2771, 0.1075, 0.1527, 0.0000]\n",
            "Token 4( ...): [0.7292, 0.1342, 0.0329, 0.0700, 0.0336]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.7544, 0.2456, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5039, 0.3044, 0.1917, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.2735, 0.3983, 0.2632, 0.0650, 0.0000]\n",
            "Token 4( ...): [0.3417, 0.3064, 0.2278, 0.0581, 0.0660]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9997, 0.0002, 0.0001, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.9997, 0.0002, 0.0001, 0.0000, 0.0000]\n",
            "Token 4( ...): [0.9999, 0.0001, 0.0000, 0.0000, 0.0000]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.6692, 0.3308, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.6741, 0.1540, 0.1719, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.6115, 0.1205, 0.1539, 0.1141, 0.0000]\n",
            "Token 4( ...): [0.4646, 0.1411, 0.1670, 0.1571, 0.0702]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8511, 0.1489, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5576, 0.2683, 0.1741, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.6114, 0.1534, 0.0741, 0.1611, 0.0000]\n",
            "Token 4( ...): [0.5469, 0.1007, 0.0676, 0.2152, 0.0697]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.3510, 0.6490, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5218, 0.1801, 0.2981, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.3000, 0.1500, 0.1936, 0.3564, 0.0000]\n",
            "Token 4( ...): [0.4271, 0.0699, 0.1284, 0.2149, 0.1597]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8922, 0.1078, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.0973, 0.8912, 0.0116, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.7996, 0.1784, 0.0105, 0.0115, 0.0000]\n",
            "Token 4( ...): [0.9832, 0.0102, 0.0010, 0.0034, 0.0022]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8897, 0.1103, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.8740, 0.0672, 0.0588, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.8688, 0.0564, 0.0456, 0.0292, 0.0000]\n",
            "Token 4( ...): [0.8599, 0.0348, 0.0334, 0.0230, 0.0490]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.5402, 0.4598, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.7872, 0.0860, 0.1268, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.3547, 0.2309, 0.1990, 0.2154, 0.0000]\n",
            "Token 4( ...): [0.9271, 0.0172, 0.0198, 0.0077, 0.0281]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9998, 0.0001, 0.0002, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.9996, 0.0001, 0.0001, 0.0001, 0.0000]\n",
            "Token 4( ...): [0.9998, 0.0001, 0.0000, 0.0001, 0.0000]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.3970, 0.6030, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.2388, 0.4753, 0.2859, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.2231, 0.3681, 0.2393, 0.1695, 0.0000]\n",
            "Token 4( ...): [0.1903, 0.3229, 0.2757, 0.1176, 0.0936]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.6105, 0.3895, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.3479, 0.5081, 0.1440, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.4824, 0.2688, 0.1239, 0.1248, 0.0000]\n",
            "Token 4( ...): [0.4226, 0.2647, 0.0816, 0.1749, 0.0560]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.9902, 0.0098, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.0142, 0.0004, 0.9854, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.0031, 0.0000, 0.5001, 0.4968, 0.0000]\n",
            "Token 4( ...): [0.0066, 0.0001, 0.2524, 0.3688, 0.3721]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8713, 0.1287, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9303, 0.0435, 0.0262, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.8946, 0.0462, 0.0505, 0.0087, 0.0000]\n",
            "Token 4( ...): [0.9157, 0.0291, 0.0186, 0.0093, 0.0273]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8470, 0.1530, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.6909, 0.1805, 0.1286, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.6094, 0.1649, 0.1188, 0.1069, 0.0000]\n",
            "Token 4( ...): [0.7006, 0.0974, 0.0815, 0.0556, 0.0649]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede observar que cada fila suma 1 por lo que es una distribución de probabilidad. Ejecuta esta celda para obtener una fila aleatoria de entre todos los heads."
      ],
      "metadata": {
        "id": "obcw4dx4E4Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "head_idx = random.randrange(0,16)\n",
        "token_idx = random.randrange(0,5)\n",
        "\n",
        "token_vec = attention_weights[head_idx, token_idx, :]\n",
        "vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "print(f\"Visualización: fila {token_idx} del Head {head_idx} [{vals}]\\n\")\n",
        "print(f\"Verificación: fila 2 del Head 0 suma = {attention_weights[head_idx, token_idx].sum():.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "iKEeVGF-EuhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67782745-2403-4452-bbe3-d36bcb322f57"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualización: fila 2 del Head 3 [0.9997, 0.0002, 0.0001, 0.0000, 0.0000]\n",
            "\n",
            "Verificación: fila 2 del Head 0 suma = 1.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplicar por V"
      ],
      "metadata": {
        "id": "Uxcn8sKlUsQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos ahora por V (Values). Usamos los pesos de atención para hacer una suma ponderada de los valores (V).\n",
        "\n",
        "Para cada token: su output es la combinación de los valores de todos los tokens a los que puede atender, ponderados por los pesos de atención.\n",
        "\n",
        "Shape: (16, 5, 5) @ (16, 5, 128) → (16, 5, 128)"
      ],
      "metadata": {
        "id": "19Uu6b0vCrM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_output = attention_weights @ V_heads  # (16, 5, 5) @ (16, 5, 128) = (16, 5, 128)\n",
        "\n",
        "print(f\"Attention output shape: {attention_output.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_output[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "cHd5mqDkJVTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0bd510-a670-4743-b9cb-39386fb90bb1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention output shape: torch.Size([16, 5, 128])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, -0.0268, 0.0001, -0.0074, -0.0203, -0.0113, -0.0069, 0.0037, 0.0077, -0.0264, 0.0089, 0.0743, -0.0110, 0.0357, -0.0081, -0.0119, 0.0223, -0.0005, -0.0158, -0.0681, -0.0191, 0.0026, -0.0196, -0.0647, 0.0632, 0.0025, 0.0083, -0.0066, 0.0099, 0.0139, 0.0199, 0.0052, -0.0139, -0.0588, -0.0068, -0.0102, -0.0105, -0.0369, 0.0047, -0.0142, 0.0287, -0.0232, -0.0006, -0.0135, -0.0284, 0.0318, -0.0401, -0.0100, 0.0217, 0.0798, -0.0268, -0.0493, 0.0201, 0.0348, 0.0252, 0.0080, -0.0221, -0.0970, -0.0056, 0.0109, -0.0025, 0.0152, -0.0430, 0.0101, -0.0115, -0.0063, -0.0262, -0.0302, 0.0068, 0.0156, 0.0106, -0.0095, -0.0237, 0.0203, 0.0236, -0.0365, 0.0313, -0.0393, -0.0204, -0.0207, -0.0192, 0.0172, 0.0244, -0.1107, -0.0117, 0.0131, -0.0048, 0.0041, -0.0074, 0.0333, -0.0161, 0.0165, -0.0058, 0.0180, -0.0902, -0.0098, 0.0532, 0.0356, -0.0033, -0.0686, -0.0261, -0.0346, -0.0287, 0.0008, -0.0222, 0.0131, -0.0130, -0.0142, 0.0031, -0.0115, -0.0222, 0.1851, 0.0110, 0.0032, 0.0084, 0.0043, 0.0030, -0.0789, -0.0556, 0.0101, 0.0171, 0.0520, 0.0119, -0.0049]\n",
            "Token 1( açò): [0.0144, -0.0013, 0.1037, 0.0479, -0.0124, -0.0266, -0.0000, -0.0075, -0.0201, -0.0112, -0.0067, 0.0035, 0.0078, -0.0258, 0.0088, 0.0744, -0.0109, 0.0356, -0.0079, -0.0118, 0.0221, -0.0005, -0.0154, -0.0675, -0.0187, 0.0025, -0.0202, -0.0645, 0.0628, 0.0025, 0.0083, -0.0066, 0.0099, 0.0136, 0.0197, 0.0048, -0.0138, -0.0578, -0.0066, -0.0104, -0.0102, -0.0366, 0.0048, -0.0140, 0.0284, -0.0229, -0.0006, -0.0135, -0.0281, 0.0317, -0.0399, -0.0100, 0.0216, 0.0795, -0.0267, -0.0488, 0.0202, 0.0345, 0.0252, 0.0079, -0.0219, -0.0970, -0.0055, 0.0109, -0.0027, 0.0153, -0.0429, 0.0100, -0.0113, -0.0064, -0.0261, -0.0300, 0.0068, 0.0153, 0.0103, -0.0094, -0.0236, 0.0200, 0.0235, -0.0358, 0.0311, -0.0394, -0.0203, -0.0206, -0.0191, 0.0173, 0.0243, -0.1100, -0.0116, 0.0132, -0.0047, 0.0041, -0.0074, 0.0330, -0.0159, 0.0165, -0.0058, 0.0178, -0.0895, -0.0096, 0.0533, 0.0356, -0.0034, -0.0680, -0.0258, -0.0348, -0.0288, 0.0008, -0.0221, 0.0130, -0.0128, -0.0142, 0.0034, -0.0114, -0.0221, 0.1844, 0.0108, 0.0029, 0.0083, 0.0042, 0.0033, -0.0804, -0.0552, 0.0101, 0.0170, 0.0516, 0.0117, -0.0049]\n",
            "Token 2(  es): [0.0121, 0.0015, 0.1000, 0.0445, -0.0102, -0.0239, -0.0017, -0.0094, -0.0169, -0.0091, -0.0041, 0.0017, 0.0084, -0.0186, 0.0078, 0.0753, -0.0093, 0.0330, -0.0052, -0.0109, 0.0193, -0.0003, -0.0107, -0.0591, -0.0137, 0.0015, -0.0302, -0.0600, 0.0566, 0.0027, 0.0079, -0.0074, 0.0098, 0.0109, 0.0176, 0.0002, -0.0128, -0.0459, -0.0044, -0.0117, -0.0058, -0.0329, 0.0060, -0.0108, 0.0248, -0.0180, -0.0003, -0.0134, -0.0233, 0.0305, -0.0367, -0.0098, 0.0204, 0.0754, -0.0246, -0.0430, 0.0211, 0.0287, 0.0246, 0.0053, -0.0209, -0.0968, -0.0042, 0.0110, -0.0042, 0.0164, -0.0410, 0.0072, -0.0099, -0.0074, -0.0252, -0.0260, 0.0067, 0.0116, 0.0068, -0.0083, -0.0219, 0.0156, 0.0211, -0.0267, 0.0289, -0.0397, -0.0209, -0.0188, -0.0189, 0.0173, 0.0230, -0.0999, -0.0098, 0.0136, -0.0035, 0.0034, -0.0077, 0.0287, -0.0128, 0.0157, -0.0067, 0.0149, -0.0802, -0.0074, 0.0552, 0.0343, -0.0039, -0.0602, -0.0216, -0.0362, -0.0304, 0.0002, -0.0206, 0.0113, -0.0098, -0.0143, 0.0073, -0.0102, -0.0201, 0.1760, 0.0082, -0.0007, 0.0073, 0.0017, 0.0070, -0.1003, -0.0503, 0.0092, 0.0153, 0.0450, 0.0094, -0.0063]\n",
            "Token 3(  or): [0.0141, 0.0001, 0.0999, 0.0460, -0.0110, -0.0252, -0.0002, -0.0076, -0.0187, -0.0107, -0.0064, 0.0028, 0.0074, -0.0235, 0.0083, 0.0745, -0.0099, 0.0347, -0.0077, -0.0122, 0.0208, -0.0000, -0.0147, -0.0652, -0.0198, 0.0022, -0.0264, -0.0627, 0.0611, 0.0024, 0.0082, -0.0073, 0.0098, 0.0131, 0.0201, 0.0041, -0.0134, -0.0559, -0.0064, -0.0099, -0.0107, -0.0351, 0.0052, -0.0133, 0.0266, -0.0214, 0.0000, -0.0124, -0.0257, 0.0303, -0.0379, -0.0093, 0.0207, 0.0769, -0.0260, -0.0467, 0.0193, 0.0309, 0.0251, 0.0071, -0.0227, -0.0959, -0.0053, 0.0112, -0.0027, 0.0146, -0.0424, 0.0084, -0.0116, -0.0070, -0.0253, -0.0288, 0.0059, 0.0143, 0.0090, -0.0094, -0.0230, 0.0183, 0.0217, -0.0352, 0.0299, -0.0377, -0.0225, -0.0194, -0.0191, 0.0176, 0.0243, -0.1052, -0.0107, 0.0118, -0.0041, 0.0031, -0.0078, 0.0318, -0.0145, 0.0160, -0.0060, 0.0168, -0.0867, -0.0093, 0.0530, 0.0342, -0.0026, -0.0661, -0.0240, -0.0327, -0.0280, 0.0007, -0.0215, 0.0115, -0.0122, -0.0133, 0.0042, -0.0112, -0.0209, 0.1841, 0.0103, 0.0020, 0.0084, 0.0039, 0.0038, -0.0803, -0.0538, 0.0100, 0.0162, 0.0479, 0.0108, -0.0054]\n",
            "Token 4( ...): [0.0122, 0.0044, 0.0917, 0.0405, -0.0076, -0.0218, -0.0017, -0.0085, -0.0143, -0.0086, -0.0048, -0.0002, 0.0070, -0.0150, 0.0063, 0.0756, -0.0072, 0.0319, -0.0058, -0.0119, 0.0168, 0.0008, -0.0107, -0.0550, -0.0188, 0.0011, -0.0415, -0.0587, 0.0553, 0.0019, 0.0081, -0.0085, 0.0101, 0.0107, 0.0204, -0.0007, -0.0123, -0.0434, -0.0047, -0.0098, -0.0095, -0.0304, 0.0066, -0.0104, 0.0214, -0.0159, 0.0018, -0.0102, -0.0189, 0.0274, -0.0326, -0.0084, 0.0185, 0.0698, -0.0246, -0.0381, 0.0184, 0.0218, 0.0242, 0.0043, -0.0221, -0.0942, -0.0043, 0.0117, -0.0037, 0.0143, -0.0405, 0.0042, -0.0104, -0.0093, -0.0238, -0.0246, 0.0042, 0.0100, 0.0051, -0.0085, -0.0209, 0.0126, 0.0177, -0.0280, 0.0260, -0.0354, -0.0252, -0.0169, -0.0188, 0.0181, 0.0237, -0.0917, -0.0082, 0.0103, -0.0023, 0.0003, -0.0090, 0.0270, -0.0109, 0.0151, -0.0065, 0.0138, -0.0744, -0.0072, 0.0526, 0.0312, -0.0019, -0.0579, -0.0186, -0.0301, -0.0276, 0.0003, -0.0190, 0.0078, -0.0094, -0.0116, 0.0080, -0.0100, -0.0179, 0.1763, 0.0076, -0.0022, 0.0078, 0.0020, 0.0068, -0.0895, -0.0488, 0.0099, 0.0142, 0.0383, 0.0082, -0.0065]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0012, 0.1335, -0.0075, -0.0087, -0.0006, 0.0005, -0.0229, -0.0121, 0.0100, -0.0023, -0.0038, 0.0155, -0.0052, 0.0117, -0.0194, 0.0149, -0.0115, 0.0175, -0.0108, 0.0035, -0.0009, -0.0090, -0.0023, -0.0008, -0.0082, -0.0201, 0.0219, 0.0187, 0.0121, -0.0175, 0.0056, -0.0124, -0.0064, -0.0108, -0.0018, 0.0265, 0.0129, 0.0160, 0.0102, 0.0108, 0.0160, -0.0102, 0.0072, 0.0076, -0.0174, -0.0030, -0.0074, 0.0082, -0.0057, -0.0010, 0.0045, -0.0182, 0.0144, -0.0050, 0.0073, 0.0205, 0.0158, 0.0072, -0.0245, -0.0097, 0.0237, 0.0227, 0.0092, 0.0187, -0.0324, -0.0199, -0.0113, -0.0069, -0.0184, -0.0117, 0.0202, -0.0199, 0.0051, -0.0040, 0.0163, 0.0134, 0.0148, -0.0318, 0.0111, -0.0055, -0.0060, -0.0199, -0.0313, -0.0209, -0.0004, 0.0061, -0.0073, 0.0019, 0.0120, 0.0266, -0.0017, -0.0083, 0.0130, -0.0143, -0.0164, -0.1361, 0.0236, 0.0235, 0.0094, -0.0067, 0.0073, -0.0145, -0.0118, 0.0230, -0.0041, 0.0061, -0.0005, 0.0053, -0.3107, -0.0061, 0.0079, -0.0214, -0.0108, 0.0005, -0.0144, 0.0206, 0.0088, -0.0122, -0.0166, -0.0069, -0.0326, -0.0126, 0.0135, 0.0052, -0.0055, 0.0015, 0.0052, 0.0133]\n",
            "Token 1( açò): [0.0123, 0.1009, -0.0066, -0.0099, -0.0095, 0.0086, -0.0140, -0.0044, 0.0075, 0.0017, 0.0087, 0.0193, -0.0211, -0.0005, -0.0145, 0.0151, 0.0003, -0.0017, -0.0033, -0.0055, -0.0036, -0.0144, 0.0110, 0.0028, 0.0089, -0.0163, 0.0120, -0.0027, -0.0130, -0.0178, -0.0046, 0.0075, 0.0333, -0.0108, -0.0017, 0.0281, -0.0041, 0.0157, -0.0011, 0.0184, 0.0030, 0.0670, 0.0198, 0.0215, -0.0022, -0.0092, -0.0008, 0.0039, -0.0057, 0.0000, 0.0016, -0.0098, 0.0041, 0.0012, 0.0137, 0.0049, 0.0165, 0.0050, -0.0032, -0.0063, 0.0296, 0.0152, -0.0039, 0.0022, -0.0291, -0.0108, -0.0138, -0.0048, -0.0136, 0.0149, 0.0043, -0.0210, 0.0143, -0.0006, 0.0160, -0.0000, 0.0035, -0.0267, -0.0118, -0.0087, 0.0026, -0.0241, -0.0123, -0.0080, -0.0000, -0.0005, -0.0197, -0.0075, 0.0046, 0.0106, -0.0098, -0.0176, 0.0140, -0.0080, -0.0193, -0.0931, 0.0198, 0.0022, 0.0306, -0.0022, -0.0048, 0.0088, 0.0024, 0.0206, -0.0019, -0.0259, 0.0055, -0.0038, -0.2174, -0.0070, 0.0005, -0.0157, -0.0011, 0.0057, -0.0022, 0.0071, 0.0114, -0.0127, 0.0005, -0.0167, -0.0103, -0.0174, -0.0002, 0.0218, -0.0054, 0.0087, 0.0045, 0.0046]\n",
            "Token 2(  es): [0.0120, 0.0802, -0.0041, -0.0104, -0.0080, 0.0078, -0.0132, -0.0012, 0.0020, 0.0021, 0.0109, 0.0176, -0.0192, -0.0014, -0.0120, 0.0089, 0.0061, -0.0029, 0.0011, -0.0108, -0.0042, -0.0159, 0.0089, 0.0041, 0.0133, -0.0114, 0.0111, -0.0050, -0.0153, -0.0144, -0.0046, 0.0145, 0.0408, -0.0086, 0.0011, 0.0195, -0.0100, 0.0128, 0.0007, 0.0190, 0.0040, 0.0957, 0.0237, 0.0172, 0.0044, -0.0079, -0.0021, 0.0019, -0.0044, -0.0005, -0.0004, -0.0093, 0.0012, -0.0003, 0.0101, 0.0065, 0.0120, 0.0065, 0.0039, -0.0058, 0.0262, 0.0113, -0.0004, -0.0057, -0.0192, -0.0048, -0.0099, -0.0046, -0.0114, 0.0158, -0.0016, -0.0211, 0.0136, 0.0029, 0.0098, -0.0043, 0.0055, -0.0213, -0.0158, -0.0038, 0.0059, -0.0305, -0.0081, -0.0078, 0.0048, 0.0016, -0.0211, -0.0060, 0.0034, 0.0035, -0.0105, -0.0194, 0.0160, -0.0079, -0.0149, -0.0715, 0.0132, -0.0090, 0.0325, -0.0006, -0.0071, 0.0104, 0.0081, 0.0199, -0.0024, -0.0305, 0.0022, -0.0056, -0.1604, -0.0048, -0.0028, -0.0158, 0.0036, 0.0098, -0.0016, 0.0022, 0.0076, -0.0069, 0.0030, -0.0206, -0.0083, -0.0161, -0.0025, 0.0186, -0.0055, 0.0087, 0.0078, 0.0059]\n",
            "Token 3(  or): [0.0057, 0.0742, -0.0027, -0.0125, 0.0010, 0.0059, -0.0114, 0.0003, 0.0025, 0.0026, 0.0092, 0.0167, -0.0168, 0.0026, -0.0112, 0.0033, 0.0081, -0.0027, 0.0013, -0.0087, -0.0052, -0.0146, 0.0088, 0.0044, 0.0140, -0.0112, 0.0068, -0.0071, -0.0159, -0.0156, 0.0025, 0.0122, 0.0356, -0.0096, 0.0005, 0.0172, -0.0085, 0.0127, 0.0042, 0.0196, 0.0051, 0.1157, 0.0255, 0.0171, -0.0007, -0.0080, -0.0041, 0.0041, -0.0049, 0.0014, 0.0020, -0.0113, -0.0017, -0.0029, 0.0066, 0.0054, 0.0110, 0.0015, 0.0051, -0.0014, 0.0223, 0.0114, -0.0004, -0.0071, -0.0177, -0.0043, -0.0082, -0.0066, -0.0109, 0.0114, 0.0032, -0.0203, 0.0082, 0.0031, 0.0103, 0.0009, 0.0066, -0.0230, -0.0082, -0.0128, -0.0002, -0.0291, -0.0126, -0.0072, 0.0045, 0.0047, -0.0146, -0.0045, 0.0082, 0.0075, -0.0102, -0.0155, 0.0155, -0.0054, -0.0121, -0.0633, 0.0141, -0.0056, 0.0278, -0.0054, -0.0049, 0.0042, 0.0022, 0.0185, 0.0011, -0.0242, 0.0058, -0.0050, -0.1365, -0.0032, -0.0073, -0.0167, 0.0040, 0.0036, -0.0017, -0.0030, 0.0090, -0.0084, 0.0022, -0.0145, -0.0108, -0.0139, 0.0012, 0.0148, -0.0052, 0.0089, 0.0104, 0.0079]\n",
            "Token 4( ...): [0.0023, 0.1010, -0.0062, -0.0117, 0.0018, 0.0020, -0.0166, -0.0072, 0.0068, -0.0009, 0.0028, 0.0162, -0.0104, 0.0080, -0.0145, 0.0096, -0.0037, 0.0073, -0.0068, -0.0027, -0.0032, -0.0119, 0.0035, 0.0008, 0.0023, -0.0144, 0.0143, 0.0062, -0.0010, -0.0179, 0.0054, -0.0006, 0.0128, -0.0096, -0.0013, 0.0210, 0.0016, 0.0147, 0.0065, 0.0152, 0.0096, 0.0583, 0.0157, 0.0124, -0.0114, -0.0030, -0.0053, 0.0057, -0.0042, 0.0012, 0.0045, -0.0143, 0.0074, -0.0026, 0.0088, 0.0120, 0.0133, 0.0043, -0.0098, -0.0066, 0.0230, 0.0176, 0.0050, 0.0053, -0.0267, -0.0126, -0.0100, -0.0075, -0.0150, -0.0000, 0.0134, -0.0197, 0.0061, -0.0003, 0.0142, 0.0060, 0.0105, -0.0287, 0.0028, -0.0093, -0.0033, -0.0232, -0.0226, -0.0139, 0.0023, 0.0040, -0.0104, -0.0029, 0.0097, 0.0181, -0.0046, -0.0128, 0.0136, -0.0095, -0.0153, -0.0995, 0.0182, 0.0104, 0.0186, -0.0071, 0.0033, -0.0057, -0.0054, 0.0188, -0.0014, -0.0088, 0.0033, 0.0004, -0.2226, -0.0054, -0.0001, -0.0180, -0.0046, 0.0005, -0.0096, 0.0084, 0.0091, -0.0110, -0.0068, -0.0077, -0.0221, -0.0127, 0.0078, 0.0109, -0.0056, 0.0061, 0.0070, 0.0106]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0236, 0.0308, 0.0247, 0.0003, -0.0147, -0.0312, 0.0550, 0.0086, -0.0274, 0.0342, -0.0096, 0.0584, 0.0084, 0.0174, 0.0022, 0.0344, -0.0254, -0.0255, 0.0147, -0.0068, -0.0180, -0.0221, -0.0280, -0.0238, -0.0153, 0.0116, -0.0280, -0.0048, -0.0186, 0.0164, 0.0284, -0.0507, 0.0667, -0.0321, -0.0313, -0.0008, 0.0558, 0.0023, 0.0042, -0.0332, -0.0056, -0.0238, 0.0336, 0.0053, -0.0498, 0.0188, 0.0039, -0.0216, -0.0039, 0.0271, 0.0520, -0.0211, -0.0073, 0.0067, -0.0047, 0.0392, 0.0272, 0.0081, -0.0352, 0.0057, -0.0338, 0.0036, 0.0144, -0.0150, -0.0680, 0.0115, 0.0086, 0.0428, -0.0103, -0.0305, 0.0323, -0.0095, 0.0125, 0.0240, -0.0388, 0.0368, 0.0578, 0.0346, -0.0078, 0.0148, -0.0256, -0.0003, -0.0308, 0.0177, 0.0304, -0.0237, 0.0452, 0.0181, -0.0392, 0.0036, 0.0022, 0.0253, 0.0149, -0.0104, -0.0237, -0.0180, -0.0086, 0.0032, 0.0461, 0.0240, 0.0008, 0.0463, 0.0173, -0.0139, -0.0276, -0.0321, 0.0268, 0.0005, -0.0448, -0.0102, 0.0062, -0.0446, -0.0137, 0.0014, -0.0325, 0.0752, 0.0415, -0.0472, -0.0143, -0.0047, 0.0013, 0.0084, 0.0744, -0.0319, -0.0092, -0.0265, -0.0062, 0.0050]\n",
            "Token 1( açò): [-0.0185, 0.0217, 0.0087, 0.0116, -0.0215, -0.0108, 0.0386, 0.0050, -0.0323, 0.0251, -0.0091, 0.0458, 0.0083, 0.0054, -0.0085, 0.0303, -0.0213, -0.0164, 0.0043, -0.0117, -0.0253, -0.0225, -0.0218, -0.0155, -0.0159, 0.0027, -0.0211, -0.0140, -0.0122, 0.0227, 0.0169, -0.0369, 0.0588, -0.0279, -0.0199, 0.0155, 0.0409, 0.0020, -0.0046, -0.0303, -0.0137, -0.0302, 0.0311, -0.0042, -0.0396, 0.0078, 0.0048, -0.0125, -0.0081, 0.0200, 0.0340, -0.0103, -0.0041, 0.0008, -0.0071, 0.0321, 0.0229, 0.0147, -0.0287, 0.0135, -0.0220, -0.0069, 0.0159, -0.0233, -0.0654, 0.0120, 0.0094, -0.0047, -0.0130, -0.0184, 0.0267, -0.0105, 0.0077, 0.0090, -0.0302, 0.0302, 0.0546, 0.0289, -0.0167, 0.0160, -0.0270, -0.0044, -0.0331, 0.0113, 0.0210, -0.0152, 0.0273, 0.0213, -0.0266, -0.0017, 0.0036, 0.0247, 0.0249, -0.0107, -0.0160, -0.0185, 0.0050, -0.0015, 0.0212, 0.0047, 0.0065, 0.0296, 0.0170, -0.0180, -0.0214, -0.0189, 0.0207, 0.0077, -0.0388, -0.0135, 0.0015, -0.0434, -0.0182, -0.0084, -0.0319, 0.0644, 0.0360, -0.0273, -0.0221, -0.0087, -0.0001, 0.0079, 0.0638, -0.0246, 0.0114, -0.0307, -0.0016, -0.0056]\n",
            "Token 2(  es): [-0.0057, 0.0269, -0.0010, 0.0108, -0.0130, -0.0077, 0.0191, 0.0046, -0.0273, 0.0179, -0.0047, 0.0285, 0.0056, -0.0006, -0.0071, 0.0239, -0.0188, -0.0125, 0.0023, -0.0028, -0.0234, -0.0274, -0.0193, -0.0146, -0.0115, -0.0063, -0.0175, -0.0089, -0.0127, 0.0251, 0.0028, -0.0165, 0.0464, -0.0102, -0.0131, 0.0303, 0.0202, -0.0052, -0.0024, -0.0231, -0.0013, -0.0221, 0.0251, 0.0003, -0.0235, 0.0028, 0.0098, -0.0088, -0.0151, 0.0113, 0.0153, -0.0039, -0.0053, -0.0015, -0.0013, 0.0186, 0.0186, 0.0168, -0.0193, 0.0068, -0.0170, -0.0035, 0.0190, -0.0278, -0.0538, 0.0087, 0.0024, -0.0655, -0.0178, -0.0121, 0.0255, -0.0054, 0.0020, 0.0074, -0.0172, 0.0231, 0.0384, 0.0184, -0.0177, 0.0121, -0.0152, -0.0031, -0.0232, 0.0081, 0.0067, -0.0133, 0.0146, 0.0148, -0.0112, -0.0047, -0.0026, 0.0171, 0.0193, -0.0034, -0.0118, -0.0095, -0.0011, 0.0042, 0.0079, -0.0067, 0.0173, 0.0224, 0.0069, -0.0156, -0.0162, -0.0185, 0.0099, 0.0062, -0.0224, -0.0190, 0.0006, -0.0316, -0.0125, -0.0091, -0.0260, 0.0496, 0.0255, -0.0143, -0.0144, -0.0068, -0.0062, 0.0027, 0.0462, -0.0136, 0.0069, -0.0260, 0.0014, -0.0026]\n",
            "Token 3(  or): [0.0021, 0.0250, -0.0114, 0.0133, -0.0106, -0.0001, 0.0039, 0.0046, -0.0235, 0.0105, -0.0015, 0.0132, 0.0034, -0.0071, -0.0092, 0.0164, -0.0160, -0.0062, -0.0032, -0.0009, -0.0249, -0.0291, -0.0150, -0.0110, -0.0094, -0.0142, -0.0123, -0.0077, -0.0093, 0.0254, -0.0067, -0.0007, 0.0340, -0.0009, -0.0042, 0.0392, 0.0051, -0.0087, -0.0031, -0.0158, 0.0022, -0.0209, 0.0213, -0.0011, -0.0103, -0.0025, 0.0115, -0.0028, -0.0179, 0.0035, 0.0018, 0.0047, -0.0036, -0.0047, -0.0002, 0.0096, 0.0145, 0.0207, -0.0134, 0.0075, -0.0100, -0.0068, 0.0192, -0.0303, -0.0450, 0.0057, -0.0012, -0.1165, -0.0189, -0.0026, 0.0205, -0.0038, -0.0014, 0.0013, -0.0079, 0.0165, 0.0270, 0.0114, -0.0193, 0.0089, -0.0102, -0.0028, -0.0164, 0.0033, -0.0035, -0.0095, -0.0003, 0.0118, 0.0006, -0.0063, -0.0054, 0.0126, 0.0190, 0.0001, -0.0062, -0.0047, 0.0017, 0.0044, -0.0065, -0.0176, 0.0226, 0.0119, 0.0027, -0.0144, -0.0103, -0.0119, 0.0036, 0.0063, -0.0108, -0.0201, -0.0010, -0.0249, -0.0106, -0.0107, -0.0206, 0.0369, 0.0185, 0.0008, -0.0120, -0.0069, -0.0084, -0.0017, 0.0324, -0.0037, 0.0114, -0.0238, 0.0039, -0.0059]\n",
            "Token 4( ...): [-0.0005, 0.0261, -0.0061, 0.0097, -0.0081, -0.0038, 0.0110, 0.0061, -0.0215, 0.0195, -0.0009, 0.0161, 0.0021, -0.0036, -0.0062, 0.0161, -0.0151, -0.0089, -0.0005, -0.0024, -0.0215, -0.0264, -0.0148, -0.0121, -0.0085, -0.0101, -0.0158, -0.0079, -0.0098, 0.0236, -0.0022, -0.0064, 0.0375, -0.0020, -0.0063, 0.0296, 0.0118, -0.0078, -0.0019, -0.0159, 0.0048, -0.0180, 0.0213, 0.0031, -0.0135, 0.0013, 0.0095, -0.0035, -0.0144, 0.0059, 0.0067, 0.0008, -0.0033, -0.0029, -0.0002, 0.0125, 0.0133, 0.0193, -0.0149, 0.0053, -0.0121, -0.0019, 0.0169, -0.0284, -0.0433, 0.0051, -0.0019, -0.1018, -0.0158, -0.0058, 0.0193, -0.0040, 0.0004, 0.0052, -0.0118, 0.0177, 0.0273, 0.0146, -0.0168, 0.0097, -0.0055, -0.0012, -0.0161, 0.0027, -0.0007, -0.0111, -0.0017, 0.0092, -0.0040, -0.0038, -0.0038, 0.0121, 0.0160, 0.0021, -0.0095, -0.0043, -0.0000, 0.0036, 0.0017, -0.0090, 0.0181, 0.0142, 0.0012, -0.0131, -0.0094, -0.0124, 0.0041, 0.0052, -0.0100, -0.0131, 0.0024, -0.0237, -0.0083, -0.0094, -0.0179, 0.0379, 0.0185, -0.0056, -0.0082, -0.0027, -0.0098, -0.0002, 0.0353, -0.0053, 0.0058, -0.0199, 0.0022, -0.0028]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0085, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0007, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0013, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0005, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0045, 0.0072, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0034, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 1( açò): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0085, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0013, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0072, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0034, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 2(  es): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0006, 0.0011, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0091, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0014, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0029, 0.0029, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0071, -0.0142, -0.0132, -0.0052, -0.0021, -0.0038, 0.0090, -0.0059, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0121, -0.0083, -0.0082, 0.0183, -0.0129, 0.0108, 0.0095, 0.0059, 0.0018, 0.0082, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 3(  or): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0011, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0091, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0014, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0029, 0.0029, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0071, -0.0142, -0.0132, -0.0052, -0.0021, -0.0038, 0.0090, -0.0059, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0121, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0018, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 4( ...): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0014, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0071, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0059, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0221, -0.0290, 0.0401, 0.0479, 0.0276, 0.0309, 0.0906, 0.0031, -0.0404, -0.0427, 0.0062, 0.0359, 0.0170, -0.0042, -0.0872, 0.0440, 0.0865, -0.0061, -0.0461, -0.0415, 0.0904, 0.0210, 0.0223, 0.1286, 0.0484, -0.0855, -0.0082, -0.0447, -0.0681, 0.0004, -0.0011, 0.0295, -0.0151, 0.0046, 0.0309, -0.1582, -0.0474, -0.0266, -0.0904, -0.0449, -0.0281, -0.0195, 0.0308, -0.0697, -0.0333, -0.0543, 0.0311, -0.0130, -0.0112, 0.0463, 0.0396, 0.0115, -0.0087, 0.0505, 0.0553, -0.0155, 0.0315, 0.0621, 0.0014, -0.0402, 0.0272, 0.0318, 0.0193, -0.0305, 0.0043, 0.0270, 0.0710, -0.0147, 0.0131, -0.0003, -0.0824, -0.0185, -0.0809, 0.0021, -0.0470, 0.0892, 0.0431, 0.0617, -0.0667, -0.0013, -0.0217, 0.0576, 0.0497, 0.0382, 0.0172, 0.0054, 0.1304, 0.0131, -0.0242, -0.0039, -0.0233, 0.0146, 0.0184, 0.0437, -0.0384, -0.0255, -0.1129, 0.0313, -0.0287, -0.0150, 0.0842, -0.0510, 0.0243, 0.0378, 0.0348, 0.0708, 0.0397, -0.0221, -0.0228, -0.0396, -0.0156, 0.1222, 0.0136, 0.0417, 0.0412, 0.0504, 0.0310, 0.0525, -0.0372, -0.0014, -0.0098, 0.0982, 0.0736, -0.0478, 0.0607, 0.0734, -0.0399, -0.0458]\n",
            "Token 1( açò): [0.0179, -0.0072, 0.0320, 0.0479, 0.0100, 0.0137, 0.0474, 0.0146, -0.0174, -0.0282, 0.0016, 0.0226, 0.0187, -0.0073, -0.0527, 0.0114, 0.0522, 0.0073, -0.0324, -0.0515, 0.0685, 0.0091, 0.0135, 0.0929, 0.0229, -0.0244, -0.0140, -0.0345, -0.0414, -0.0017, -0.0019, 0.0238, -0.0145, 0.0052, 0.0078, -0.1036, -0.0492, -0.0270, -0.0664, -0.0348, -0.0216, -0.0189, 0.0230, -0.0689, -0.0291, -0.0462, 0.0049, 0.0117, 0.0017, 0.0252, 0.0150, 0.0025, -0.0039, 0.0136, 0.0680, 0.0044, 0.0387, 0.0675, 0.0030, -0.0328, 0.0373, 0.0129, 0.0175, -0.0142, -0.0076, 0.0062, 0.0312, -0.0166, -0.0051, 0.0055, -0.0378, 0.0125, -0.0664, -0.0002, -0.0267, 0.0547, 0.0261, 0.0601, -0.0420, 0.0093, -0.0238, 0.0415, 0.0294, 0.0136, 0.0028, 0.0143, 0.0936, -0.0088, -0.0153, -0.0075, -0.0156, -0.0046, 0.0143, 0.0386, -0.0255, -0.0001, -0.0944, 0.0012, -0.0234, -0.0319, 0.0610, -0.0423, 0.0333, 0.0035, 0.0047, 0.0626, 0.0643, -0.0335, -0.0020, -0.0400, -0.0220, 0.0903, -0.0107, 0.0271, 0.0120, 0.0274, 0.0173, 0.0553, -0.0155, 0.0058, -0.0196, 0.0454, 0.0681, 0.0630, 0.0111, 0.0511, -0.0116, -0.0389]\n",
            "Token 2(  es): [0.0203, -0.0148, 0.0343, 0.0334, 0.0152, 0.0188, 0.0537, 0.0136, -0.0240, -0.0327, 0.0078, 0.0202, 0.0144, -0.0008, -0.0595, 0.0249, 0.0575, 0.0040, -0.0323, -0.0417, 0.0674, 0.0141, 0.0173, 0.0901, 0.0312, -0.0627, -0.0103, -0.0344, -0.0429, -0.0030, -0.0039, 0.0273, -0.0141, 0.0004, 0.0136, -0.0973, -0.0503, -0.0238, -0.0624, -0.0381, -0.0194, -0.0170, 0.0211, -0.0548, -0.0288, -0.0383, 0.0097, 0.0090, -0.0010, 0.0237, 0.0180, 0.0059, -0.0094, 0.0219, 0.0504, -0.0010, 0.0285, 0.0545, 0.0089, -0.0276, 0.0286, 0.0131, 0.0164, -0.0193, 0.0048, 0.0171, 0.0342, -0.0114, 0.0023, 0.0046, -0.0461, -0.0003, -0.0584, -0.0002, -0.0327, 0.0555, 0.0240, 0.0519, -0.0421, 0.0046, -0.0108, 0.0431, 0.0285, 0.0225, 0.0134, 0.0088, 0.0886, 0.0042, -0.0126, -0.0024, -0.0154, 0.0016, 0.0169, 0.0317, -0.0253, -0.0104, -0.0800, 0.0079, -0.0212, -0.0198, 0.0612, -0.0369, 0.0247, 0.0161, 0.0100, 0.0534, 0.0417, -0.0276, -0.0044, -0.0215, -0.0144, 0.0824, 0.0005, 0.0334, 0.0192, 0.0331, 0.0144, 0.0471, -0.0224, -0.0007, -0.0139, 0.0618, 0.0595, 0.0330, 0.0291, 0.0432, -0.0247, -0.0367]\n",
            "Token 3(  or): [0.0232, -0.0173, 0.0318, 0.0249, 0.0149, 0.0161, 0.0479, 0.0139, -0.0258, -0.0316, 0.0034, 0.0098, 0.0143, 0.0003, -0.0574, 0.0214, 0.0527, 0.0006, -0.0230, -0.0357, 0.0623, 0.0145, 0.0192, 0.0811, 0.0242, -0.0688, -0.0129, -0.0266, -0.0346, -0.0064, -0.0071, 0.0256, -0.0123, -0.0022, 0.0151, -0.0827, -0.0400, -0.0230, -0.0531, -0.0309, -0.0253, -0.0164, 0.0126, -0.0519, -0.0302, -0.0310, 0.0084, 0.0125, -0.0016, 0.0267, 0.0138, 0.0095, -0.0038, 0.0234, 0.0477, -0.0060, 0.0222, 0.0474, 0.0128, -0.0259, 0.0174, 0.0128, 0.0159, -0.0186, 0.0104, 0.0174, 0.0293, -0.0130, 0.0065, 0.0051, -0.0444, -0.0023, -0.0552, -0.0043, -0.0248, 0.0516, 0.0264, 0.0402, -0.0401, 0.0030, -0.0154, 0.0382, 0.0238, 0.0240, 0.0188, 0.0087, 0.0736, 0.0057, -0.0067, 0.0022, -0.0134, 0.0079, 0.0102, 0.0265, -0.0213, -0.0154, -0.0738, 0.0049, -0.0128, -0.0243, 0.0464, -0.0291, 0.0231, 0.0169, 0.0150, 0.0536, 0.0386, -0.0239, -0.0092, -0.0150, -0.0106, 0.0754, 0.0028, 0.0339, 0.0196, 0.0288, 0.0163, 0.0438, -0.0262, 0.0072, -0.0123, 0.0561, 0.0518, 0.0406, 0.0311, 0.0391, -0.0214, -0.0349]\n",
            "Token 4( ...): [0.0190, -0.0099, 0.0264, 0.0145, 0.0118, 0.0137, 0.0308, 0.0222, -0.0216, -0.0260, -0.0009, -0.0017, 0.0126, 0.0046, -0.0435, 0.0170, 0.0528, 0.0082, -0.0106, -0.0337, 0.0519, 0.0094, 0.0215, 0.0668, 0.0149, -0.0674, -0.0148, -0.0203, -0.0213, -0.0071, -0.0063, 0.0235, -0.0137, -0.0052, 0.0179, -0.0577, -0.0335, -0.0168, -0.0383, -0.0237, -0.0246, -0.0139, 0.0115, -0.0447, -0.0279, -0.0156, 0.0010, 0.0202, 0.0072, 0.0207, 0.0057, 0.0062, -0.0007, 0.0088, 0.0448, -0.0016, 0.0155, 0.0388, 0.0206, -0.0192, 0.0138, 0.0113, 0.0184, -0.0171, 0.0084, 0.0090, 0.0140, -0.0157, 0.0070, 0.0057, -0.0276, 0.0088, -0.0461, -0.0061, -0.0169, 0.0414, 0.0187, 0.0301, -0.0279, 0.0065, -0.0160, 0.0331, 0.0209, 0.0162, 0.0194, 0.0155, 0.0467, 0.0054, -0.0024, 0.0006, -0.0111, 0.0141, 0.0086, 0.0173, -0.0218, -0.0143, -0.0561, 0.0013, -0.0064, -0.0275, 0.0367, -0.0263, 0.0229, 0.0122, 0.0074, 0.0425, 0.0366, -0.0212, -0.0016, -0.0087, -0.0091, 0.0615, -0.0005, 0.0223, 0.0087, 0.0207, 0.0128, 0.0366, -0.0222, 0.0077, -0.0094, 0.0405, 0.0419, 0.0608, 0.0189, 0.0266, -0.0135, -0.0277]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0429, -0.0179, -0.0109, 0.0065, -0.0191, 0.0015, -0.0085, -0.0321, 0.0119, 0.0308, 0.0113, 0.0059, -0.0289, 0.0423, 0.0151, 0.0074, -0.0384, 0.0037, -0.0305, 0.0015, -0.0040, 0.0103, -0.0560, 0.0235, 0.0191, -0.0373, -0.0335, -0.0556, -0.0582, 0.0301, -0.0319, 0.0070, -0.0166, 0.0305, 0.0567, 0.0362, 0.0125, 0.0276, -0.0410, -0.0074, 0.0592, 0.0183, -0.0192, -0.0051, 0.0118, 0.0172, -0.0361, 0.0137, -0.0272, 0.0263, 0.0320, -0.0083, -0.0086, -0.0123, -0.0059, 0.0086, 0.0198, -0.0217, 0.0067, -0.0164, -0.0423, 0.0467, 0.0434, 0.0203, -0.0385, 0.0447, -0.0076, -0.0229, -0.0403, 0.0496, 0.0166, -0.0098, 0.0369, -0.0623, -0.0457, -0.0143, -0.0256, -0.0392, 0.0264, -0.0000, 0.0254, 0.0334, -0.0010, 0.0080, -0.0341, -0.0262, 0.0178, 0.0157, -0.0243, 0.0067, 0.0442, 0.0192, 0.0506, -0.0183, 0.0356, 0.0331, 0.0869, 0.0204, -0.0065, 0.0286, -0.0079, 0.0066, -0.0190, -0.0041, -0.0381, -0.0184, -0.0527, 0.0158, 0.0007, 0.0062, 0.0185, 0.0118, 0.0339, 0.0244, 0.0137, -0.0148, 0.0298, 0.0210, 0.0185, 0.0420, 0.0194, -0.0270, -0.0173, 0.0514, -0.0193, -0.0056, 0.0338, -0.0442]\n",
            "Token 1( açò): [0.0322, -0.0127, -0.0020, 0.0064, -0.0078, 0.0103, -0.0165, -0.0268, 0.0145, 0.0360, 0.0096, 0.0054, -0.0199, 0.0378, 0.0143, 0.0123, -0.0260, 0.0013, -0.0223, 0.0120, -0.0056, 0.0119, -0.0428, 0.0190, 0.0138, -0.0269, -0.0265, -0.0505, -0.0440, 0.0280, -0.0293, 0.0051, -0.0177, 0.0230, 0.0546, 0.0291, 0.0130, 0.0234, -0.0303, -0.0114, 0.0532, 0.0127, -0.0196, -0.0023, 0.0055, 0.0105, -0.0262, 0.0096, -0.0294, 0.0216, 0.0210, -0.0111, -0.0069, -0.0110, -0.0083, 0.0083, 0.0192, -0.0132, 0.0065, -0.0064, -0.0328, 0.0424, 0.0404, 0.0168, -0.0349, 0.0362, -0.0036, -0.0195, -0.0257, 0.0414, 0.0097, -0.0096, 0.0376, -0.0556, -0.0466, -0.0114, -0.0204, -0.0233, 0.0215, -0.0009, 0.0250, 0.0298, -0.0006, 0.0100, -0.0231, -0.0223, 0.0152, 0.0079, -0.0208, 0.0026, 0.0346, 0.0170, 0.0443, -0.0185, 0.0256, 0.0344, 0.0156, 0.0192, -0.0002, 0.0279, -0.0102, 0.0047, -0.0141, -0.0001, -0.0290, -0.0169, -0.0407, 0.0118, -0.0034, 0.0008, 0.0199, 0.0106, 0.0290, 0.0189, 0.0122, -0.0183, 0.0195, 0.0124, 0.0148, 0.0364, 0.0130, -0.0229, -0.0139, 0.0408, -0.0145, -0.0066, 0.0311, -0.0387]\n",
            "Token 2(  es): [0.0141, -0.0021, 0.0129, 0.0058, 0.0033, 0.0159, -0.0114, -0.0181, 0.0163, 0.0377, 0.0048, 0.0083, -0.0074, 0.0235, 0.0096, 0.0185, -0.0119, -0.0015, -0.0147, 0.0167, -0.0061, 0.0145, -0.0261, 0.0072, 0.0076, -0.0098, -0.0159, -0.0326, -0.0193, 0.0167, -0.0228, 0.0018, -0.0202, 0.0085, 0.0380, 0.0177, 0.0121, 0.0136, -0.0132, -0.0151, 0.0359, 0.0003, -0.0155, -0.0024, -0.0047, -0.0045, -0.0103, 0.0011, -0.0223, 0.0134, 0.0047, -0.0153, -0.0031, -0.0052, -0.0029, 0.0087, 0.0162, -0.0050, 0.0025, 0.0042, -0.0211, 0.0255, 0.0308, 0.0115, -0.0250, 0.0206, 0.0023, -0.0135, -0.0136, 0.0252, 0.0005, -0.0133, 0.0335, -0.0380, -0.0429, -0.0096, -0.0104, -0.0093, 0.0084, 0.0004, 0.0188, 0.0195, -0.0071, 0.0108, -0.0034, -0.0173, 0.0052, 0.0020, -0.0181, -0.0043, 0.0205, 0.0080, 0.0297, -0.0148, 0.0137, 0.0318, -0.1003, 0.0138, 0.0073, 0.0238, -0.0155, -0.0037, -0.0025, 0.0024, -0.0117, -0.0112, -0.0211, 0.0067, -0.0068, -0.0021, 0.0158, 0.0117, 0.0159, 0.0123, 0.0170, -0.0175, 0.0082, 0.0021, 0.0045, 0.0196, 0.0031, -0.0202, -0.0044, 0.0212, -0.0032, -0.0077, 0.0214, -0.0298]\n",
            "Token 3(  or): [0.0218, -0.0042, 0.0038, 0.0052, 0.0007, 0.0119, -0.0081, -0.0150, 0.0092, 0.0310, 0.0049, 0.0066, -0.0099, 0.0281, 0.0093, 0.0123, -0.0210, -0.0039, -0.0122, 0.0103, -0.0059, 0.0108, -0.0309, 0.0110, 0.0079, -0.0196, -0.0166, -0.0410, -0.0323, 0.0189, -0.0217, 0.0010, -0.0178, 0.0176, 0.0404, 0.0180, 0.0135, 0.0156, -0.0220, -0.0106, 0.0414, 0.0077, -0.0153, -0.0104, -0.0003, 0.0010, -0.0155, 0.0110, -0.0205, 0.0171, 0.0100, -0.0117, -0.0046, -0.0110, 0.0033, 0.0106, 0.0157, -0.0072, 0.0029, -0.0063, -0.0263, 0.0257, 0.0302, 0.0154, -0.0257, 0.0283, 0.0008, -0.0134, -0.0207, 0.0274, 0.0070, -0.0074, 0.0276, -0.0374, -0.0344, -0.0082, -0.0091, -0.0206, 0.0155, -0.0008, 0.0182, 0.0185, -0.0011, 0.0110, -0.0123, -0.0207, 0.0108, 0.0056, -0.0174, 0.0063, 0.0258, 0.0091, 0.0316, -0.0091, 0.0231, 0.0246, -0.0500, 0.0142, 0.0103, 0.0223, -0.0072, 0.0012, -0.0076, -0.0045, -0.0139, -0.0109, -0.0302, 0.0006, -0.0053, 0.0032, 0.0164, 0.0104, 0.0257, 0.0153, 0.0156, -0.0135, 0.0159, 0.0149, 0.0102, 0.0211, -0.0002, -0.0182, -0.0079, 0.0248, -0.0050, -0.0057, 0.0248, -0.0319]\n",
            "Token 4( ...): [0.0235, -0.0033, 0.0024, 0.0037, 0.0028, 0.0059, -0.0023, -0.0108, 0.0051, 0.0248, 0.0020, 0.0046, -0.0110, 0.0231, 0.0083, 0.0063, -0.0220, -0.0057, -0.0107, 0.0089, -0.0059, 0.0072, -0.0294, 0.0113, 0.0044, -0.0208, -0.0127, -0.0401, -0.0340, 0.0149, -0.0175, 0.0013, -0.0156, 0.0152, 0.0354, 0.0154, 0.0136, 0.0153, -0.0200, -0.0081, 0.0358, 0.0070, -0.0129, -0.0125, 0.0005, -0.0002, -0.0138, 0.0109, -0.0164, 0.0154, 0.0114, -0.0104, -0.0071, -0.0094, 0.0085, 0.0100, 0.0154, -0.0071, 0.0007, -0.0122, -0.0303, 0.0212, 0.0285, 0.0139, -0.0213, 0.0266, 0.0032, -0.0110, -0.0212, 0.0237, 0.0079, -0.0077, 0.0234, -0.0335, -0.0284, -0.0080, -0.0065, -0.0245, 0.0138, -0.0002, 0.0159, 0.0168, 0.0000, 0.0114, -0.0108, -0.0227, 0.0085, 0.0084, -0.0135, 0.0100, 0.0255, 0.0048, 0.0244, -0.0056, 0.0243, 0.0205, -0.0602, 0.0112, 0.0125, 0.0189, -0.0039, -0.0005, -0.0053, -0.0040, -0.0109, -0.0083, -0.0286, -0.0027, -0.0047, 0.0064, 0.0132, 0.0101, 0.0277, 0.0156, 0.0135, -0.0136, 0.0159, 0.0168, 0.0112, 0.0160, -0.0021, -0.0166, -0.0081, 0.0212, -0.0059, -0.0023, 0.0220, -0.0307]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0137, 0.0080, -0.0237, 0.0203, 0.0457, -0.0550, -0.0337, -0.0210, 0.0034, 0.0230, -0.0106, 0.0489, 0.0458, 0.0026, -0.0006, -0.0319, -0.0073, -0.0139, -0.0402, 0.0432, 0.0131, -0.0196, 0.0178, 0.0868, -0.0280, 0.0066, -0.0154, -0.0950, -0.0445, -0.0329, 0.0157, -0.0258, -0.0374, -0.0414, 0.0055, 0.0018, 0.0316, 0.0247, 0.0244, 0.0332, 0.0334, -0.0201, -0.0257, -0.0182, -0.0344, 0.0531, 0.0240, -0.0031, -0.0365, 0.0279, 0.0039, -0.0460, -0.0138, -0.0035, -0.0246, -0.0191, -0.0275, -0.0409, -0.0339, -0.0039, 0.0118, 0.0337, 0.0427, 0.0922, -0.0504, 0.0117, 0.0087, -0.0035, 0.0444, 0.0249, -0.0034, -0.0242, 0.0166, -0.0017, -0.0351, -0.0057, -0.0211, 0.0220, 0.0111, 0.0040, -0.0161, -0.0115, -0.0214, 0.0154, -0.0330, 0.0168, 0.0002, -0.0092, 0.0007, 0.0167, 0.0224, 0.0023, 0.0288, -0.0171, 0.0161, -0.0218, -0.0303, 0.0090, 0.0009, -0.0059, 0.0047, -0.0149, -0.0364, -0.0213, 0.0151, 0.0148, 0.0186, -0.0135, 0.0105, 0.0293, 0.1226, 0.0057, -0.0366, 0.0019, -0.0199, 0.0339, 0.0305, -0.0020, -0.0159, 0.0146, -0.0081, 0.0179, -0.0184, -0.0295, -0.0021, -0.0045, -0.0225, 0.0009]\n",
            "Token 1( açò): [0.0091, 0.0006, 0.0052, 0.0179, -0.0036, -0.0319, -0.0023, 0.0118, 0.0274, 0.0156, 0.0241, 0.0069, 0.0423, -0.0051, 0.0024, -0.0230, 0.0096, -0.0289, -0.0047, 0.0220, -0.0098, 0.0136, 0.0140, 0.0459, 0.0131, -0.0079, 0.0059, 0.0063, -0.0649, 0.0013, 0.0266, -0.0320, -0.0089, -0.0282, -0.0070, 0.0151, 0.0292, -0.0267, -0.0057, 0.0157, 0.0061, 0.0041, 0.0233, 0.0007, -0.0093, 0.0349, 0.0139, 0.0189, -0.0091, 0.0071, 0.0374, -0.0152, -0.0266, 0.0049, -0.0026, 0.0288, 0.0086, -0.0048, -0.0273, 0.0156, -0.0141, -0.0061, 0.0348, 0.0691, -0.0044, 0.0155, 0.0515, 0.0285, 0.0305, 0.0315, -0.0004, 0.0016, 0.0022, -0.0117, 0.0067, -0.0087, 0.0230, -0.1009, -0.0154, -0.0004, -0.0014, -0.0023, 0.0005, -0.0099, -0.0472, 0.0095, -0.0072, -0.0002, -0.0141, 0.0015, 0.0228, 0.0342, 0.0202, 0.0396, -0.0109, 0.0158, 0.0444, -0.0005, -0.0429, -0.0292, -0.0177, 0.0115, 0.0084, 0.0073, 0.0031, 0.0071, -0.0393, -0.0053, 0.0050, 0.0307, -0.1395, -0.0048, 0.0065, -0.0192, 0.0439, -0.0047, 0.0172, 0.0067, 0.0138, -0.0089, -0.0331, 0.0164, 0.0205, -0.0205, -0.0252, 0.0099, -0.0255, 0.0026]\n",
            "Token 2(  es): [0.0017, 0.0060, -0.0118, 0.0131, 0.0272, -0.0257, -0.0117, 0.0046, 0.0146, 0.0216, 0.0034, 0.0235, 0.0381, 0.0106, 0.0016, -0.0219, 0.0033, -0.0068, -0.0222, 0.0260, 0.0017, -0.0090, 0.0074, 0.0521, -0.0109, -0.0045, -0.0027, -0.0129, -0.0411, -0.0192, 0.0301, -0.0281, -0.0227, -0.0138, 0.0035, 0.0041, 0.0421, 0.0084, 0.0058, 0.0125, 0.0129, -0.0158, -0.0063, -0.0001, -0.0177, 0.0330, 0.0133, -0.0031, -0.0130, 0.0290, 0.0270, -0.0232, -0.0020, -0.0018, -0.0089, -0.0002, -0.0235, -0.0140, -0.0175, 0.0021, 0.0039, 0.0198, 0.0191, 0.0638, -0.0138, 0.0129, 0.0130, 0.0007, 0.0223, 0.0123, -0.0094, -0.0235, 0.0067, -0.0116, -0.0182, 0.0034, -0.0045, -0.0479, 0.0052, -0.0063, -0.0053, -0.0055, -0.0045, 0.0054, -0.0299, 0.0024, -0.0056, 0.0005, -0.0050, 0.0118, 0.0035, 0.0011, 0.0145, 0.0106, -0.0023, 0.0003, -0.0092, 0.0014, -0.0079, -0.0064, -0.0038, 0.0025, -0.0107, -0.0062, 0.0111, 0.0023, 0.0042, -0.0120, 0.0047, 0.0158, -0.0023, 0.0088, -0.0229, 0.0001, 0.0004, 0.0018, 0.0111, -0.0032, -0.0039, 0.0058, -0.0110, 0.0118, -0.0097, -0.0132, -0.0122, 0.0022, -0.0197, -0.0029]\n",
            "Token 3(  or): [-0.0022, 0.0008, -0.0004, 0.0028, 0.0198, -0.0083, -0.0030, -0.0095, 0.0024, 0.0129, -0.0015, 0.0104, 0.0294, -0.0027, 0.0048, -0.0201, 0.0008, -0.0088, -0.0088, 0.0222, 0.0040, -0.0023, -0.0010, 0.0354, -0.0047, -0.0192, -0.0097, 0.0296, -0.0172, -0.0143, 0.0103, -0.0140, -0.0064, -0.0048, -0.0028, -0.0019, 0.0292, 0.0063, 0.0066, 0.0185, 0.0101, -0.0054, 0.0096, -0.0012, -0.0104, 0.0259, 0.0113, 0.0006, -0.0033, 0.0187, 0.0136, -0.0206, -0.0082, 0.0001, -0.0028, 0.0026, -0.0172, -0.0046, -0.0073, 0.0045, -0.0060, 0.0156, 0.0093, 0.0303, -0.0042, 0.0060, 0.0072, -0.0022, 0.0172, 0.0024, -0.0080, -0.0217, 0.0007, -0.0039, -0.0114, 0.0054, -0.0087, -0.0286, 0.0013, -0.0016, 0.0044, -0.0011, -0.0052, -0.0005, -0.0195, 0.0041, -0.0059, 0.0008, 0.0004, 0.0058, 0.0162, 0.0011, 0.0194, 0.0094, -0.0056, 0.0005, -0.0080, -0.0063, -0.0110, 0.0002, -0.0085, 0.0035, -0.0137, -0.0024, -0.0048, -0.0059, -0.0081, -0.0058, 0.0014, 0.0089, -0.0741, 0.0052, -0.0085, -0.0021, 0.0004, 0.0048, 0.0094, -0.0117, 0.0014, 0.0026, -0.0116, 0.0087, -0.0093, -0.0101, -0.0115, 0.0057, -0.0138, -0.0009]\n",
            "Token 4( ...): [-0.0093, 0.0069, -0.0062, 0.0172, 0.0305, -0.0184, -0.0111, -0.0210, 0.0001, 0.0134, -0.0057, 0.0240, 0.0233, -0.0008, 0.0035, -0.0200, -0.0036, -0.0061, -0.0133, 0.0184, 0.0088, -0.0163, 0.0086, 0.0426, -0.0102, -0.0096, -0.0090, -0.0129, -0.0156, -0.0149, 0.0042, -0.0202, -0.0152, -0.0086, -0.0036, -0.0008, 0.0328, 0.0105, 0.0109, 0.0158, 0.0159, -0.0149, -0.0016, -0.0038, -0.0174, 0.0257, 0.0116, -0.0034, -0.0107, 0.0196, 0.0085, -0.0314, -0.0115, -0.0054, -0.0113, -0.0016, -0.0172, -0.0102, -0.0154, 0.0046, 0.0001, 0.0257, 0.0148, 0.0367, -0.0119, 0.0074, 0.0117, -0.0049, 0.0198, 0.0045, -0.0063, -0.0229, 0.0060, -0.0004, -0.0184, 0.0076, -0.0104, 0.0168, 0.0066, 0.0020, -0.0015, -0.0026, -0.0119, 0.0010, -0.0183, 0.0065, -0.0039, -0.0039, 0.0039, 0.0088, 0.0104, 0.0025, 0.0179, -0.0031, -0.0094, -0.0066, -0.0152, -0.0052, -0.0037, -0.0029, 0.0022, -0.0079, -0.0134, -0.0022, 0.0043, 0.0071, 0.0001, -0.0075, -0.0004, 0.0104, -0.0468, 0.0087, -0.0214, -0.0044, -0.0143, 0.0111, 0.0137, -0.0113, -0.0133, 0.0055, -0.0124, 0.0007, -0.0099, -0.0178, -0.0120, -0.0002, -0.0147, -0.0044]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0055, 0.0139, -0.0012, -0.0055, 0.0251, -0.0036, 0.0015, -0.0022, -0.0732, 0.0079, -0.0148, 0.0687, 0.0036, 0.0005, 0.0201, 0.0075, -0.0117, 0.0190, 0.0304, -0.0223, 0.0071, -0.0002, 0.0273, -0.0095, -0.0012, 0.0283, 0.0040, 0.0132, -0.0066, 0.0220, 0.0036, 0.0009, 0.0129, 0.0181, -0.0057, -0.0031, 0.0045, 0.0143, 0.0093, -0.0031, 0.0114, 0.0489, -0.0122, 0.0292, -0.0012, 0.0085, 0.0165, -0.0084, 0.0140, 0.0126, -0.0089, -0.0227, -0.0112, -0.0071, 0.0103, -0.0059, 0.0055, 0.0089, -0.0063, -0.0031, -0.0033, -0.0002, -0.0066, -0.0092, -0.0054, -0.0083, 0.0177, -0.0101, 0.0079, 0.0222, 0.0193, 0.0034, 0.0088, -0.0045, 0.0040, 0.0077, 0.0002, 0.0144, -0.0081, -0.0013, -0.0100, -0.0006, -0.0259, -0.0131, 0.0034, 0.0045, 0.0063, 0.1089, -0.0027, -0.0056, -0.0120, -0.0134, 0.0021, 0.0010, -0.0113, 0.0281, -0.0135, -0.0065, 0.0098, 0.0206, -0.0175, 0.0132, 0.0058, 0.0153, 0.0041, 0.0014, 0.0211, -0.0076, 0.0025, 0.0004, 0.0026, 0.0061, 0.0317, 0.0028, -0.0218, -0.0232, 0.0152, 0.0148, -0.0182, -0.0057, 0.0162, -0.0056, 0.0038, -0.0057, -0.0133, 0.0034, 0.0156, -0.0053]\n",
            "Token 1( açò): [-0.0002, 0.0096, -0.0025, 0.0170, 0.0290, 0.0008, -0.0016, -0.0033, -0.0659, 0.0067, -0.0241, 0.0580, 0.0042, -0.0020, 0.0189, 0.0152, -0.0077, 0.0173, 0.0319, -0.0188, 0.0067, -0.0010, 0.0262, -0.0111, -0.0015, 0.0229, 0.0080, 0.0161, -0.0061, 0.0198, 0.0044, 0.0003, 0.0158, 0.0170, -0.0050, -0.0032, 0.0061, 0.0055, 0.0067, -0.0002, 0.0122, 0.0440, -0.0089, 0.0467, 0.0030, 0.0089, 0.0162, -0.0042, 0.0096, 0.0098, -0.0013, -0.0196, -0.0090, -0.0012, 0.0100, -0.0025, 0.0101, 0.0042, -0.0055, -0.0063, -0.0066, -0.0014, -0.0101, -0.0058, -0.0067, -0.0055, 0.0197, -0.0119, 0.0095, 0.0217, 0.0205, -0.0006, 0.0085, 0.0014, 0.0092, 0.0064, 0.0001, 0.0139, -0.0094, 0.0010, -0.0040, -0.0001, -0.0302, -0.0163, 0.0054, 0.0094, -0.0016, 0.0963, 0.0003, -0.0015, -0.0158, -0.0102, -0.0004, 0.0044, -0.0051, 0.0265, -0.0042, -0.0108, 0.0152, 0.0216, -0.0128, 0.0129, 0.0060, 0.0161, 0.0073, 0.0024, 0.0195, 0.0015, -0.0003, -0.0008, 0.0046, 0.0079, 0.0367, 0.0093, -0.0211, -0.0152, 0.0161, 0.0065, -0.0198, -0.0077, 0.0170, -0.0048, -0.0015, -0.0009, -0.0080, 0.0089, 0.0160, -0.0029]\n",
            "Token 2(  es): [0.0381, -0.0215, -0.0122, 0.1800, 0.0576, 0.0327, -0.0233, -0.0114, -0.0121, -0.0020, -0.0919, -0.0206, 0.0085, -0.0202, 0.0097, 0.0707, 0.0214, 0.0044, 0.0428, 0.0061, 0.0038, -0.0064, 0.0181, -0.0220, -0.0033, -0.0166, 0.0371, 0.0367, -0.0014, 0.0039, 0.0105, -0.0043, 0.0373, 0.0094, -0.0006, -0.0041, 0.0175, -0.0591, -0.0123, 0.0201, 0.0179, 0.0079, 0.0146, 0.1747, 0.0338, 0.0119, 0.0134, 0.0266, -0.0228, -0.0106, 0.0540, 0.0034, 0.0067, 0.0418, 0.0082, 0.0223, 0.0432, -0.0301, 0.0005, -0.0298, -0.0302, -0.0110, -0.0348, 0.0189, -0.0158, 0.0150, 0.0337, -0.0255, 0.0207, 0.0173, 0.0293, -0.0296, 0.0063, 0.0450, 0.0463, -0.0032, -0.0006, 0.0102, -0.0194, 0.0181, 0.0398, 0.0039, -0.0621, -0.0397, 0.0197, 0.0453, -0.0589, 0.0030, 0.0222, 0.0288, -0.0433, 0.0133, -0.0175, 0.0294, 0.0398, 0.0138, 0.0632, -0.0426, 0.0542, 0.0288, 0.0224, 0.0106, 0.0075, 0.0211, 0.0303, 0.0094, 0.0068, 0.0682, -0.0207, -0.0097, 0.0188, 0.0215, 0.0724, 0.0554, -0.0159, 0.0431, 0.0224, -0.0539, -0.0321, -0.0227, 0.0227, 0.0008, -0.0402, 0.0336, 0.0312, 0.0494, 0.0185, 0.0151]\n",
            "Token 3(  or): [0.0026, 0.0065, -0.0039, 0.0315, 0.0319, 0.0039, -0.0034, -0.0043, -0.0591, 0.0058, -0.0317, 0.0492, 0.0044, -0.0034, 0.0174, 0.0199, -0.0054, 0.0154, 0.0320, -0.0167, 0.0066, -0.0006, 0.0248, -0.0113, -0.0014, 0.0190, 0.0103, 0.0173, -0.0056, 0.0180, 0.0044, 0.0005, 0.0180, 0.0163, -0.0049, -0.0029, 0.0074, -0.0009, 0.0049, 0.0011, 0.0125, 0.0395, -0.0069, 0.0589, 0.0057, 0.0093, 0.0155, -0.0011, 0.0061, 0.0083, 0.0038, -0.0168, -0.0078, 0.0029, 0.0100, -0.0002, 0.0128, 0.0009, -0.0047, -0.0085, -0.0089, -0.0024, -0.0119, -0.0036, -0.0072, -0.0030, 0.0211, -0.0130, 0.0099, 0.0206, 0.0212, -0.0029, 0.0084, 0.0051, 0.0118, 0.0057, 0.0002, 0.0134, -0.0102, 0.0026, 0.0002, 0.0004, -0.0326, -0.0183, 0.0067, 0.0127, -0.0066, 0.0853, 0.0025, 0.0013, -0.0180, -0.0078, -0.0017, 0.0066, -0.0011, 0.0247, 0.0016, -0.0136, 0.0185, 0.0222, -0.0087, 0.0121, 0.0055, 0.0153, 0.0092, 0.0028, 0.0175, 0.0078, -0.0025, -0.0019, 0.0057, 0.0087, 0.0382, 0.0132, -0.0206, -0.0096, 0.0162, 0.0007, -0.0209, -0.0088, 0.0175, -0.0043, -0.0047, 0.0020, -0.0031, 0.0123, 0.0158, -0.0005]\n",
            "Token 4( ...): [-0.0050, 0.0134, -0.0012, -0.0039, 0.0254, -0.0031, 0.0011, -0.0023, -0.0717, 0.0077, -0.0170, 0.0672, 0.0036, 0.0003, 0.0198, 0.0079, -0.0113, 0.0187, 0.0300, -0.0219, 0.0070, -0.0002, 0.0270, -0.0095, -0.0012, 0.0278, 0.0043, 0.0134, -0.0068, 0.0217, 0.0035, 0.0011, 0.0133, 0.0179, -0.0057, -0.0029, 0.0047, 0.0133, 0.0091, -0.0028, 0.0115, 0.0478, -0.0118, 0.0311, -0.0009, 0.0086, 0.0164, -0.0078, 0.0135, 0.0124, -0.0082, -0.0222, -0.0110, -0.0065, 0.0103, -0.0057, 0.0058, 0.0084, -0.0060, -0.0034, -0.0038, -0.0003, -0.0070, -0.0088, -0.0054, -0.0080, 0.0179, -0.0102, 0.0079, 0.0221, 0.0193, 0.0032, 0.0088, -0.0040, 0.0044, 0.0077, 0.0002, 0.0143, -0.0081, -0.0012, -0.0093, -0.0006, -0.0260, -0.0134, 0.0037, 0.0048, 0.0055, 0.1072, -0.0024, -0.0051, -0.0122, -0.0129, 0.0017, 0.0014, -0.0106, 0.0280, -0.0126, -0.0072, 0.0101, 0.0205, -0.0169, 0.0131, 0.0056, 0.0151, 0.0045, 0.0013, 0.0210, -0.0069, 0.0020, 0.0002, 0.0029, 0.0062, 0.0318, 0.0036, -0.0216, -0.0223, 0.0151, 0.0138, -0.0183, -0.0059, 0.0164, -0.0055, 0.0036, -0.0053, -0.0129, 0.0039, 0.0156, -0.0049]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0426, -0.0096, -0.0343, 0.0244, -0.0478, -0.0356, 0.0070, -0.0133, 0.0226, 0.0381, 0.0267, -0.0337, 0.0052, -0.0321, 0.0465, 0.0408, -0.0201, -0.0082, 0.1777, 0.0133, -0.0273, 0.0305, -0.0170, -0.0181, 0.0003, -0.0178, -0.0150, 0.0299, 0.0174, 0.0020, 0.0276, 0.0131, -0.0018, 0.0175, 0.0361, -0.0124, 0.0980, 0.0267, 0.0003, 0.0670, 0.0405, -0.0489, -0.0274, -0.0258, 0.0312, -0.0348, -0.0237, 0.0058, -0.0261, 0.0438, 0.0029, -0.0107, 0.0158, 0.0279, -0.0304, -0.0152, 0.0388, -0.0024, 0.0142, -0.0005, 0.0241, 0.0438, -0.0350, 0.0179, 0.0080, 0.0016, 0.0113, -0.0008, 0.0144, 0.0534, -0.0531, -0.0081, 0.0383, -0.0376, 0.0016, 0.1004, -0.0007, -0.0236, 0.0716, -0.0640, 0.0194, 0.0254, 0.0323, 0.0060, -0.0135, -0.0411, -0.0492, 0.0072, -0.0336, -0.0192, 0.0542, 0.0818, -0.0420, 0.0201, -0.0037, -0.0138, -0.0026, 0.0002, -0.0119, -0.0106, -0.0442, -0.0095, 0.0305, -0.0138, -0.0141, 0.0267, 0.0279, -0.0399, 0.0395, -0.0810, -0.0254, -0.0475, -0.0272, 0.0032, 0.0134, -0.0181, -0.0056, -0.0121, 0.0068, 0.0228, -0.0353, 0.0179, -0.0057, 0.0463, -0.0073, -0.0316, 0.0095, 0.0244]\n",
            "Token 1( açò): [0.0380, -0.0089, -0.0264, 0.0192, -0.0403, -0.0312, 0.0082, -0.0110, 0.0237, 0.0326, 0.0235, -0.0292, 0.0041, -0.0317, 0.0398, 0.0381, -0.0239, -0.0068, 0.1287, 0.0144, -0.0225, 0.0222, -0.0142, -0.0204, 0.0019, -0.0163, -0.0090, 0.0247, 0.0119, -0.0017, 0.0200, 0.0103, -0.0050, 0.0184, 0.0302, -0.0157, 0.0831, 0.0238, 0.0005, 0.0597, 0.0351, -0.0454, -0.0266, -0.0228, 0.0261, -0.0268, -0.0196, 0.0053, -0.0230, 0.0384, 0.0009, -0.0095, 0.0128, 0.0286, -0.0259, -0.0117, 0.0340, -0.0012, 0.0152, -0.0030, 0.0201, 0.0400, -0.0352, 0.0137, 0.0084, -0.0001, 0.0077, -0.0051, 0.0090, 0.0489, -0.0448, -0.0111, 0.0381, -0.0343, 0.0003, 0.0949, -0.0027, -0.0236, 0.0639, -0.0525, 0.0183, 0.0212, 0.0299, 0.0042, -0.0119, -0.0328, -0.0472, 0.0008, -0.0290, -0.0197, 0.0479, 0.0712, -0.0392, 0.0172, -0.0041, -0.0132, 0.0001, 0.0006, -0.0083, -0.0090, -0.0367, -0.0093, 0.0266, -0.0151, -0.0139, 0.0199, 0.0192, -0.0365, 0.0389, -0.0708, -0.0238, -0.0440, -0.0209, 0.0071, 0.0083, -0.0192, 0.0010, -0.0097, 0.0016, 0.0208, -0.0327, 0.0154, -0.0011, 0.0435, -0.0072, -0.0292, 0.0084, 0.0224]\n",
            "Token 2(  es): [0.0376, -0.0070, -0.0280, 0.0205, -0.0387, -0.0322, 0.0075, -0.0091, 0.0214, 0.0329, 0.0231, -0.0295, 0.0059, -0.0299, 0.0417, 0.0371, -0.0213, -0.0058, 0.1272, 0.0130, -0.0217, 0.0254, -0.0126, -0.0180, 0.0009, -0.0146, -0.0125, 0.0250, 0.0137, -0.0005, 0.0214, 0.0088, -0.0028, 0.0176, 0.0298, -0.0120, 0.0846, 0.0246, -0.0002, 0.0578, 0.0346, -0.0433, -0.0274, -0.0221, 0.0270, -0.0277, -0.0200, 0.0034, -0.0219, 0.0371, 0.0012, -0.0096, 0.0119, 0.0257, -0.0258, -0.0121, 0.0330, -0.0009, 0.0132, -0.0039, 0.0186, 0.0377, -0.0336, 0.0134, 0.0083, -0.0014, 0.0074, -0.0020, 0.0100, 0.0470, -0.0440, -0.0100, 0.0361, -0.0337, 0.0012, 0.0915, -0.0007, -0.0251, 0.0625, -0.0520, 0.0197, 0.0227, 0.0274, 0.0031, -0.0117, -0.0347, -0.0444, 0.0032, -0.0316, -0.0174, 0.0486, 0.0695, -0.0384, 0.0179, -0.0053, -0.0145, -0.0022, -0.0006, -0.0098, -0.0088, -0.0376, -0.0084, 0.0263, -0.0132, -0.0136, 0.0235, 0.0222, -0.0352, 0.0366, -0.0699, -0.0234, -0.0423, -0.0231, 0.0072, 0.0092, -0.0157, -0.0017, -0.0089, 0.0026, 0.0198, -0.0295, 0.0177, -0.0028, 0.0414, -0.0070, -0.0271, 0.0068, 0.0219]\n",
            "Token 3(  or): [0.0363, -0.0068, -0.0286, 0.0205, -0.0390, -0.0318, 0.0064, -0.0092, 0.0225, 0.0318, 0.0225, -0.0289, 0.0061, -0.0295, 0.0416, 0.0362, -0.0211, -0.0063, 0.1250, 0.0136, -0.0220, 0.0266, -0.0132, -0.0187, -0.0001, -0.0147, -0.0130, 0.0258, 0.0136, 0.0005, 0.0219, 0.0093, -0.0020, 0.0181, 0.0292, -0.0108, 0.0844, 0.0234, 0.0004, 0.0583, 0.0342, -0.0432, -0.0272, -0.0218, 0.0260, -0.0286, -0.0210, 0.0035, -0.0234, 0.0367, 0.0018, -0.0094, 0.0120, 0.0253, -0.0262, -0.0123, 0.0331, -0.0014, 0.0136, -0.0014, 0.0185, 0.0375, -0.0331, 0.0140, 0.0075, -0.0006, 0.0075, -0.0019, 0.0107, 0.0471, -0.0457, -0.0090, 0.0362, -0.0328, 0.0005, 0.0908, -0.0005, -0.0235, 0.0629, -0.0517, 0.0191, 0.0223, 0.0271, 0.0021, -0.0119, -0.0348, -0.0432, 0.0027, -0.0322, -0.0182, 0.0482, 0.0701, -0.0378, 0.0181, -0.0059, -0.0147, -0.0029, -0.0001, -0.0099, -0.0092, -0.0373, -0.0081, 0.0261, -0.0133, -0.0129, 0.0251, 0.0222, -0.0350, 0.0367, -0.0697, -0.0231, -0.0415, -0.0232, 0.0062, 0.0096, -0.0144, -0.0009, -0.0096, 0.0034, 0.0191, -0.0296, 0.0176, -0.0033, 0.0417, -0.0060, -0.0270, 0.0068, 0.0212]\n",
            "Token 4( ...): [0.0350, -0.0067, -0.0285, 0.0224, -0.0437, -0.0323, 0.0054, -0.0095, 0.0212, 0.0340, 0.0236, -0.0280, 0.0059, -0.0286, 0.0403, 0.0365, -0.0202, -0.0054, 0.1215, 0.0122, -0.0226, 0.0255, -0.0131, -0.0173, -0.0019, -0.0132, -0.0183, 0.0250, 0.0140, 0.0002, 0.0228, 0.0112, -0.0025, 0.0187, 0.0306, -0.0097, 0.0841, 0.0239, -0.0015, 0.0597, 0.0339, -0.0431, -0.0259, -0.0208, 0.0265, -0.0302, -0.0215, 0.0031, -0.0250, 0.0376, 0.0008, -0.0093, 0.0123, 0.0244, -0.0258, -0.0138, 0.0329, 0.0006, 0.0120, -0.0019, 0.0217, 0.0311, -0.0323, 0.0148, 0.0080, 0.0007, 0.0073, -0.0026, 0.0151, 0.0462, -0.0473, -0.0065, 0.0350, -0.0301, 0.0011, 0.0902, -0.0019, -0.0240, 0.0623, -0.0527, 0.0190, 0.0222, 0.0259, 0.0026, -0.0124, -0.0367, -0.0410, 0.0055, -0.0317, -0.0172, 0.0469, 0.0685, -0.0364, 0.0192, -0.0068, -0.0165, -0.0009, 0.0005, -0.0106, -0.0098, -0.0352, -0.0080, 0.0289, -0.0121, -0.0126, 0.0252, 0.0223, -0.0343, 0.0366, -0.0674, -0.0228, -0.0439, -0.0240, 0.0045, 0.0088, -0.0134, -0.0032, -0.0105, 0.0030, 0.0182, -0.0294, 0.0182, -0.0034, 0.0385, -0.0047, -0.0277, 0.0080, 0.0188]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0082, 0.0044, -0.0388, 0.0688, -0.0155, -0.0115, 0.0115, 0.0654, -0.0244, 0.0053, 0.0018, -0.0656, -0.2784, -0.0140, -0.0084, -0.0213, -0.0501, -0.1128, -0.0044, -0.0102, 0.0006, 0.0030, -0.0028, -0.0827, 0.0802, -0.0160, 0.0056, 0.0223, -0.0234, 0.1145, 0.0095, 0.0337, 0.0387, 0.0022, 0.0051, 0.0031, 0.0525, -0.0164, -0.0345, 0.0089, -0.0272, -0.0330, -0.0132, 0.0019, 0.0195, 0.0224, -0.1362, 0.0201, 0.0272, 0.0109, 0.1376, 0.0112, -0.0089, 0.0290, 0.0076, -0.0390, 0.0390, -0.1032, -0.0040, -0.0001, -0.0326, 0.0029, 0.0285, -0.0119, -0.0068, 0.0038, -0.0639, 0.0073, -0.0255, 0.0892, -0.0094, -0.0070, -0.1912, 0.0324, 0.0044, 0.0051, -0.0011, 0.0104, 0.0325, 0.0066, -0.0412, 0.0335, 0.0107, 0.3033, -0.0347, 0.3077, 0.0069, 0.0240, -0.0177, -0.0010, 0.0330, 0.0083, -0.0235, -0.0332, 0.0038, -0.0553, -0.0496, -0.0037, -0.0268, -0.0617, -0.0153, -0.0212, 0.0197, 0.0494, -0.0094, -0.0758, -0.0527, -0.0053, 0.0151, 0.0351, 0.0119, 0.0151, 0.0172, -0.0358, 0.0389, 0.0642, 0.0308, 0.0001, -0.0109, 0.0588, -0.0568, 0.0008, -0.0115, -0.0285, 0.0363, -0.0448, 0.0119, -0.0096]\n",
            "Token 1( açò): [-0.0070, 0.0235, -0.0010, 0.0057, -0.0194, 0.0007, 0.0722, 0.0292, -0.0426, 0.0064, 0.0008, -0.0193, -0.1551, -0.0032, -0.0088, -0.0101, -0.0133, -0.0793, -0.0017, -0.0255, -0.0053, -0.0102, 0.0007, -0.0857, 0.0276, -0.0151, -0.0171, 0.0090, -0.0180, 0.0383, -0.0054, 0.0259, 0.0203, -0.0021, 0.0125, 0.0040, 0.0608, -0.0223, 0.0018, -0.0188, -0.0231, -0.0300, 0.0008, 0.0108, -0.0068, 0.0200, -0.0716, 0.0194, 0.0018, -0.0067, 0.0709, 0.0129, -0.0128, 0.0232, -0.0055, -0.0036, 0.0762, -0.0568, -0.0071, -0.0068, -0.0467, -0.0027, 0.0352, -0.0050, 0.0217, -0.0021, -0.0485, -0.0104, 0.0114, 0.0383, -0.0140, 0.0151, -0.0928, 0.0251, 0.0172, 0.0281, 0.0129, 0.0094, 0.0301, -0.0321, -0.0224, -0.0002, 0.0251, 0.1675, 0.0013, 0.1269, -0.0121, 0.0230, -0.0074, -0.0062, 0.0308, 0.0102, -0.0210, -0.0190, -0.0086, -0.0189, -0.0223, 0.0164, 0.0440, 0.0147, -0.0434, -0.0185, 0.0008, 0.0125, -0.0167, -0.0278, -0.0103, -0.0145, 0.0320, -0.0071, -0.0065, 0.0029, 0.0339, -0.0259, 0.0295, 0.0377, 0.0146, 0.0134, -0.0136, 0.0384, -0.0349, 0.0202, 0.0054, 0.0205, 0.0222, -0.0589, 0.0049, -0.0008]\n",
            "Token 2(  es): [-0.0105, 0.0034, -0.0295, 0.0535, -0.0128, -0.0036, 0.0257, 0.0509, -0.0234, 0.0034, 0.0016, -0.0455, -0.2161, -0.0091, -0.0082, -0.0225, -0.0380, -0.0932, -0.0061, -0.0075, 0.0011, -0.0010, -0.0011, -0.0630, 0.0597, -0.0164, 0.0041, 0.0178, -0.0158, 0.0866, 0.0062, 0.0277, 0.0320, 0.0007, 0.0123, 0.0029, 0.0521, -0.0143, -0.0166, -0.0022, -0.0233, -0.0271, -0.0098, 0.0041, 0.0124, 0.0195, -0.1088, 0.0220, 0.0199, 0.0069, 0.1046, 0.0136, -0.0103, 0.0193, 0.0017, -0.0298, 0.0358, -0.0758, -0.0021, -0.0095, -0.0336, -0.0001, 0.0264, -0.0128, 0.0019, 0.0008, -0.0605, -0.0002, -0.0151, 0.0731, -0.0131, -0.0072, -0.1499, 0.0261, -0.0014, 0.0156, 0.0019, 0.0150, 0.0269, -0.0006, -0.0356, 0.0245, 0.0165, 0.2389, -0.0288, 0.2332, -0.0001, 0.0260, -0.0130, -0.0008, 0.0263, 0.0089, -0.0231, -0.0291, 0.0059, -0.0392, -0.0417, 0.0012, -0.0075, -0.0410, -0.0175, -0.0142, 0.0131, 0.0308, -0.0108, -0.0726, -0.0381, -0.0024, 0.0107, 0.0203, 0.0099, 0.0089, 0.0233, -0.0273, 0.0317, 0.0550, 0.0279, 0.0049, -0.0086, 0.0488, -0.0438, 0.0062, -0.0045, -0.0164, 0.0272, -0.0395, 0.0083, -0.0082]\n",
            "Token 3(  or): [-0.0209, -0.0013, -0.0027, 0.0059, -0.0030, 0.0067, 0.0457, 0.0169, -0.0198, 0.0006, 0.0013, -0.0072, -0.0922, 0.0058, -0.0080, -0.0176, -0.0127, -0.0620, -0.0114, 0.0054, -0.0012, -0.0088, 0.0086, -0.0024, 0.0149, -0.0075, 0.0031, 0.0032, -0.0189, 0.0302, -0.0054, 0.0260, 0.0217, -0.0014, 0.0121, 0.0013, 0.0460, -0.0183, 0.0095, -0.0102, -0.0062, -0.0226, -0.0041, 0.0163, 0.0044, 0.0006, -0.0562, 0.0093, 0.0005, 0.0109, 0.0357, 0.0141, -0.0026, 0.0129, -0.0024, -0.0077, 0.0333, -0.0323, -0.0029, -0.0112, -0.0364, 0.0020, 0.0165, -0.0191, 0.0228, -0.0070, -0.0442, -0.0176, -0.0055, 0.0425, -0.0175, 0.0038, -0.0712, -0.0033, 0.0025, 0.0206, 0.0110, 0.0121, 0.0201, -0.0104, -0.0166, 0.0020, 0.0142, 0.1059, -0.0184, 0.0798, -0.0019, 0.0226, 0.0080, 0.0002, 0.0105, 0.0006, -0.0023, -0.0235, 0.0053, -0.0153, -0.0305, 0.0101, 0.0289, -0.0088, -0.0270, -0.0068, 0.0015, 0.0002, -0.0116, -0.0789, -0.0041, 0.0021, 0.0104, 0.0029, 0.0038, 0.0126, 0.0419, -0.0182, 0.0219, 0.0233, 0.0129, 0.0117, -0.0030, 0.0280, -0.0205, 0.0134, 0.0090, 0.0050, 0.0193, -0.0120, 0.0050, -0.0014]\n",
            "Token 4( ...): [-0.0096, 0.0046, -0.0366, 0.0630, -0.0145, -0.0093, 0.0136, 0.0620, -0.0233, 0.0054, 0.0020, -0.0616, -0.2582, -0.0140, -0.0082, -0.0208, -0.0467, -0.1082, -0.0047, -0.0090, 0.0008, 0.0011, -0.0026, -0.0715, 0.0722, -0.0148, 0.0059, 0.0199, -0.0213, 0.1087, 0.0075, 0.0317, 0.0378, 0.0005, 0.0056, 0.0015, 0.0496, -0.0154, -0.0304, 0.0080, -0.0253, -0.0305, -0.0100, 0.0009, 0.0177, 0.0207, -0.1223, 0.0207, 0.0240, 0.0102, 0.1254, 0.0122, -0.0083, 0.0276, 0.0066, -0.0357, 0.0363, -0.0940, -0.0041, -0.0016, -0.0315, 0.0025, 0.0267, -0.0114, -0.0043, 0.0023, -0.0471, 0.0055, -0.0227, 0.0840, -0.0095, -0.0067, -0.1776, 0.0300, 0.0036, 0.0065, -0.0002, 0.0103, 0.0314, 0.0041, -0.0393, 0.0316, 0.0117, 0.2793, -0.0322, 0.2922, 0.0058, 0.0237, -0.0157, -0.0011, 0.0314, 0.0079, -0.0220, -0.0318, 0.0043, -0.0511, -0.0515, -0.0029, -0.0230, -0.0534, -0.0151, -0.0200, 0.0172, 0.0430, -0.0085, -0.0787, -0.0463, -0.0051, 0.0134, 0.0319, 0.0101, 0.0141, 0.0164, -0.0324, 0.0369, 0.0615, 0.0270, 0.0001, -0.0097, 0.0553, -0.0513, 0.0023, -0.0101, -0.0225, 0.0340, -0.0454, 0.0119, -0.0086]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0358, 0.0062, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0192, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 1( açò): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0192, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0069, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0085, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 2(  es): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0019, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0223, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0085, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0165, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0092, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 3(  or): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0186, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0223, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0071, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0165, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0092, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 4( ...): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0358, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0069, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0071, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0407, 0.0040, -0.0497, -0.0331, 0.0371, -0.0452, -0.0056, 0.0236, -0.0285, 0.0027, -0.0325, -0.0657, -0.0058, 0.0110, 0.0444, 0.0132, 0.0108, 0.0178, 0.0607, -0.0131, 0.0115, 0.0075, 0.0325, -0.0135, 0.0385, 0.0438, 0.0078, 0.0472, 0.1044, 0.0118, 0.0202, -0.0145, -0.0153, -0.0284, -0.0293, -0.0131, 0.1406, 0.0315, -0.0288, -0.0296, -0.0457, 0.0093, -0.0108, 0.0410, -0.0025, 0.0497, -0.0323, 0.0217, 0.0047, 0.0358, -0.0100, 0.0260, 0.0499, -0.0337, -0.0065, 0.0330, -0.0308, -0.0198, -0.0612, -0.0367, -0.0313, 0.0033, -0.0048, -0.0335, -0.0262, -0.0835, -0.0458, 0.0185, 0.0034, 0.0003, 0.0006, -0.0087, -0.0207, -0.0105, -0.0033, 0.0349, -0.0087, 0.0042, 0.0083, -0.0081, 0.0115, 0.0034, 0.0122, -0.0185, -0.0353, -0.0268, 0.0150, -0.0414, 0.0096, 0.0236, -0.0289, -0.0392, 0.0180, 0.0320, -0.0395, 0.0245, 0.0237, -0.0432, 0.0076, 0.0048, -0.0359, -0.0407, -0.0110, -0.0209, 0.0097, -0.0135, -0.0082, 0.0389, -0.0310, 0.0027, 0.0046, 0.0221, 0.0085, -0.0060, 0.1247, -0.0413, -0.0394, -0.0230, -0.0387, 0.0891, 0.0645, -0.0384, -0.0269, -0.0325, 0.0308, -0.0027, 0.0419, 0.0007]\n",
            "Token 1( açò): [0.0179, -0.0031, -0.0218, -0.0029, 0.0297, -0.0305, 0.0133, 0.0186, -0.0027, 0.0117, -0.0190, -0.0387, -0.0031, 0.0068, 0.0385, 0.0365, -0.0134, 0.0281, 0.0072, -0.0174, -0.0055, -0.0201, 0.0152, -0.0232, -0.0101, -0.0010, -0.0136, 0.0244, 0.0465, 0.0403, 0.0301, -0.0019, 0.0104, -0.0306, -0.0114, 0.0157, 0.0183, 0.0139, -0.0265, 0.0005, -0.0307, -0.0050, 0.0149, 0.0291, 0.1674, 0.0340, -0.0438, -0.0004, -0.0093, 0.0136, -0.0236, 0.0123, 0.0277, -0.0035, 0.0029, 0.0139, -0.0242, 0.0138, -0.0169, -0.0182, 0.0024, -0.0111, -0.0089, -0.0083, -0.0018, -0.0374, -0.0100, 0.0213, 0.0242, 0.0300, 0.0288, -0.0165, -0.0252, -0.0010, 0.0021, 0.0407, 0.0214, 0.0204, -0.0053, -0.0152, -0.0134, 0.0120, 0.0221, -0.0116, -0.0253, -0.0265, 0.0256, -0.0489, 0.0021, 0.0017, -0.0106, -0.0236, 0.0161, 0.0184, 0.0047, 0.0018, 0.0168, -0.0020, -0.0022, -0.0042, -0.0072, -0.0191, -0.0131, 0.0077, 0.0155, -0.0080, 0.0314, 0.0114, -0.0180, -0.0182, 0.0296, 0.0221, 0.0044, -0.0209, 0.3196, -0.0041, 0.0024, -0.0120, -0.0275, 0.1755, 0.0209, -0.0435, 0.0108, 0.0070, 0.0353, -0.0144, 0.0458, 0.0009]\n",
            "Token 2(  es): [0.0106, -0.0129, -0.0113, 0.0085, 0.0243, -0.0257, 0.0107, 0.0160, 0.0028, 0.0032, -0.0124, -0.0180, -0.0095, 0.0036, 0.0253, 0.0252, -0.0090, 0.0330, 0.0037, -0.0175, -0.0164, -0.0164, 0.0022, -0.0140, -0.0103, -0.0078, -0.0173, 0.0194, 0.0296, 0.0243, 0.0212, 0.0057, 0.0117, -0.0239, -0.0089, 0.0068, -0.0013, 0.0082, -0.0204, 0.0064, -0.0185, 0.0021, 0.0102, 0.0181, 0.1137, 0.0279, -0.0357, -0.0041, -0.0071, -0.0076, -0.0113, 0.0105, 0.0162, -0.0040, 0.0043, 0.0068, -0.0097, 0.0037, -0.0063, 0.0011, 0.0022, -0.0222, 0.0042, -0.0024, -0.0022, -0.0184, -0.0125, 0.0133, 0.0242, 0.0305, 0.0184, -0.0098, -0.0115, 0.0002, 0.0009, 0.0296, 0.0278, 0.0171, -0.0070, -0.0021, -0.0163, 0.0102, 0.0134, 0.0042, -0.0167, -0.0151, 0.0194, -0.0389, 0.0024, -0.0038, -0.0209, -0.0119, 0.0067, 0.0143, 0.0081, -0.0059, 0.0128, 0.0042, -0.0070, -0.0029, -0.0099, -0.0150, -0.0162, 0.0164, 0.0170, 0.0060, 0.0277, 0.0083, -0.0173, -0.0161, 0.0159, 0.0174, 0.0137, -0.0049, 0.3082, -0.0052, 0.0060, -0.0050, -0.0161, 0.1604, 0.0158, -0.0349, 0.0135, 0.0132, 0.0287, -0.0194, 0.0431, 0.0025]\n",
            "Token 3(  or): [0.0108, -0.0164, -0.0133, 0.0067, 0.0190, -0.0247, 0.0116, 0.0158, -0.0005, 0.0037, -0.0110, -0.0144, -0.0047, 0.0056, 0.0252, 0.0195, -0.0101, 0.0243, 0.0057, -0.0098, -0.0147, -0.0139, 0.0042, -0.0107, -0.0080, -0.0089, -0.0105, 0.0142, 0.0221, -0.0008, 0.0179, 0.0080, 0.0063, -0.0220, -0.0118, 0.0002, 0.0020, 0.0057, -0.0218, 0.0012, -0.0202, 0.0025, 0.0043, 0.0154, 0.0684, 0.0271, -0.0319, -0.0015, -0.0030, -0.0033, -0.0114, 0.0083, 0.0153, -0.0031, 0.0002, 0.0083, -0.0083, -0.0011, -0.0123, 0.0078, -0.0025, -0.0172, 0.0040, -0.0029, -0.0061, -0.0210, -0.0122, 0.0057, 0.0160, 0.0243, 0.0190, -0.0071, -0.0078, 0.0014, 0.0013, 0.0239, 0.0215, 0.0087, -0.0051, -0.0018, -0.0163, 0.0077, 0.0089, 0.0021, -0.0178, -0.0092, 0.0155, -0.0297, 0.0029, -0.0018, -0.0169, -0.0090, 0.0103, 0.0100, 0.0035, -0.0078, 0.0088, 0.0010, -0.0076, -0.0019, -0.0104, -0.0131, -0.0114, 0.0084, 0.0173, 0.0039, 0.0233, 0.0079, -0.0168, -0.0147, 0.0200, 0.0135, 0.0093, -0.0054, 0.2889, -0.0074, 0.0033, -0.0063, -0.0105, 0.1562, 0.0159, -0.0258, 0.0112, 0.0085, 0.0229, -0.0155, 0.0398, -0.0040]\n",
            "Token 4( ...): [0.0054, -0.0166, -0.0090, 0.0128, 0.0128, -0.0199, 0.0099, 0.0155, -0.0030, 0.0058, -0.0064, -0.0095, -0.0044, 0.0036, 0.0194, 0.0135, -0.0097, 0.0242, 0.0011, -0.0114, -0.0162, -0.0104, 0.0034, -0.0118, -0.0095, -0.0042, -0.0090, 0.0105, 0.0210, -0.0143, 0.0168, 0.0100, 0.0070, -0.0203, -0.0081, -0.0005, 0.0042, 0.0039, -0.0183, -0.0004, -0.0161, 0.0058, 0.0015, 0.0133, 0.0273, 0.0265, -0.0277, -0.0033, -0.0026, -0.0102, 0.0006, 0.0094, 0.0145, -0.0034, 0.0017, 0.0073, -0.0070, -0.0006, -0.0068, 0.0084, -0.0040, -0.0168, 0.0093, -0.0046, -0.0002, -0.0158, -0.0130, 0.0080, 0.0162, 0.0235, 0.0093, -0.0065, -0.0075, 0.0057, -0.0045, 0.0270, 0.0239, 0.0077, -0.0062, 0.0002, -0.0144, 0.0104, 0.0077, 0.0060, -0.0168, -0.0030, 0.0139, -0.0283, 0.0066, -0.0035, -0.0184, -0.0073, 0.0067, 0.0098, 0.0036, -0.0093, 0.0048, 0.0007, -0.0088, 0.0000, -0.0105, -0.0098, -0.0087, 0.0120, 0.0161, 0.0056, 0.0231, 0.0076, -0.0149, -0.0105, 0.0116, 0.0121, 0.0115, -0.0028, 0.2595, -0.0080, 0.0036, -0.0045, -0.0062, 0.1414, 0.0136, -0.0230, 0.0078, 0.0080, 0.0201, -0.0140, 0.0357, -0.0028]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0182, 0.0278, -0.0238, 0.0184, -0.0484, -0.0129, -0.0744, -0.0377, 0.0210, 0.0746, -0.0314, -0.2231, -0.0435, -0.0089, -0.0150, -0.0390, -0.0371, -0.0254, -0.0239, -0.0260, 0.0078, -0.0231, 0.0109, 0.0994, -0.0080, 0.4786, -0.0564, -0.0219, -0.0073, -0.0100, 0.0057, -0.0898, -0.0445, 0.0153, 0.0170, -0.0438, -0.0153, -0.0398, -0.0501, 0.0140, -0.0388, 0.0280, 0.1396, 0.0470, -0.0258, 0.0049, 0.0089, -0.0369, -0.0166, -0.0142, -0.0206, 0.0254, -0.0579, 0.0036, 0.0057, -0.0407, 0.0027, -0.0016, 0.0671, 0.0408, -0.1123, 0.0057, -0.0251, -0.0340, -0.0063, 0.0677, 0.0105, 0.0189, 0.0103, 0.0971, 0.0243, 0.0232, -0.0005, -0.0012, -0.0069, -0.2391, 0.0272, -0.0959, 0.0056, 0.0062, -0.0495, 0.1669, 0.0021, 0.0177, -0.0288, 0.0043, -0.0005, 0.0352, -0.0418, 0.1269, 0.0270, -0.0283, -0.0291, -0.0242, -0.0116, -0.0298, -0.0332, 0.0427, -0.0112, 0.0025, -0.0091, 0.0133, -0.0210, 0.0138, -0.0229, 0.1473, -0.0059, 0.0106, 0.0180, 0.0127, -0.1283, -0.0005, 0.1513, 0.0197, -0.0378, -0.0330, 0.0355, -0.0313, 0.0141, -0.0028, 0.2614, 0.0045, 0.0306, -0.0921, 0.0130, -0.0359, 0.0489, -0.0099]\n",
            "Token 1( açò): [0.0065, 0.0147, -0.0556, 0.0101, -0.0283, 0.0082, -0.0237, -0.0187, 0.0102, 0.0352, -0.0042, -0.0764, -0.0302, 0.0028, -0.0049, -0.0133, -0.0193, -0.0168, -0.0177, -0.0098, -0.0015, -0.0200, 0.0082, 0.0569, -0.0097, 0.4528, -0.0242, -0.0055, -0.0134, -0.0124, 0.0101, -0.0507, -0.0174, 0.0201, 0.0153, -0.0243, -0.0011, -0.0391, -0.0256, -0.0197, 0.0005, 0.0374, 0.0861, 0.0104, -0.0294, 0.0020, 0.0162, -0.0194, 0.0092, -0.0181, -0.0244, 0.0309, -0.0205, 0.0126, 0.0140, -0.0156, -0.0208, -0.0114, 0.0527, 0.0259, -0.0699, -0.0127, 0.0086, -0.0400, -0.0271, 0.0794, -0.0086, 0.0189, -0.0082, 0.0699, 0.0138, 0.0263, -0.0060, 0.0149, -0.0088, -0.0752, 0.0056, -0.0714, 0.0131, -0.0112, -0.0487, 0.0937, 0.0109, -0.0276, 0.0033, -0.0103, 0.0161, 0.0127, -0.0472, 0.0633, -0.0152, -0.0297, -0.0307, 0.0032, -0.0251, -0.0463, -0.0155, 0.0326, -0.0025, -0.0269, -0.0159, 0.0093, 0.0030, 0.0202, -0.0294, 0.1312, -0.0046, 0.0090, 0.0074, 0.0034, -0.1757, -0.0096, 0.0877, 0.0117, -0.0160, -0.0057, 0.0058, 0.0063, 0.0264, -0.0108, 0.1677, 0.0123, 0.0174, -0.0671, 0.0078, -0.0325, 0.0112, -0.0160]\n",
            "Token 2(  es): [-0.0025, 0.0057, -0.0641, 0.0056, -0.0164, 0.0175, 0.0039, -0.0116, 0.0030, 0.0133, 0.0114, -0.0287, -0.0179, 0.0128, 0.0074, -0.0076, -0.0060, -0.0147, -0.0186, 0.0005, -0.0088, -0.0136, 0.0029, 0.0254, -0.0080, 0.4414, -0.0096, -0.0010, -0.0230, -0.0077, 0.0115, -0.0169, -0.0014, 0.0262, 0.0098, -0.0083, 0.0088, -0.0252, -0.0095, -0.0242, 0.0307, 0.0354, 0.0289, -0.0059, -0.0187, -0.0009, 0.0157, -0.0155, 0.0190, -0.0145, -0.0266, 0.0359, -0.0186, 0.0066, 0.0160, -0.0021, -0.0266, -0.0081, 0.0258, 0.0069, -0.0309, -0.0194, 0.0193, -0.0364, -0.0273, 0.0719, -0.0097, 0.0155, -0.0099, 0.0554, 0.0115, 0.0175, -0.0048, 0.0188, -0.0056, -0.0262, -0.0051, -0.0470, 0.0123, -0.0158, -0.0478, 0.0581, 0.0156, -0.0468, 0.0075, -0.0135, 0.0257, 0.0010, -0.0440, 0.0214, -0.0278, -0.0221, -0.0286, 0.0113, -0.0256, -0.0432, -0.0044, 0.0234, -0.0024, -0.0415, -0.0150, 0.0054, 0.0261, 0.0215, -0.0200, 0.1075, -0.0056, 0.0068, 0.0026, 0.0011, -0.1663, -0.0068, 0.0817, 0.0079, 0.0005, 0.0130, -0.0041, 0.0221, 0.0297, -0.0105, 0.1030, 0.0110, 0.0045, -0.0506, 0.0064, -0.0270, 0.0004, -0.0192]\n",
            "Token 3(  or): [0.0037, 0.0133, -0.0403, 0.0107, -0.0179, 0.0027, -0.0140, -0.0127, 0.0111, 0.0328, -0.0058, -0.1063, -0.0218, 0.0046, 0.0015, -0.0197, -0.0131, -0.0173, -0.0214, -0.0084, -0.0021, -0.0124, 0.0042, 0.0392, -0.0049, 0.4470, -0.0228, -0.0054, -0.0186, -0.0141, 0.0142, -0.0321, -0.0100, 0.0197, 0.0057, -0.0178, 0.0073, -0.0180, -0.0157, 0.0035, 0.0123, 0.0229, 0.0475, 0.0113, -0.0126, 0.0030, 0.0109, -0.0192, 0.0156, -0.0136, -0.0214, 0.0258, -0.0363, 0.0044, 0.0100, -0.0159, -0.0127, -0.0057, 0.0288, 0.0147, -0.0462, -0.0091, 0.0025, -0.0297, -0.0187, 0.0582, -0.0001, 0.0173, 0.0026, 0.0744, 0.0149, 0.0123, -0.0078, 0.0050, -0.0081, -0.1119, 0.0070, -0.0539, 0.0066, 0.0001, -0.0494, 0.0934, 0.0105, -0.0248, -0.0226, -0.0053, 0.0157, 0.0123, -0.0331, 0.0569, -0.0060, -0.0199, -0.0289, -0.0021, -0.0112, -0.0314, -0.0139, 0.0261, -0.0054, -0.0211, -0.0149, 0.0022, 0.0092, 0.0129, -0.0177, 0.1081, -0.0016, 0.0063, 0.0073, 0.0011, -0.1313, -0.0054, 0.1046, 0.0078, -0.0103, -0.0027, 0.0119, -0.0041, 0.0218, -0.0023, 0.1387, 0.0066, 0.0097, -0.0521, 0.0042, -0.0203, 0.0079, -0.0131]\n",
            "Token 4( ...): [0.0049, 0.0116, -0.0374, 0.0142, -0.0071, 0.0024, -0.0068, -0.0062, 0.0087, 0.0293, -0.0095, -0.0726, -0.0210, 0.0069, -0.0003, -0.0164, -0.0154, -0.0159, -0.0218, -0.0082, 0.0018, -0.0117, 0.0017, 0.0346, -0.0061, 0.4371, -0.0219, -0.0024, -0.0171, -0.0351, 0.0148, -0.0246, -0.0073, 0.0167, 0.0038, -0.0177, 0.0106, -0.0156, -0.0115, 0.0068, 0.0122, 0.0200, 0.0562, 0.0155, -0.0113, 0.0033, 0.0147, -0.0149, 0.0225, -0.0152, -0.0186, 0.0236, -0.0184, 0.0081, 0.0082, -0.0126, -0.0124, -0.0073, 0.0304, 0.0111, -0.0454, -0.0070, 0.0100, -0.0314, -0.0223, 0.0679, -0.0036, 0.0206, 0.0019, 0.0620, 0.0121, 0.0118, -0.0086, 0.0043, -0.0107, -0.1044, 0.0074, -0.0432, 0.0070, 0.0023, -0.0438, 0.0994, 0.0114, -0.0256, -0.0237, -0.0048, 0.0120, 0.0124, -0.0301, 0.0565, -0.0100, -0.0195, -0.0319, -0.0025, -0.0100, -0.0299, -0.0138, 0.0259, -0.0055, -0.0179, -0.0153, -0.0005, 0.0018, 0.0106, -0.0085, 0.0984, 0.0011, 0.0059, 0.0035, 0.0006, -0.1228, -0.0109, 0.0726, 0.0012, -0.0099, -0.0033, 0.0059, -0.0030, 0.0226, -0.0048, 0.1291, 0.0041, 0.0067, -0.0436, 0.0006, -0.0173, -0.0008, -0.0098]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0174, -0.1290, -0.0275, 0.0390, -0.0243, -0.0411, 0.1415, -0.0819, -0.0737, 0.1021, -0.2416, 0.0862, 0.0627, -0.0187, 0.0868, 0.0286, 0.0697, -0.0132, 0.0228, 0.0469, 0.0987, 0.1704, -0.0052, 0.1327, -0.1521, 0.0016, 0.0844, 0.0589, 0.0819, 0.0113, -0.1218, -0.0122, -0.1546, 0.0135, 0.0594, -0.0307, 0.0543, 0.0743, 0.0812, -0.1029, 0.0549, -0.0187, -0.0453, 0.0048, 0.0856, -0.0043, 0.0479, 0.0066, 0.0117, 0.1035, -0.0402, -0.0406, -0.0410, -0.0960, 0.0719, -0.1367, -0.0420, 0.0475, -0.1495, 0.0632, -0.1488, 0.0732, -0.0100, -0.0732, -0.0740, 0.0335, -0.0052, 0.0140, 0.0049, 0.0566, 0.0908, -0.1148, 0.0772, -0.2123, -0.0500, 0.0275, -0.0897, 0.0328, 0.0599, 0.0228, -0.1711, -0.0188, 0.1615, -0.0410, 0.0348, -0.0388, -0.0342, -0.0094, -0.0558, 0.1272, 0.1143, -0.0472, 0.2274, 0.0234, -0.0762, 0.0199, -0.0847, 0.0246, 0.0474, -0.0429, 0.0770, -0.0558, 0.0554, -0.0315, -0.0858, 0.0613, -0.1620, -0.1234, -0.1278, -0.0133, -0.0333, -0.0997, -0.0709, -0.0608, -0.0352, 0.0190, 0.0251, -0.0354, 0.0087, 0.0099, 0.1437, -0.0669, -0.1836, 0.0371, -0.0154, -0.0116, 0.0631, -0.0049]\n",
            "Token 1( açò): [0.0165, -0.1277, -0.0267, 0.0388, -0.0233, -0.0406, 0.1394, -0.0825, -0.0730, 0.1009, -0.2448, 0.0848, 0.0629, -0.0182, 0.0857, 0.0290, 0.0702, -0.0127, 0.0226, 0.0466, 0.0975, 0.1689, -0.0058, 0.1309, -0.1502, 0.0016, 0.0827, 0.0589, 0.0810, 0.0107, -0.1204, -0.0124, -0.1533, 0.0133, 0.0597, -0.0305, 0.0532, 0.0742, 0.0804, -0.1019, 0.0541, -0.0188, -0.0455, 0.0039, 0.0842, -0.0038, 0.0475, 0.0062, 0.0119, 0.1030, -0.0397, -0.0401, -0.0401, -0.0948, 0.0711, -0.1354, -0.0416, 0.0466, -0.1482, 0.0628, -0.1473, 0.0724, -0.0104, -0.0721, -0.0730, 0.0329, -0.0057, 0.0142, 0.0054, 0.0568, 0.0894, -0.1141, 0.0759, -0.2104, -0.0499, 0.0277, -0.0890, 0.0327, 0.0588, 0.0223, -0.1691, -0.0189, 0.1600, -0.0404, 0.0353, -0.0381, -0.0343, -0.0094, -0.0549, 0.1259, 0.1138, -0.0468, 0.2262, 0.0229, -0.0760, 0.0189, -0.0835, 0.0242, 0.0473, -0.0424, 0.0762, -0.0549, 0.0547, -0.0313, -0.0846, 0.0600, -0.1604, -0.1225, -0.1262, -0.0134, -0.0329, -0.0988, -0.0700, -0.0603, -0.0353, 0.0186, 0.0247, -0.0357, 0.0094, 0.0095, 0.1421, -0.0662, -0.1818, 0.0369, -0.0150, -0.0119, 0.0626, -0.0060]\n",
            "Token 2(  es): [-0.0306, -0.0086, -0.0014, 0.0018, -0.0187, -0.0262, -0.0071, 0.0244, 0.0263, 0.0100, 0.2072, 0.0188, -0.0234, -0.0050, -0.0421, -0.0119, -0.0478, -0.0244, -0.0381, -0.0072, 0.0288, 0.0159, 0.0017, -0.0165, -0.0104, 0.0151, -0.0043, 0.0210, -0.0279, -0.0105, 0.0075, 0.0113, -0.0182, -0.0033, 0.0368, -0.0442, -0.0052, 0.0237, 0.0044, 0.0204, -0.0731, 0.0362, -0.0356, -0.0278, -0.0226, 0.0008, 0.0147, -0.0068, -0.0001, -0.0196, 0.0175, -0.0105, -0.0384, -0.0074, 0.0082, -0.0170, -0.0211, 0.0421, 0.0086, -0.0135, -0.0160, 0.0446, -0.0020, -0.0054, -0.0325, 0.0303, -0.0412, -0.0598, -0.0270, 0.0191, -0.0115, 0.0011, -0.0192, 0.0328, 0.0162, 0.0184, 0.0273, -0.0317, -0.0041, 0.0438, 0.0038, -0.0078, 0.0124, -0.0019, -0.0018, 0.0233, -0.0112, 0.0143, 0.0187, -0.0180, -0.0104, 0.0304, 0.0274, -0.0413, 0.0339, 0.0010, -0.0156, 0.0034, 0.0168, 0.0266, -0.0257, 0.0327, -0.0126, 0.0146, -0.0004, -0.0097, 0.0004, 0.0096, 0.0260, 0.0290, -0.0055, -0.0191, 0.0232, 0.0295, 0.0483, -0.0169, 0.0072, -0.0017, 0.0081, 0.0261, -0.0041, -0.0052, -0.0030, -0.0053, -0.0032, -0.0245, -0.0266, -0.0733]\n",
            "Token 3(  or): [-0.0137, -0.0018, 0.0047, -0.0163, 0.0107, -0.0227, -0.0205, 0.0164, 0.0061, 0.0017, 0.2374, 0.0059, 0.0057, -0.0125, -0.0278, 0.0065, -0.0236, -0.0061, -0.0007, -0.0093, 0.0297, 0.0091, 0.0114, -0.0167, 0.0180, 0.0058, 0.0355, -0.0033, -0.0251, -0.0124, 0.0068, -0.0058, -0.0152, -0.0091, 0.0283, -0.0368, 0.0010, 0.0028, -0.0095, 0.0116, -0.0197, -0.0032, -0.0163, -0.0035, -0.0397, -0.0055, 0.0097, -0.0022, 0.0044, -0.0014, 0.0106, -0.0191, -0.0210, -0.0021, 0.0077, -0.0187, 0.0019, 0.0029, 0.0169, -0.0016, -0.0182, -0.0039, 0.0071, -0.0148, -0.0175, -0.0020, -0.0091, -0.0460, -0.0115, 0.0479, 0.0037, 0.0027, -0.0335, 0.0285, -0.0281, 0.0330, 0.0254, -0.0354, -0.0106, 0.0406, -0.0081, -0.0312, 0.0421, 0.0034, 0.0008, 0.0140, -0.0066, 0.0441, 0.0106, -0.0013, -0.0188, 0.0123, 0.0395, -0.0361, 0.0110, 0.0139, -0.0206, -0.0090, 0.0084, 0.0214, -0.0254, 0.0209, 0.0071, 0.0137, -0.0065, -0.0178, 0.0194, 0.0021, -0.0054, 0.0189, 0.0123, -0.0293, 0.0189, 0.0022, 0.0159, -0.0105, -0.0046, -0.0094, 0.0102, 0.0198, -0.0119, 0.0064, 0.0184, 0.0053, -0.0141, 0.0027, -0.0236, -0.0689]\n",
            "Token 4( ...): [-0.0145, 0.0169, 0.0103, -0.0381, 0.0162, -0.0152, -0.0288, 0.0190, -0.0010, -0.0161, 0.2393, -0.0202, 0.0018, -0.0213, 0.0033, -0.0061, -0.0163, 0.0195, -0.0109, -0.0282, 0.0248, 0.0053, -0.0229, -0.0009, 0.0111, -0.0092, 0.0449, -0.0055, -0.0282, -0.0070, 0.0026, 0.0155, -0.0005, 0.0085, 0.0124, -0.0147, -0.0093, -0.0100, -0.0287, 0.0098, 0.0151, 0.0266, -0.0070, -0.0099, -0.0410, 0.0052, -0.0223, 0.0064, -0.0236, 0.0317, -0.0146, 0.0186, -0.0168, -0.0099, 0.0089, -0.0008, 0.0073, -0.0044, 0.0313, 0.0094, -0.0176, -0.0019, 0.0337, 0.0121, -0.0066, -0.0335, 0.0305, -0.0397, 0.0178, 0.0454, -0.0069, -0.0091, -0.0209, 0.0440, -0.0171, 0.0206, 0.0120, -0.0273, 0.0034, 0.0292, -0.0456, -0.0355, 0.0629, -0.0307, -0.0015, 0.0279, -0.0119, 0.0551, 0.0132, 0.0005, -0.0069, -0.0027, 0.0549, -0.0188, -0.0122, 0.0219, -0.0053, 0.0104, 0.0156, -0.0070, -0.0322, 0.0113, 0.0180, 0.0341, -0.0332, -0.0189, 0.0211, -0.0133, -0.0457, 0.0018, 0.0179, -0.0250, -0.0292, 0.0085, 0.0308, -0.0115, -0.0204, -0.0136, 0.0009, 0.0039, -0.0218, -0.0017, 0.0006, 0.0258, -0.0138, 0.0135, -0.0024, -0.0997]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0047, -0.0257, 0.0017, -0.0183, 0.0019, 0.0056, -0.0100, -0.0132, 0.1737, 0.0199, 0.0034, -0.0206, -0.0048, -0.0170, 0.0134, -0.0037, 0.0539, 0.0126, -0.0030, 0.0112, 0.0023, -0.0160, -0.0135, -0.0081, 0.0034, 0.0137, -0.0313, -0.0036, 0.0004, 0.0018, -0.0093, 0.0033, 0.0707, 0.0157, -0.0020, -0.0057, 0.0237, 0.0178, -0.0091, -0.0050, -0.0054, 0.0295, -0.0015, -0.0073, -0.0123, -0.0233, -0.0199, -0.0040, 0.0128, -0.0164, 0.0014, 0.0244, 0.0009, 0.0141, 0.0015, 0.0273, -0.0145, 0.0269, -0.0152, 0.0184, -0.0153, 0.0031, 0.0141, 0.0237, 0.0006, -0.0062, 0.0159, -0.0139, -0.0013, -0.0235, 0.0055, -0.0081, -0.0150, -0.0120, -0.0027, 0.1849, 0.0579, -0.0015, -0.0084, -0.0140, 0.0085, -0.0086, -0.0061, -0.0093, 0.0316, 0.0459, 0.0138, -0.0074, 0.0038, -0.0230, -0.1040, -0.0241, 0.0048, 0.0354, -0.0063, -0.0017, 0.0075, -0.0067, 0.0426, -0.0177, 0.0050, -0.0179, 0.0019, -0.0027, -0.0206, 0.0091, -0.0078, -0.0297, 0.0069, -0.0174, -0.0084, 0.0168, 0.0218, -0.0191, -0.0087, 0.0003, 0.0153, 0.0221, -0.0013, -0.0060, -0.0136, 0.0040, 0.0135, -0.0218, -0.0255, -0.0075, 0.0050, -0.0041]\n",
            "Token 1( açò): [-0.0017, -0.0269, 0.0002, -0.0203, 0.0035, 0.0009, -0.0052, -0.0104, 0.0919, 0.0169, 0.0079, -0.0141, -0.0099, -0.0135, 0.0104, -0.0014, 0.0448, 0.0113, 0.0032, 0.0057, 0.0070, -0.0205, -0.0076, -0.0056, 0.0034, 0.0064, -0.0268, -0.0070, 0.0013, 0.0049, -0.0082, 0.0021, 0.0368, 0.0152, -0.0039, -0.0033, 0.0269, 0.0132, -0.0077, -0.0034, -0.0034, 0.0197, 0.0021, -0.0067, -0.0093, -0.0198, -0.0135, -0.0026, 0.0050, -0.0143, 0.0029, 0.0195, 0.0010, 0.0142, 0.0030, 0.0251, -0.0138, 0.0170, -0.0111, 0.0140, -0.0124, 0.0009, 0.0160, 0.0181, 0.0033, -0.0087, 0.0162, -0.0083, 0.0045, -0.0268, 0.0059, -0.0078, -0.0114, -0.0072, 0.0027, 0.1090, 0.0596, 0.0023, -0.0044, -0.0124, 0.0093, -0.0079, -0.0052, -0.0034, 0.0336, 0.0461, 0.0099, -0.0104, -0.0064, -0.0211, -0.0864, -0.0192, 0.0001, 0.0302, -0.0029, -0.0002, 0.0023, -0.0043, 0.0371, -0.0175, 0.0047, -0.0147, -0.0068, -0.0018, -0.0194, 0.0111, -0.0071, -0.0264, -0.0008, -0.0140, -0.0097, 0.0096, 0.0199, -0.0189, -0.0052, -0.0021, 0.0126, 0.0245, -0.0011, 0.0031, -0.0173, 0.0052, 0.0101, -0.0261, -0.0208, -0.0115, -0.0006, -0.0101]\n",
            "Token 2(  es): [0.0015, -0.0250, 0.0009, -0.0185, 0.0021, 0.0038, -0.0077, -0.0130, 0.1328, 0.0189, 0.0042, -0.0186, -0.0064, -0.0159, 0.0125, -0.0021, 0.0491, 0.0117, -0.0015, 0.0091, 0.0053, -0.0174, -0.0115, -0.0067, 0.0033, 0.0113, -0.0283, -0.0051, 0.0001, 0.0045, -0.0094, 0.0028, 0.0513, 0.0152, -0.0025, -0.0046, 0.0245, 0.0159, -0.0092, -0.0035, -0.0048, 0.0255, -0.0004, -0.0064, -0.0106, -0.0219, -0.0172, -0.0019, 0.0091, -0.0144, 0.0022, 0.0213, 0.0003, 0.0143, 0.0016, 0.0257, -0.0139, 0.0241, -0.0135, 0.0162, -0.0134, 0.0031, 0.0146, 0.0215, 0.0018, -0.0073, 0.0150, -0.0128, 0.0006, -0.0252, 0.0053, -0.0072, -0.0132, -0.0111, -0.0007, 0.1445, 0.0567, 0.0008, -0.0067, -0.0135, 0.0080, -0.0071, -0.0043, -0.0075, 0.0318, 0.0443, 0.0122, -0.0080, -0.0001, -0.0223, -0.0956, -0.0222, 0.0028, 0.0327, -0.0052, -0.0003, 0.0065, -0.0053, 0.0385, -0.0169, 0.0052, -0.0162, -0.0008, -0.0026, -0.0188, 0.0098, -0.0078, -0.0282, 0.0043, -0.0152, -0.0087, 0.0137, 0.0199, -0.0177, -0.0067, -0.0010, 0.0130, 0.0221, -0.0017, -0.0028, -0.0147, 0.0044, 0.0119, -0.0219, -0.0236, -0.0088, 0.0036, -0.0064]\n",
            "Token 3(  or): [0.0005, -0.0232, 0.0006, -0.0177, 0.0023, 0.0033, -0.0068, -0.0135, 0.1136, 0.0187, 0.0036, -0.0184, -0.0063, -0.0159, 0.0123, -0.0013, 0.0466, 0.0111, -0.0018, 0.0086, 0.0067, -0.0172, -0.0115, -0.0066, 0.0028, 0.0111, -0.0264, -0.0053, -0.0004, 0.0055, -0.0098, 0.0025, 0.0383, 0.0149, -0.0025, -0.0042, 0.0244, 0.0156, -0.0099, -0.0028, -0.0047, 0.0244, -0.0005, -0.0057, -0.0100, -0.0217, -0.0163, -0.0002, 0.0081, -0.0130, 0.0025, 0.0194, -0.0003, 0.0144, 0.0017, 0.0248, -0.0137, 0.0248, -0.0128, 0.0154, -0.0121, 0.0038, 0.0145, 0.0208, 0.0022, -0.0074, 0.0139, -0.0135, 0.0004, -0.0255, 0.0053, -0.0063, -0.0126, -0.0116, -0.0004, 0.1245, 0.0534, 0.0016, -0.0062, -0.0137, 0.0072, -0.0057, -0.0030, -0.0072, 0.0311, 0.0425, 0.0116, -0.0077, -0.0008, -0.0217, -0.0923, -0.0217, 0.0019, 0.0313, -0.0051, 0.0005, 0.0068, -0.0046, 0.0358, -0.0161, 0.0053, -0.0150, -0.0005, -0.0030, -0.0171, 0.0096, -0.0079, -0.0277, 0.0040, -0.0143, -0.0085, 0.0132, 0.0185, -0.0165, -0.0058, -0.0013, 0.0117, 0.0211, -0.0025, -0.0022, -0.0149, 0.0043, 0.0113, -0.0209, -0.0227, -0.0086, 0.0040, -0.0070]\n",
            "Token 4( ...): [0.0018, -0.0228, 0.0002, -0.0171, 0.0029, 0.0039, -0.0077, -0.0132, 0.1293, 0.0189, 0.0028, -0.0171, -0.0049, -0.0163, 0.0129, -0.0019, 0.0481, 0.0115, -0.0019, 0.0081, 0.0032, -0.0160, -0.0126, -0.0081, 0.0025, 0.0127, -0.0280, -0.0052, 0.0001, 0.0042, -0.0089, 0.0026, 0.0492, 0.0142, -0.0021, -0.0045, 0.0225, 0.0166, -0.0087, -0.0041, -0.0053, 0.0267, -0.0014, -0.0074, -0.0111, -0.0221, -0.0166, -0.0025, 0.0100, -0.0144, 0.0020, 0.0214, 0.0014, 0.0134, 0.0025, 0.0257, -0.0142, 0.0252, -0.0131, 0.0162, -0.0130, 0.0028, 0.0145, 0.0215, 0.0010, -0.0060, 0.0151, -0.0135, 0.0005, -0.0230, 0.0043, -0.0079, -0.0137, -0.0106, -0.0020, 0.1426, 0.0549, 0.0003, -0.0068, -0.0142, 0.0079, -0.0074, -0.0044, -0.0071, 0.0308, 0.0439, 0.0133, -0.0073, 0.0017, -0.0213, -0.0949, -0.0231, 0.0030, 0.0318, -0.0059, 0.0003, 0.0065, -0.0052, 0.0384, -0.0163, 0.0049, -0.0157, 0.0008, -0.0013, -0.0183, 0.0080, -0.0078, -0.0275, 0.0045, -0.0162, -0.0084, 0.0156, 0.0207, -0.0163, -0.0068, 0.0003, 0.0141, 0.0218, -0.0009, -0.0033, -0.0140, 0.0055, 0.0117, -0.0223, -0.0229, -0.0068, 0.0043, -0.0058]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0001, 0.0307, 0.0271, -0.0246, -0.0394, 0.0076, -0.0123, -0.0422, 0.0018, -0.0177, 0.0159, -0.1882, -0.0059, -0.0860, 0.0484, 0.0425, -0.0093, 0.0199, -0.0861, -0.0626, 0.0114, 0.0836, 0.0464, 0.0303, -0.0357, -0.0059, -0.0544, -0.0463, 0.0213, -0.0427, -0.0057, -0.0707, 0.0545, 0.0537, 0.0455, -0.0080, 0.0390, 0.0064, 0.0354, 0.0331, 0.0277, -0.0118, -0.0700, -0.0105, -0.0450, -0.0372, 0.0022, 0.0125, 0.0657, -0.0320, -0.0387, -0.0384, 0.0522, -0.0265, -0.0388, -0.0121, -0.0267, -0.0129, -0.0442, 0.0392, -0.0763, -0.0208, -0.0551, 0.0279, 0.0169, 0.0234, 0.0365, -0.0378, -0.0244, 0.0053, 0.0434, 0.0782, -0.0278, 0.0360, 0.0731, -0.0018, -0.0129, 0.0435, -0.0001, 0.0153, 0.0565, 0.0466, 0.0478, 0.0126, 0.0878, 0.0205, -0.0379, 0.0196, -0.0189, -0.0570, -0.0158, -0.0292, 0.0518, -0.0603, -0.0417, 0.0241, 0.0282, -0.0203, 0.0455, 0.0216, -0.0056, -0.0294, 0.0469, 0.0629, 0.0501, 0.0504, -0.0487, 0.0207, 0.0416, 0.0635, -0.0154, -0.0286, 0.0226, -0.1066, 0.0062, -0.0501, -0.0181, -0.0159, 0.0404, 0.0115, 0.0482, 0.0203, 0.0467, 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]\n",
            "Token 1( açò): [-0.0072, 0.0325, 0.0232, -0.0172, -0.0274, 0.0081, -0.0106, -0.0300, 0.0059, -0.0154, 0.0134, -0.1027, -0.0075, -0.0752, 0.0413, 0.0336, -0.0112, 0.0143, -0.0730, -0.0490, 0.0109, 0.0749, 0.0388, 0.0263, -0.0239, -0.0093, -0.0411, -0.0409, 0.0213, -0.0310, -0.0018, -0.0572, 0.0452, 0.0462, 0.0437, -0.0099, 0.0369, -0.0006, 0.0344, 0.0223, 0.0211, -0.0088, -0.0522, -0.0081, -0.0394, -0.0280, -0.0023, 0.0099, 0.0568, -0.0252, -0.0351, -0.0375, 0.0475, -0.0223, -0.0331, -0.0136, -0.0223, -0.0075, -0.0389, 0.0319, -0.0649, -0.0205, -0.0430, 0.0276, 0.0215, 0.0208, 0.0327, -0.0259, -0.0220, 0.0064, 0.0322, 0.0697, -0.0272, 0.0275, 0.0699, -0.0050, -0.0087, 0.0365, 0.0026, 0.0132, 0.0515, 0.0352, 0.0370, 0.0119, 0.0722, 0.0160, -0.0351, 0.0208, -0.0149, -0.0472, -0.0243, -0.0272, 0.0475, -0.0512, -0.0361, 0.0167, 0.0261, -0.0109, 0.0370, 0.0119, -0.0011, -0.0316, 0.0413, 0.0535, 0.0439, 0.0405, -0.0463, 0.0144, 0.0317, 0.0550, -0.0158, -0.0212, 0.0170, -0.0919, 0.0038, -0.0398, -0.0144, -0.0038, 0.0311, 0.0102, 0.0315, 0.0124, 0.0417, 0.0436, -0.0060, -0.0241, -0.0086, -0.0052]\n",
            "Token 2(  es): [-0.0124, 0.0284, 0.0200, -0.0174, -0.0191, 0.0090, -0.0088, -0.0210, 0.0065, -0.0124, 0.0077, -0.0301, -0.0048, -0.0653, 0.0280, 0.0272, -0.0107, 0.0118, -0.0604, -0.0408, 0.0094, 0.0603, 0.0339, 0.0221, -0.0168, -0.0088, -0.0315, -0.0369, 0.0233, -0.0214, 0.0015, -0.0470, 0.0335, 0.0386, 0.0368, -0.0051, 0.0307, -0.0019, 0.0277, 0.0144, 0.0166, -0.0040, -0.0368, -0.0082, -0.0343, -0.0244, -0.0029, 0.0078, 0.0495, -0.0224, -0.0280, -0.0324, 0.0346, -0.0205, -0.0286, -0.0157, -0.0156, -0.0025, -0.0287, 0.0262, -0.0514, -0.0217, -0.0374, 0.0179, 0.0229, 0.0159, 0.0291, -0.0193, -0.0212, 0.0058, 0.0263, 0.0566, -0.0201, 0.0219, 0.0569, -0.0028, -0.0019, 0.0284, 0.0073, 0.0152, 0.0438, 0.0305, 0.0274, 0.0121, 0.0586, 0.0133, -0.0304, 0.0197, -0.0187, -0.0399, -0.0147, -0.0213, 0.0431, -0.0433, -0.0300, 0.0101, 0.0250, -0.0105, 0.0281, 0.0055, 0.0003, -0.0260, 0.0328, 0.0431, 0.0361, 0.0296, -0.0426, 0.0094, 0.0261, 0.0436, -0.0129, -0.0180, 0.0121, -0.0770, 0.0026, -0.0357, -0.0057, 0.0022, 0.0218, 0.0035, 0.0211, 0.0067, 0.0383, 0.0282, -0.0065, -0.0226, -0.0092, -0.0040]\n",
            "Token 3(  or): [-0.0122, 0.0256, 0.0193, -0.0156, -0.0183, 0.0087, -0.0086, -0.0170, 0.0058, -0.0133, 0.0062, 0.0076, -0.0067, -0.0575, 0.0218, 0.0249, -0.0075, 0.0108, -0.0539, -0.0385, 0.0092, 0.0523, 0.0309, 0.0203, -0.0134, -0.0086, -0.0280, -0.0319, 0.0205, -0.0215, 0.0022, -0.0446, 0.0280, 0.0331, 0.0323, -0.0039, 0.0268, -0.0005, 0.0237, 0.0123, 0.0166, -0.0042, -0.0293, -0.0080, -0.0310, -0.0226, -0.0029, 0.0046, 0.0438, -0.0205, -0.0236, -0.0311, 0.0286, -0.0207, -0.0264, -0.0157, -0.0131, -0.0014, -0.0234, 0.0232, -0.0467, -0.0198, -0.0348, 0.0152, 0.0220, 0.0139, 0.0268, -0.0181, -0.0219, 0.0049, 0.0239, 0.0504, -0.0161, 0.0206, 0.0497, -0.0024, -0.0011, 0.0256, 0.0098, 0.0138, 0.0400, 0.0272, 0.0232, 0.0097, 0.0536, 0.0122, -0.0262, 0.0176, -0.0163, -0.0360, -0.0077, -0.0188, 0.0385, -0.0385, -0.0275, 0.0099, 0.0217, -0.0098, 0.0251, 0.0056, 0.0018, -0.0231, 0.0300, 0.0363, 0.0306, 0.0244, -0.0379, 0.0086, 0.0242, 0.0385, -0.0124, -0.0138, 0.0133, -0.0710, 0.0042, -0.0334, -0.0045, 0.0040, 0.0178, 0.0044, 0.0194, 0.0037, 0.0320, 0.0235, -0.0061, -0.0225, -0.0085, -0.0020]\n",
            "Token 4( ...): [-0.0113, 0.0257, 0.0207, -0.0195, -0.0237, 0.0068, -0.0081, -0.0233, 0.0029, -0.0124, 0.0067, -0.0414, -0.0024, -0.0635, 0.0288, 0.0299, -0.0074, 0.0150, -0.0598, -0.0457, 0.0108, 0.0595, 0.0321, 0.0231, -0.0175, -0.0056, -0.0360, -0.0368, 0.0179, -0.0271, 0.0017, -0.0517, 0.0343, 0.0399, 0.0338, -0.0045, 0.0275, 0.0020, 0.0256, 0.0215, 0.0206, -0.0043, -0.0434, -0.0060, -0.0302, -0.0262, -0.0009, 0.0075, 0.0462, -0.0203, -0.0272, -0.0317, 0.0346, -0.0199, -0.0253, -0.0143, -0.0151, -0.0035, -0.0285, 0.0268, -0.0524, -0.0196, -0.0398, 0.0200, 0.0193, 0.0157, 0.0284, -0.0216, -0.0204, 0.0087, 0.0276, 0.0531, -0.0189, 0.0282, 0.0571, -0.0000, -0.0037, 0.0334, 0.0087, 0.0123, 0.0437, 0.0338, 0.0305, 0.0128, 0.0600, 0.0136, -0.0270, 0.0160, -0.0152, -0.0454, -0.0112, -0.0209, 0.0415, -0.0421, -0.0314, 0.0136, 0.0204, -0.0129, 0.0297, 0.0088, -0.0023, -0.0250, 0.0328, 0.0425, 0.0336, 0.0312, -0.0385, 0.0137, 0.0277, 0.0435, -0.0115, -0.0186, 0.0123, -0.0777, 0.0060, -0.0358, -0.0049, -0.0015, 0.0233, 0.0055, 0.0288, 0.0099, 0.0359, 0.0336, -0.0068, -0.0197, -0.0091, -0.0009]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora concatenamos todos los heads de vuelta para volver a nuestra matriz (5, 2048). Y además hacemos otra trnasformación lineal (o_proj (2048 → 2048)) para mezclar la información de todos los heads."
      ],
      "metadata": {
        "id": "CuGj-8SaKQyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Transpose - volver a poner tokens primero\n",
        "attention_output_transposed = attention_output.transpose(0, 1)  # (16, 5, 128) → (5, 16, 128)\n",
        "\n",
        "print(f\"Después del transpose: {attention_output_transposed.shape}\")\n",
        "\n",
        "# Paso 2: Reshape/Concatenar - unir los 16 heads de 128 → 2048\n",
        "attention_output_concat = attention_output_transposed.reshape(5, 2048)\n",
        "\n",
        "print(f\"Después de concatenar: {attention_output_concat.shape}  # (5, 2048)\")\n",
        "\n",
        "\n",
        "for token_idx in range(5):\n",
        "    token_vec = attention_output_concat[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Us0x-K-UJVQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55243888-8b47-475c-8d6c-d6d4c2754e52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después del transpose: torch.Size([5, 16, 128])\n",
            "Después de concatenar: torch.Size([5, 2048])  # (5, 2048)\n",
            "Token 0 ( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (2048 dims)\n",
            "Token 1 ( açò): [0.0144, -0.0013, 0.1037, 0.0479, -0.0124, ..., 0.0436, -0.0060, -0.0241, -0.0086, -0.0052]  (2048 dims)\n",
            "Token 2 (  es): [0.0121, 0.0015, 0.1000, 0.0445, -0.0102, ..., 0.0282, -0.0065, -0.0226, -0.0092, -0.0040]  (2048 dims)\n",
            "Token 3 (  or): [0.0141, 0.0001, 0.0999, 0.0460, -0.0110, ..., 0.0235, -0.0061, -0.0225, -0.0085, -0.0020]  (2048 dims)\n",
            "Token 4 ( ...): [0.0122, 0.0044, 0.0917, 0.0405, -0.0076, ..., 0.0336, -0.0068, -0.0197, -0.0091, -0.0009]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pesos de la operación o_proj (proyección de salida)"
      ],
      "metadata": {
        "id": "YF_UBAwrL7-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los pesos de o_proj\n",
        "W_o = model.model.layers[0].self_attn.o_proj.weight\n",
        "\n",
        "print(f\"Matriz de pesos W_o: {W_o.shape}  # (2048, 2048)\")\n",
        "print(f\"Shape de los pesos W_o: {W_o.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_o[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_o[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "2zVvqO9pNo-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5daa167e-5e30-4206-cfa6-d005b11c6653"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de pesos W_o: torch.Size([2048, 2048])  # (2048, 2048)\n",
            "Shape de los pesos W_o: torch.Size([2048, 2048])\n",
            "[0.0012, 0.0014, -0.0007, -0.0010, 0.0011, ..., -0.0024, -0.0012, -0.0012, -0.0013, 0.0005]  (2048 dims)\n",
            "[0.0008, -0.0016, 0.0043, -0.0023, -0.0038, ..., -0.0037, 0.0012, 0.0015, 0.0006, -0.0003]  (2048 dims)\n",
            "[0.0007, -0.0027, -0.0009, -0.0022, 0.0019, ..., 0.0019, 0.0023, -0.0003, -0.0003, -0.0005]  (2048 dims)\n",
            "[0.0013, -0.0008, 0.0036, 0.0007, -0.0025, ..., -0.0006, -0.0008, -0.0015, -0.0016, -0.0009]  (2048 dims)\n",
            "[0.0009, -0.0025, -0.0022, 0.0001, -0.0022, ..., 0.0014, 0.0008, -0.0026, 0.0009, -0.0012]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0004, -0.0004, 0.0010, -0.0007, -0.0006, ..., 0.0009, 0.0031, -0.0010, 0.0004, 0.0016]  (2048 dims)\n",
            "[0.0006, -0.0015, 0.0029, -0.0005, 0.0012, ..., -0.0007, -0.0000, 0.0009, -0.0008, -0.0015]  (2048 dims)\n",
            "[-0.0010, -0.0024, 0.0042, -0.0010, 0.0010, ..., 0.0013, -0.0008, 0.0007, 0.0015, -0.0008]  (2048 dims)\n",
            "[-0.0013, 0.0008, -0.0028, 0.0007, 0.0008, ..., 0.0014, -0.0003, -0.0019, -0.0009, -0.0010]  (2048 dims)\n",
            "[0.0001, 0.0009, 0.0004, -0.0019, -0.0010, ..., 0.0021, -0.0006, 0.0006, -0.0004, -0.0007]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos la última transformación para tener la atención final."
      ],
      "metadata": {
        "id": "jnnwQgL8NpeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_final = attention_output_concat @ W_o.T\n",
        "\n",
        "print(f\"Después de o_proj: {attention_final.shape}  # (5, 2048)\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = attention_final[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "4AgyjtrJJVOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c031428-15c2-45cd-e223-3c94bd4950fd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de o_proj: torch.Size([5, 2048])  # (5, 2048)\n",
            "Token 0 ( <s>): [-0.0077, -0.0031, -0.0027, 0.0026, 0.0045, ..., 0.0116, -0.0028, 0.0030, 0.0061, -0.0100]  (2048 dims)\n",
            "Token 1 ( açò): [-0.0064, -0.0025, -0.0039, 0.0026, 0.0060, ..., 0.0140, 0.0028, 0.0009, 0.0037, -0.0105]  (2048 dims)\n",
            "Token 2 (  es): [-0.0001, -0.0026, -0.0017, -0.0060, 0.0001, ..., 0.0073, -0.0050, -0.0023, 0.0015, -0.0014]  (2048 dims)\n",
            "Token 3 (  or): [-0.0007, -0.0032, -0.0037, -0.0002, 0.0021, ..., 0.0055, -0.0009, -0.0023, 0.0010, -0.0037]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0021, 0.0014, -0.0023, -0.0028, -0.0002, ..., 0.0042, -0.0037, -0.0018, 0.0021, -0.0003]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto hemos completado la primera etapa del plan en 5 puntos para el bloque de atención:\n",
        "Siguiendo el plan, lo siguiente es otr normalización en la capa:\\\n",
        "✓ Atención\\\n",
        "→ Residual connection\\\n",
        "→ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "4Am8-CSbUxAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Residual connection (sumar con la entrada original antes de normalizar). Esto ayuda al flujo de gradientes durante el entrenamiento (evita vanishing gradients) y permite entrenar redes muy profundas."
      ],
      "metadata": {
        "id": "NmoiZVEIOecS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual connection\n",
        "hidden_states = normalized[0] + attention_final\n",
        "\n",
        "print(f\"Después de residual connection: {hidden_states.shape}\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = hidden_states[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "dlQiYGEjJVLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd693682-6197-4759-cd8b-7229e7594986"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de residual connection: torch.Size([5, 2048])\n",
            "Token 0 ( <s>): [-0.0154, -0.0082, -0.0190, -0.0304, -0.0047, ..., -0.0151, 0.0109, -0.0555, 0.0108, -0.0262]  (2048 dims)\n",
            "Token 1 ( açò): [-0.1925, -0.0907, -0.0450, 0.2986, 0.5665, ..., -0.2850, -0.2904, -0.0316, -0.1307, 0.1599]  (2048 dims)\n",
            "Token 2 (  es): [-0.1272, 0.1225, 0.5053, 0.0994, 0.1803, ..., -0.0878, 0.4650, 0.3488, -0.3658, -0.3550]  (2048 dims)\n",
            "Token 3 (  or): [-0.2113, 0.1843, -0.3802, 0.0402, 0.1009, ..., 0.3025, 0.1247, 0.2198, 0.0524, 0.0924]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0349, 0.0433, 0.0604, 0.2331, 0.2821, ..., -0.7592, 0.1180, -0.0845, 0.0643, 0.4262]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiendo el plan, lo siguiente es otr normalización en la capa:\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "→ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "UC3gNePWUh8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# post_attention_layernorm\n",
        "residual = hidden_states  # Guardamos para la próxima residual (celda 67)\n",
        "hidden_states_normalized = model.model.layers[0].post_attention_layernorm(hidden_states)\n",
        "\n",
        "print(f\"Después de post_attention_layernorm: {hidden_states_normalized.shape}\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = hidden_states[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "TLHV8Q_7UhOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df716713-8588-4719-e7c2-4ef09216990a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de post_attention_layernorm: torch.Size([5, 2048])\n",
            "Token 0 ( <s>): [-0.0154, -0.0082, -0.0190, -0.0304, -0.0047, ..., -0.0151, 0.0109, -0.0555, 0.0108, -0.0262]  (2048 dims)\n",
            "Token 1 ( açò): [-0.1925, -0.0907, -0.0450, 0.2986, 0.5665, ..., -0.2850, -0.2904, -0.0316, -0.1307, 0.1599]  (2048 dims)\n",
            "Token 2 (  es): [-0.1272, 0.1225, 0.5053, 0.0994, 0.1803, ..., -0.0878, 0.4650, 0.3488, -0.3658, -0.3550]  (2048 dims)\n",
            "Token 3 (  or): [-0.2113, 0.1843, -0.3802, 0.0402, 0.1009, ..., 0.3025, 0.1247, 0.2198, 0.0524, 0.0924]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0349, 0.0433, 0.0604, 0.2331, 0.2821, ..., -0.7592, 0.1180, -0.0845, 0.0643, 0.4262]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02SiVxlDOwqY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamoms ahora a una capa Multilayer Perceptron(MLP):\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "✓ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "tHHNZKAhV-J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto es una combinación de un aumento de la dimensión y una reducción, en este caso subimos a 5440. La elección de este número es un poco técnica pero la explico para no dejar cabos sueltos. Tiene que ver con optimización de hardware y conocimiento ya adquirido de entrenar otras estructuras.\n",
        "Este MLP usa la activación SwiGLU, esto hace que tengamos 3 matrices lineales aprendidas (gate_proj, up_proj, down_proj) en lugar de las dos tradicionales.\n",
        "Se intenta entonces mantener el mismo número de parámetros que se haría con dos capas (que es hacer un x4 en las dims), por tanto:\\\n",
        "\n",
        "MLP estándar (2 capas):  2 × (hidden × 4×hidden) = 8 × hidden²\\\n",
        "SwiGLU (3 capas):        3 × (hidden × intermediate) \\\n",
        "\n",
        "\n",
        "Para igualar FLOPs:\\\n",
        "3 × intermediate ≈ 8 × hidden\\\n",
        "intermediate ≈ (8/3) × hidden ≈ 2.67 × hidden\n",
        "\n",
        "\n",
        "Para nuestro modelo:\n",
        "hidden_size = 2048\\\n",
        "8/3 × 2048 ≈ 5461\\\n",
        "Redondeado a múltiplo cercano de potencia de 2: 5440 = 85 × 64"
      ],
      "metadata": {
        "id": "jtRst2V9XYAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a mostrar a continuación las tres matrices:\\\n",
        "gate_proj: (2048 → 5440)\\\n",
        "up_proj: (2048 → 5440)\\\n",
        "down_proj: (5440 → 2048)"
      ],
      "metadata": {
        "id": "ofezBwJ8Zmn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Por si no conoces esta activación te dejo la estructura aquí:\n",
        "# Input: x (2048 dims)\n",
        "#     ↓\n",
        "# gate_proj(x) → (5440 dims)  ← rama 1\n",
        "# up_proj(x)   → (5440 dims)  ← rama 2\n",
        "#     ↓\n",
        "# Combinar: Swish(gate_proj(x)) ⊙ up_proj(x)  ← multiplicación elemento a elemento\n",
        "#     ↓\n",
        "# down_proj → (2048 dims)\n",
        "# ```\n",
        "\n",
        "# **Fórmula:**\n",
        "# ```\n",
        "# output = down_proj( Swish(gate_proj(x)) ⊙ up_proj(x) )"
      ],
      "metadata": {
        "id": "Sz-yzOEhaUHp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entender gate_proj. Vemos la matriz de pesos W_proj y multiplicamos pot la salida de la normalized layer:"
      ],
      "metadata": {
        "id": "S5DPTgCcanPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_gate = model.model.layers[0].mlp.gate_proj.weight  # (5440, 2048)\n",
        "\n",
        "\n",
        "print(f\"\\nW_gate (gate_proj): {W_gate.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_gate[i]\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_gate.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_gate[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_gate.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_gate.shape[0]} dims) \"*11)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5M1SSPxaett",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f395ba-6caa-4224-96b0-2ff00021d951"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_gate (gate_proj): torch.Size([5440, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0063, 0.0026, -0.0066, -0.0099, -0.0037, ..., 0.0129, 0.0176, -0.0011, 0.0125, 0.0138]  (2048 dims)\n",
            "[0.0028, 0.0225, 0.0190, 0.0093, 0.0201, ..., -0.0320, 0.0240, -0.0173, -0.0046, 0.0269]  (2048 dims)\n",
            "[-0.0094, -0.0106, -0.0076, -0.0159, 0.0009, ..., -0.0025, 0.0008, -0.0161, 0.0039, -0.0182]  (2048 dims)\n",
            "[0.0092, -0.0143, 0.0014, -0.0058, 0.0054, ..., 0.0057, 0.0374, 0.0008, -0.0106, 0.0025]  (2048 dims)\n",
            "[-0.0273, -0.0114, -0.0006, 0.0206, 0.0198, ..., -0.0175, 0.0070, 0.0041, 0.0091, -0.0037]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0070, 0.0062, 0.0032, 0.0325, 0.0063, ..., 0.0020, 0.0121, 0.0074, -0.0342, 0.0081]  (2048 dims)\n",
            "[0.0034, 0.0271, -0.0147, 0.0008, -0.0264, ..., 0.0096, -0.0101, 0.0068, -0.0051, -0.0123]  (2048 dims)\n",
            "[0.0017, -0.0031, -0.0096, 0.0013, 0.0049, ..., -0.0179, 0.0154, -0.0045, 0.0156, -0.0029]  (2048 dims)\n",
            "[0.0276, -0.0107, 0.0160, -0.0171, 0.0325, ..., -0.0156, -0.0010, -0.0188, -0.0013, 0.0083]  (2048 dims)\n",
            "[-0.0149, -0.0161, -0.0122, -0.0052, -0.0116, ..., 0.0209, -0.0244, 0.0132, 0.0134, -0.0036]  (2048 dims)\n",
            "\n",
            " (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos `hidden_states_normalized` (la salida del post_attention_layernorm) por W_gate:\n"
      ],
      "metadata": {
        "id": "RAdCyZAjHAFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gate_output = hidden_states_normalized @ W_gate.T  # (5, 2048) @ (2048, 5440) = (5, 5440)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {gate_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = gate_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({gate_output.shape[1]} dims)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea6ngsqsHDZ_",
        "outputId": "12d20ff8-617a-4ce8-dca9-81c7af0be612"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.2301, -0.0203, -0.1844, -0.1309, 0.0579, ..., 0.0858, 0.0993, 0.0085, 0.0286, -0.0227]  (5440 dims)\n",
            "[-0.1141, 0.2136, -0.0862, 0.1255, -0.0316, ..., -0.0032, -0.0201, -0.1215, 0.1928, 0.0242]  (5440 dims)\n",
            "[-0.4397, 0.0266, -0.4585, -0.2792, 0.1301, ..., 0.0193, 0.0677, -0.2811, -0.1714, -0.3655]  (5440 dims)\n",
            "[-0.1588, -0.2861, -0.3716, -0.5853, -0.0135, ..., -0.0050, -0.0439, -0.2610, -0.3269, -0.1673]  (5440 dims)\n",
            "[-0.5520, -0.4076, -0.5507, -0.3132, -0.0121, ..., -0.1518, -0.1965, -0.4999, -0.1406, -0.3440]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo mismo con up_proj:"
      ],
      "metadata": {
        "id": "Q3SeaAZzIAGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_up = model.model.layers[0].mlp.up_proj.weight      # (5440, 2048)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {W_up.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_up[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_up.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_up[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_up.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_up.shape[0]} dims) \"*11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftYXIcUUFUAi",
        "outputId": "eae68f9c-6240-495e-98c8-05486b6cf8ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5440, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0232, 0.0148, 0.0086, 0.0011, 0.0030, ..., 0.0020, 0.0105, -0.0013, -0.0025, 0.0005]  (2048 dims)\n",
            "[0.0148, 0.0039, 0.0054, 0.0132, 0.0262, ..., 0.0110, 0.0024, -0.0229, 0.0049, -0.0170]  (2048 dims)\n",
            "[0.0113, -0.0081, -0.0240, 0.0106, -0.0188, ..., -0.0240, 0.0028, -0.0022, 0.0034, 0.0170]  (2048 dims)\n",
            "[0.0031, 0.0095, 0.0381, 0.0109, 0.0171, ..., 0.0097, 0.0276, 0.0023, 0.0041, -0.0010]  (2048 dims)\n",
            "[-0.0170, 0.0145, -0.0059, -0.0110, 0.0039, ..., 0.0023, -0.0135, -0.0214, 0.0107, -0.0214]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0051, 0.0002, -0.0075, 0.0070, 0.0297, ..., 0.0135, 0.0195, 0.0223, -0.0352, -0.0012]  (2048 dims)\n",
            "[-0.0073, -0.0028, 0.0122, 0.0074, 0.0155, ..., -0.0057, -0.0052, 0.0003, 0.0029, -0.0143]  (2048 dims)\n",
            "[-0.0072, -0.0215, -0.0074, 0.0048, 0.0058, ..., -0.0117, -0.0055, -0.0156, -0.0135, -0.0033]  (2048 dims)\n",
            "[0.0131, -0.0057, -0.0150, -0.0152, 0.0078, ..., 0.0064, -0.0057, -0.0030, 0.0167, 0.0065]  (2048 dims)\n",
            "[-0.0016, -0.0009, 0.0037, 0.0195, -0.0217, ..., 0.0156, -0.0009, 0.0066, 0.0010, 0.0114]  (2048 dims)\n",
            "\n",
            " (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos `hidden_states_normalized` (la salida del post_attention_layernorm) por W_up:"
      ],
      "metadata": {
        "id": "cEeDmIftHjD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "up_output = hidden_states_normalized @ W_up.T  # (5, 2048) @ (2048, 5440) = (5, 5440)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {up_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = up_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({up_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyJnyPF8HjiJ",
        "outputId": "4c26f59f-769a-4d07-d386-9407273b71b4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[0.1364, 0.0267, -0.0092, -0.0883, 0.0097, ..., 0.0462, 0.0049, 0.1218, 0.0402, 0.0662]  (5440 dims)\n",
            "[0.1068, -0.0558, 0.0571, 0.0145, -0.0728, ..., 0.0251, 0.1501, -0.0627, -0.0286, 0.0423]  (5440 dims)\n",
            "[-0.0670, -0.0713, 0.0135, -0.1234, 0.0492, ..., 0.1957, -0.0729, 0.0221, 0.0221, -0.0974]  (5440 dims)\n",
            "[-0.0382, 0.0599, -0.1039, 0.0294, 0.2355, ..., -0.0861, -0.0017, 0.1348, -0.0554, -0.1734]  (5440 dims)\n",
            "[-0.1870, 0.0461, -0.0889, -0.0484, 0.2200, ..., 0.2325, 0.1005, 0.0932, 0.1295, 0.0161]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo mismo con down_proj:"
      ],
      "metadata": {
        "id": "9kHhLMkSIZGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_down = model.model.layers[0].mlp.down_proj.weight  # (2048, 5440)\n",
        "\n",
        "print(f\"\\nW_down (down_proj): {W_down.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_down[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_down.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_down[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_down.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_down.shape[0]} dims) \"*11)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttf46xatFYKL",
        "outputId": "d41f944c-b142-43dd-aeda-249542fa9e12"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_down (down_proj): torch.Size([2048, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.0015, 0.0040, -0.0049, 0.0017, 0.0132, ..., -0.0155, 0.0054, -0.0222, 0.0164, -0.0004]  (5440 dims)\n",
            "[0.0014, 0.0139, 0.0009, -0.0155, -0.0161, ..., -0.0167, 0.0065, -0.0045, 0.0020, 0.0063]  (5440 dims)\n",
            "[-0.0137, -0.0206, 0.0031, -0.0031, -0.0002, ..., -0.0089, -0.0005, -0.0096, 0.0023, 0.0277]  (5440 dims)\n",
            "[0.0050, -0.0073, -0.0092, 0.0161, 0.0065, ..., -0.0079, -0.0076, -0.0031, -0.0034, -0.0153]  (5440 dims)\n",
            "[0.0121, 0.0129, -0.0260, -0.0178, -0.0025, ..., -0.0320, 0.0132, 0.0035, 0.0077, -0.0117]  (5440 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0089, -0.0063, 0.0014, -0.0131, -0.0269, ..., -0.0125, -0.0049, 0.0173, -0.0060, 0.0042]  (5440 dims)\n",
            "[0.0197, 0.0010, 0.0157, -0.0162, -0.0189, ..., 0.0214, -0.0049, -0.0090, -0.0078, -0.0042]  (5440 dims)\n",
            "[-0.0166, 0.0064, -0.0103, 0.0146, -0.0066, ..., 0.0162, -0.0039, 0.0270, 0.0017, -0.0081]  (5440 dims)\n",
            "[-0.0125, -0.0085, 0.0013, 0.0019, -0.0078, ..., -0.0204, 0.0259, 0.0010, -0.0176, 0.0004]  (5440 dims)\n",
            "[0.0018, -0.0126, 0.0271, 0.0208, 0.0014, ..., 0.0029, 0.0081, -0.0108, 0.0092, 0.0010]  (5440 dims)\n",
            "\n",
            " (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para combinarlas hemos de hacer la activación SwiGLU:\n",
        "\n",
        "Dimensiones en cada paso:\\\n",
        "\n",
        "Input: (5, 2048)\\\n",
        "gate_proj: (5, 2048) @ (2048, 5440) → (5, 5440)\\\n",
        "up_proj: (5, 2048) @ (2048, 5440) → (5, 5440)\\\n",
        "SwiGLU: Swish + ⊙: (5, 5440) (no cambia dimensión)\\\n",
        "down_proj: (5, 5440) @ (5440, 2048) → (5, 2048)\\\n",
        "\n",
        "Recordamos que Swish es: Swish(x) = x * sigmoid(x)\n"
      ],
      "metadata": {
        "id": "DqkYDCEyI4Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_states (5, 2048)\n",
        "#     ↓\n",
        "# ├→ × W_gate^T → gate_output (5, 5440)\n",
        "# │                    ↓\n",
        "# │               Swish(gate_output) (5, 5440)\n",
        "# │                    ↓\n",
        "# └→ × W_up^T   → up_output (5, 5440)\n",
        "#                      ↓\n",
        "#         Swish(gate_output) ⊙ up_output  (5, 5440)\n",
        "#         (multiplicación elemento a elemento)\n",
        "#                      ↓\n",
        "#               × W_down^T → output (5, 2048)"
      ],
      "metadata": {
        "id": "0EePnfDDJaJr"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opción 1: Manual\n",
        "swish_gate = gate_output * torch.sigmoid(gate_output)\n",
        "\n",
        "# Opción 2: Usando SiLU (que es exactamente Swish)\n",
        "# swish_gate = torch.nn.functional.silu(gate_output)\n",
        "\n",
        "print(f\"swish_gate shape: {swish_gate.shape}  # (5, 5440)\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = swish_gate[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({swish_gate.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY60VsHfKJzj",
        "outputId": "441560a0-83ea-4dff-f596-f975bd46053f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swish_gate shape: torch.Size([5, 5440])  # (5, 5440)\n",
            "\n",
            "================================================================================\n",
            "[-0.1019, -0.0101, -0.0837, -0.0612, 0.0298, ..., 0.0448, 0.0521, 0.0043, 0.0145, -0.0112]  (5440 dims)\n",
            "[-0.0538, 0.1182, -0.0412, 0.0667, -0.0156, ..., -0.0016, -0.0099, -0.0571, 0.1057, 0.0122]  (5440 dims)\n",
            "[-0.1723, 0.0135, -0.1776, -0.1202, 0.0693, ..., 0.0097, 0.0350, -0.1209, -0.0784, -0.1497]  (5440 dims)\n",
            "[-0.0731, -0.1227, -0.1517, -0.2094, -0.0067, ..., -0.0025, -0.0215, -0.1135, -0.1370, -0.0767]  (5440 dims)\n",
            "[-0.2017, -0.1628, -0.2014, -0.1323, -0.0060, ..., -0.0701, -0.0886, -0.1887, -0.0654, -0.1427]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Multiplicar swish_gate ⊙ up_output (elemento a elemento)."
      ],
      "metadata": {
        "id": "FiV0Rg6HLAnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_intermediate = swish_gate * up_output  # (5, 5440)\n",
        "\n",
        "print(f\"swish_gate shape: {mlp_intermediate.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = mlp_intermediate[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({mlp_intermediate.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9vN3Np_Fo_h",
        "outputId": "cc2c9bfa-11f4-4fce-b2d2-b2d290304e07"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swish_gate shape: torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.0139, -0.0003, 0.0008, 0.0054, 0.0003, ..., 0.0021, 0.0003, 0.0005, 0.0006, -0.0007]  (5440 dims)\n",
            "[-0.0057, -0.0066, -0.0024, 0.0010, 0.0011, ..., -0.0000, -0.0015, 0.0036, -0.0030, 0.0005]  (5440 dims)\n",
            "[0.0115, -0.0010, -0.0024, 0.0148, 0.0034, ..., 0.0019, -0.0026, -0.0027, -0.0017, 0.0146]  (5440 dims)\n",
            "[0.0028, -0.0073, 0.0158, -0.0062, -0.0016, ..., 0.0002, 0.0000, -0.0153, 0.0076, 0.0133]  (5440 dims)\n",
            "[0.0377, -0.0075, 0.0179, 0.0064, -0.0013, ..., -0.0163, -0.0089, -0.0176, -0.0085, -0.0023]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Multiplicar por down_proj (transpuesta) para volver a 2048 dimensiones."
      ],
      "metadata": {
        "id": "haHcwjUFL6Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proyección final del MLP\n",
        "mlp_output = mlp_intermediate @ W_down.T  # (5, 5440) @ (5440, 2048) = (5, 2048)\n",
        "\n",
        "print(f\"mlp_output shape: {mlp_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = mlp_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({mlp_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Goo91hSVFYG3",
        "outputId": "18457593-b1e3-4819-8247-5ce4f0982364"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlp_output shape: torch.Size([5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0039, -0.0090, 0.0049, -0.0112, 0.0105, ..., 0.0098, 0.0073, 0.0010, 0.0006, -0.0037]  (2048 dims)\n",
            "[-0.0005, -0.0053, 0.0008, 0.0018, 0.0028, ..., 0.0037, -0.0050, -0.0073, 0.0027, -0.0076]  (2048 dims)\n",
            "[-0.0034, -0.0181, 0.0096, -0.0115, 0.0040, ..., 0.0180, -0.0284, 0.0156, 0.0081, 0.0119]  (2048 dims)\n",
            "[-0.0087, -0.0096, 0.0255, 0.0002, 0.0256, ..., 0.0059, -0.0241, -0.0102, -0.0292, -0.0039]  (2048 dims)\n",
            "[-0.0015, 0.0067, 0.0150, -0.0110, 0.0103, ..., -0.0003, -0.0202, 0.0139, -0.0120, -0.0186]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamoms ahora a una capa Multilayer Perceptron(MLP):\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "✓ post_attention_layernorm (RMSNorm)\\\n",
        "✓ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "G82zsI9-U_dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añadimos ahora otra residual connection, y así damos tick al último elemento de la lista:"
      ],
      "metadata": {
        "id": "E9pV2qWmMSOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sumar con el residual guardado antes del MLP\n",
        "final_output = residual + mlp_output  # (5, 2048)\n",
        "\n",
        "print(f\"Salida final del decoder layer 0: {final_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = final_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({final_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6WGt6w3FYEi",
        "outputId": "d9f607aa-9c5b-4559-d103-72c70a7f8ad9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida final del decoder layer 0: torch.Size([5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0194, -0.0172, -0.0141, -0.0416, 0.0058, ..., -0.0054, 0.0182, -0.0545, 0.0113, -0.0299]  (2048 dims)\n",
            "[-0.1930, -0.0961, -0.0442, 0.3005, 0.5693, ..., -0.2813, -0.2953, -0.0389, -0.1280, 0.1523]  (2048 dims)\n",
            "[-0.1305, 0.1045, 0.5149, 0.0879, 0.1843, ..., -0.0698, 0.4366, 0.3644, -0.3577, -0.3431]  (2048 dims)\n",
            "[-0.2199, 0.1747, -0.3547, 0.0404, 0.1265, ..., 0.3084, 0.1006, 0.2095, 0.0231, 0.0884]  (2048 dims)\n",
            "[-0.0364, 0.0500, 0.0754, 0.2221, 0.2924, ..., -0.7594, 0.0979, -0.0705, 0.0522, 0.4075]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vale por fin hemos terminado con 1 bloque de attention ¡pero hay 23 más!..."
      ],
      "metadata": {
        "id": "Qoh49IgsP0Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Número de capas: {model.config.num_hidden_layers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHDBWdqyPqV8",
        "outputId": "34a78683-094b-4ba6-dd20-9fb1b094fba4"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de capas: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La salida de cada capa se convierte en la entrada de la siguiente. Cada capa refina y transforma la representación de los tokens.\n",
        "\n",
        "Las 24 capas procesan secuencialmente, manteniendo siempre la dimensión (5, 2048)."
      ],
      "metadata": {
        "id": "-rtThY4ONadd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"APILAMIENTO DE LAS 24 CAPAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Usar el modelo completo para obtener las salidas de todas las capas\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=tokens,\n",
        "        output_hidden_states=True,  # Esto nos da la salida de cada capa\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "# outputs.hidden_states contiene:\n",
        "# [0] = embeddings\n",
        "# [1] = salida capa 0\n",
        "# [2] = salida capa 1\n",
        "# ...\n",
        "# [24] = salida capa 23\n",
        "\n",
        "print(f\"Número de hidden states (embeddings + 24 capas): {len(outputs.hidden_states)}\")\n",
        "\n",
        "# Mostrar evolución de cada capa\n",
        "for layer_idx in range(25):  # 0=embeddings, 1-24=capas\n",
        "    hidden = outputs.hidden_states[layer_idx]\n",
        "\n",
        "    if layer_idx == 0:\n",
        "        print(f\"\\nEmbeddings iniciales:\")\n",
        "    else:\n",
        "        print(f\"\\nCapa {layer_idx-1:2d} completada:\")\n",
        "\n",
        "    for i in range(5):\n",
        "        row = hidden[0,i]\n",
        "\n",
        "        # Primeros 5 valores\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "        # Últimos 5 valores\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "        print(f\"[{first_vals}, ..., {last_vals}]  ({hidden.shape[2]} dims)\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Shape final: {outputs.hidden_states[-1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PP3vo-pFYBe",
        "outputId": "a2e0bae3-ac04-42d7-d5f8-1f0abf8b7245"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "APILAMIENTO DE LAS 24 CAPAS\n",
            "================================================================================\n",
            "Número de hidden states (embeddings + 24 capas): 25\n",
            "\n",
            "Embeddings iniciales:\n",
            "[-0.0004, -0.0002, -0.0007, -0.0016, -0.0005, ..., -0.0009, 0.0008, -0.0027, 0.0003, -0.0008]  (2048 dims)\n",
            "[-0.0081, -0.0030, -0.0015, 0.0126, 0.0275, ..., -0.0092, -0.0146, -0.0013, -0.0075, 0.0078]  (2048 dims)\n",
            "[-0.0039, 0.0030, 0.0129, 0.0031, 0.0062, ..., -0.0020, 0.0165, 0.0102, -0.0143, -0.0113]  (2048 dims)\n",
            "[-0.0063, 0.0043, -0.0093, 0.0012, 0.0033, ..., 0.0062, 0.0043, 0.0063, 0.0019, 0.0030]  (2048 dims)\n",
            "[-0.0011, 0.0011, 0.0017, 0.0077, 0.0106, ..., -0.0179, 0.0047, -0.0026, 0.0027, 0.0149]  (2048 dims)\n",
            "\n",
            "Capa  0 completada:\n",
            "[0.0061, -0.0134, 0.0077, 0.0040, -0.0027, ..., 0.0107, -0.0186, 0.0021, 0.0059, -0.0128]  (2048 dims)\n",
            "[-0.0184, -0.0122, -0.0208, 0.0246, 0.0348, ..., 0.0054, -0.0165, 0.0060, -0.0049, -0.0173]  (2048 dims)\n",
            "[-0.0019, -0.0045, 0.0096, 0.0011, 0.0045, ..., 0.0052, 0.0139, 0.0134, -0.0110, -0.0154]  (2048 dims)\n",
            "[-0.0023, 0.0120, -0.0126, 0.0009, 0.0031, ..., 0.0065, 0.0018, 0.0031, 0.0043, 0.0011]  (2048 dims)\n",
            "[0.0021, 0.0041, 0.0005, 0.0012, 0.0077, ..., -0.0100, 0.0070, -0.0078, 0.0018, 0.0150]  (2048 dims)\n",
            "\n",
            "Capa  1 completada:\n",
            "[0.0080, -0.0083, -0.0598, -0.0063, -0.0015, ..., 0.0208, -0.0131, 0.0059, -0.0048, -0.0163]  (2048 dims)\n",
            "[0.0006, -0.0059, -0.0024, 0.0364, 0.0169, ..., 0.0193, 0.0002, 0.0152, -0.0031, -0.0176]  (2048 dims)\n",
            "[-0.0050, -0.0088, 0.0306, 0.0166, -0.0046, ..., 0.0239, 0.0409, 0.0256, -0.0221, -0.0294]  (2048 dims)\n",
            "[-0.0151, -0.0002, -0.0091, -0.0064, -0.0003, ..., 0.0067, 0.0228, -0.0028, 0.0030, 0.0107]  (2048 dims)\n",
            "[0.0054, 0.0044, 0.0151, -0.0013, 0.0087, ..., -0.0205, 0.0104, -0.0055, -0.0028, 0.0105]  (2048 dims)\n",
            "\n",
            "Capa  2 completada:\n",
            "[-0.0503, -0.0942, -0.2990, 0.0324, 0.1797, ..., 0.1450, -0.1362, -0.0488, -0.0426, 0.2972]  (2048 dims)\n",
            "[0.0028, 0.0053, -0.0369, 0.0404, 0.0148, ..., 0.0185, -0.0028, -0.0031, -0.0092, 0.0076]  (2048 dims)\n",
            "[-0.0016, -0.0066, 0.0321, 0.0127, -0.0069, ..., 0.0317, 0.0347, 0.0129, -0.0101, -0.0330]  (2048 dims)\n",
            "[-0.0219, 0.0020, -0.0351, -0.0084, -0.0122, ..., -0.0049, 0.0171, -0.0090, -0.0118, 0.0137]  (2048 dims)\n",
            "[0.0105, 0.0078, 0.0156, -0.0001, 0.0047, ..., -0.0228, 0.0164, -0.0197, -0.0032, -0.0006]  (2048 dims)\n",
            "\n",
            "Capa  3 completada:\n",
            "[-0.0510, -0.0951, -0.2940, 0.0412, 0.1759, ..., 0.1598, -0.1284, -0.0665, -0.0438, 0.2779]  (2048 dims)\n",
            "[0.0104, -0.0015, -0.0492, 0.0501, 0.0319, ..., 0.0166, -0.0260, 0.0107, -0.0134, 0.0031]  (2048 dims)\n",
            "[-0.0134, -0.0002, 0.0399, 0.0091, -0.0158, ..., 0.0176, 0.0343, 0.0386, -0.0102, -0.0270]  (2048 dims)\n",
            "[-0.0171, -0.0128, -0.0446, 0.0048, -0.0140, ..., -0.0157, 0.0258, -0.0149, 0.0045, 0.0087]  (2048 dims)\n",
            "[0.0136, -0.0055, 0.0188, 0.0015, -0.0109, ..., -0.0379, 0.0082, -0.0012, 0.0053, -0.0033]  (2048 dims)\n",
            "\n",
            "Capa  4 completada:\n",
            "[-0.0871, -0.0718, -0.2653, 0.2059, 0.1697, ..., 0.2432, -0.1823, -0.0043, -0.1436, 0.3555]  (2048 dims)\n",
            "[0.0069, 0.0176, -0.0524, 0.0525, 0.0280, ..., 0.0172, -0.0328, 0.0241, 0.0073, -0.0038]  (2048 dims)\n",
            "[-0.0166, 0.0106, 0.0144, 0.0160, -0.0229, ..., 0.0151, 0.0335, 0.0214, -0.0074, -0.0252]  (2048 dims)\n",
            "[-0.0185, -0.0130, -0.0438, 0.0146, -0.0122, ..., -0.0216, 0.0212, -0.0133, -0.0002, -0.0082]  (2048 dims)\n",
            "[-0.0101, 0.0096, 0.0012, 0.0201, -0.0301, ..., -0.0527, 0.0247, 0.0088, 0.0001, -0.0021]  (2048 dims)\n",
            "\n",
            "Capa  5 completada:\n",
            "[-0.0673, -0.0137, -0.2893, 0.2206, 0.1803, ..., 0.2666, -0.1903, 0.0131, -0.1543, 0.3765]  (2048 dims)\n",
            "[0.0095, -0.0018, -0.0796, 0.0367, 0.0425, ..., 0.0229, -0.0345, 0.0069, 0.0129, 0.0055]  (2048 dims)\n",
            "[-0.0108, 0.0282, 0.0437, 0.0074, -0.0209, ..., 0.0309, 0.0374, 0.0026, -0.0065, -0.0406]  (2048 dims)\n",
            "[-0.0253, -0.0205, -0.0055, 0.0159, 0.0138, ..., 0.0051, 0.0061, -0.0533, 0.0007, -0.0438]  (2048 dims)\n",
            "[-0.0441, -0.0256, 0.0500, 0.0276, -0.0088, ..., -0.0271, 0.0217, 0.0270, -0.0282, 0.0036]  (2048 dims)\n",
            "\n",
            "Capa  6 completada:\n",
            "[-0.0812, -0.0109, -0.3070, 0.2436, 0.2062, ..., 0.2559, -0.1881, 0.0263, -0.1697, 0.3873]  (2048 dims)\n",
            "[0.0378, 0.0057, -0.0681, 0.0406, 0.0361, ..., -0.0075, -0.0360, -0.0075, 0.0402, -0.0092]  (2048 dims)\n",
            "[-0.0101, 0.0243, 0.0492, 0.0595, -0.0046, ..., -0.0044, 0.0661, -0.0028, 0.0318, -0.0233]  (2048 dims)\n",
            "[-0.0056, -0.0206, -0.0070, -0.0047, 0.0008, ..., -0.0038, 0.0404, -0.0694, 0.0087, -0.0509]  (2048 dims)\n",
            "[-0.0500, 0.0123, 0.0392, -0.0421, 0.0030, ..., 0.0138, 0.0505, -0.0133, -0.0343, 0.0133]  (2048 dims)\n",
            "\n",
            "Capa  7 completada:\n",
            "[-0.0462, -0.0272, -0.2010, 0.2290, 0.2099, ..., 0.2054, -0.1714, 0.0008, -0.1601, 0.3828]  (2048 dims)\n",
            "[0.0167, 0.0029, -0.0728, 0.0660, 0.0501, ..., 0.0286, -0.0110, -0.0167, -0.0027, -0.0062]  (2048 dims)\n",
            "[0.0231, 0.0349, 0.0355, 0.0328, 0.0221, ..., -0.0137, 0.0726, -0.0193, 0.0191, -0.0041]  (2048 dims)\n",
            "[-0.0573, -0.0031, -0.0168, -0.0363, -0.0092, ..., -0.0050, 0.0449, -0.0732, -0.0016, -0.0784]  (2048 dims)\n",
            "[-0.0934, 0.0205, 0.0331, -0.1103, 0.0050, ..., 0.0804, -0.0034, -0.0176, -0.0527, 0.0111]  (2048 dims)\n",
            "\n",
            "Capa  8 completada:\n",
            "[-0.0453, -0.0306, -0.1601, 0.2141, 0.1713, ..., 0.1829, -0.1890, -0.0084, -0.1404, 0.3771]  (2048 dims)\n",
            "[-0.0081, -0.0099, -0.0761, 0.0494, 0.0227, ..., 0.0288, 0.0355, 0.0450, 0.0002, -0.0050]  (2048 dims)\n",
            "[0.0341, 0.0133, 0.0087, 0.0424, -0.0055, ..., -0.0033, 0.0847, 0.0378, 0.0355, 0.0218]  (2048 dims)\n",
            "[-0.0134, -0.0261, -0.0278, -0.0538, -0.0011, ..., 0.0281, 0.0372, -0.0616, 0.0089, -0.0550]  (2048 dims)\n",
            "[-0.0823, 0.0392, -0.0021, -0.0832, -0.0346, ..., 0.0679, -0.0144, 0.0073, -0.0377, 0.0090]  (2048 dims)\n",
            "\n",
            "Capa  9 completada:\n",
            "[0.0011, -0.0609, -0.1237, 0.2069, 0.1589, ..., 0.1948, -0.1965, -0.0130, -0.1161, 0.3894]  (2048 dims)\n",
            "[0.0153, -0.0162, -0.0854, 0.0417, 0.0358, ..., 0.0395, 0.0213, 0.0590, -0.0176, 0.0125]  (2048 dims)\n",
            "[0.0350, 0.0355, -0.0246, 0.0399, 0.0274, ..., -0.0189, 0.0667, 0.0355, 0.0022, 0.0082]  (2048 dims)\n",
            "[-0.0350, -0.0088, -0.0262, -0.0287, 0.0003, ..., 0.0458, 0.0550, -0.0780, -0.0007, -0.0399]  (2048 dims)\n",
            "[-0.0543, 0.0488, 0.0050, 0.0165, -0.0239, ..., 0.0818, -0.0528, -0.0700, -0.0364, -0.0137]  (2048 dims)\n",
            "\n",
            "Capa 10 completada:\n",
            "[-0.0011, -0.0519, -0.1007, 0.1981, 0.1603, ..., 0.1631, -0.1749, -0.0114, -0.0925, 0.3941]  (2048 dims)\n",
            "[0.0522, -0.0793, -0.1073, 0.0569, 0.0727, ..., 0.0095, 0.0366, 0.0167, -0.0221, 0.0033]  (2048 dims)\n",
            "[0.0426, 0.0009, -0.0617, 0.0518, 0.0730, ..., -0.0230, 0.0780, -0.0145, 0.0121, -0.0250]  (2048 dims)\n",
            "[-0.0351, -0.0368, -0.0406, 0.0114, 0.0526, ..., 0.0319, 0.0687, -0.1145, 0.0089, -0.0461]  (2048 dims)\n",
            "[-0.0720, 0.0583, 0.0403, 0.0393, 0.0001, ..., 0.0788, -0.0216, -0.0778, -0.0396, -0.0402]  (2048 dims)\n",
            "\n",
            "Capa 11 completada:\n",
            "[0.0022, -0.0411, -0.1030, 0.1885, 0.1484, ..., 0.1392, -0.1442, -0.0022, -0.0653, 0.4037]  (2048 dims)\n",
            "[0.0710, -0.0818, -0.1154, 0.0538, 0.0745, ..., -0.0074, 0.0050, -0.0044, -0.0260, -0.0156]  (2048 dims)\n",
            "[0.0726, 0.0220, -0.0609, 0.0398, 0.0526, ..., -0.0137, 0.0502, 0.0048, -0.0140, -0.0309]  (2048 dims)\n",
            "[-0.0137, -0.0105, -0.0585, -0.0072, 0.0272, ..., 0.0054, 0.0790, -0.1347, -0.0288, -0.0360]  (2048 dims)\n",
            "[-0.1376, 0.0280, 0.0541, 0.0430, 0.0080, ..., 0.0529, 0.0328, -0.1104, -0.0124, -0.0269]  (2048 dims)\n",
            "\n",
            "Capa 12 completada:\n",
            "[0.0097, -0.0098, -0.0636, 0.1444, 0.1026, ..., 0.1329, -0.1288, 0.0402, -0.0167, 0.3945]  (2048 dims)\n",
            "[0.0615, -0.0782, -0.0912, 0.0716, 0.0261, ..., -0.0116, 0.0330, 0.0257, -0.0618, -0.0370]  (2048 dims)\n",
            "[0.0539, 0.0164, -0.0189, 0.0162, 0.0088, ..., 0.0255, 0.0458, 0.0153, 0.0070, -0.0567]  (2048 dims)\n",
            "[0.0068, -0.0410, -0.0553, 0.0002, -0.0167, ..., 0.0463, 0.1167, -0.1185, -0.0078, -0.0572]  (2048 dims)\n",
            "[-0.1127, -0.0310, 0.0645, 0.0763, 0.0552, ..., 0.0599, 0.0852, -0.1056, -0.0681, -0.0016]  (2048 dims)\n",
            "\n",
            "Capa 13 completada:\n",
            "[0.0072, -0.0149, -0.0629, 0.1233, 0.0996, ..., 0.1322, -0.1242, 0.0684, -0.0124, 0.3958]  (2048 dims)\n",
            "[0.0635, -0.0553, -0.1304, 0.0678, 0.0653, ..., 0.0263, 0.0248, -0.0205, -0.0532, -0.0503]  (2048 dims)\n",
            "[0.0542, -0.0006, -0.0765, 0.0168, -0.0030, ..., 0.0268, 0.0746, -0.0063, 0.0104, -0.0630]  (2048 dims)\n",
            "[0.0126, -0.0652, -0.0203, -0.0127, -0.0224, ..., 0.0624, 0.0665, -0.1828, 0.0191, -0.0600]  (2048 dims)\n",
            "[-0.0575, -0.0227, 0.1040, 0.0808, 0.0451, ..., 0.0664, 0.1149, -0.0976, -0.0436, -0.0224]  (2048 dims)\n",
            "\n",
            "Capa 14 completada:\n",
            "[0.0160, -0.0111, -0.0505, 0.1153, 0.0980, ..., 0.1370, -0.1623, 0.1032, -0.0072, 0.3889]  (2048 dims)\n",
            "[0.0848, -0.0617, -0.1216, 0.1024, 0.0798, ..., 0.0182, 0.0789, -0.1109, -0.0640, -0.0504]  (2048 dims)\n",
            "[0.0396, 0.0336, -0.0840, 0.0524, 0.0134, ..., 0.0407, 0.1319, -0.0459, 0.0235, -0.0814]  (2048 dims)\n",
            "[-0.0472, -0.0187, -0.1156, -0.0087, -0.0277, ..., 0.0208, 0.0665, -0.3155, -0.0688, -0.1461]  (2048 dims)\n",
            "[-0.0114, -0.0204, 0.1165, 0.0441, 0.0487, ..., 0.0523, 0.1299, -0.0862, -0.0708, 0.0268]  (2048 dims)\n",
            "\n",
            "Capa 15 completada:\n",
            "[0.0606, 0.0399, -0.0828, 0.1002, 0.0552, ..., 0.1800, -0.1963, 0.0938, 0.0517, 0.3584]  (2048 dims)\n",
            "[0.0560, -0.0703, -0.1377, 0.0915, 0.0524, ..., 0.0164, 0.0742, -0.0831, -0.0710, 0.0127]  (2048 dims)\n",
            "[-0.0026, 0.0865, -0.1657, 0.0031, 0.0111, ..., 0.0935, 0.1854, 0.0272, 0.0611, -0.0948]  (2048 dims)\n",
            "[-0.0058, 0.0155, -0.1627, 0.0265, -0.0830, ..., 0.1075, 0.0679, -0.3015, -0.0951, -0.1244]  (2048 dims)\n",
            "[-0.1019, -0.0195, 0.0597, -0.0324, 0.0345, ..., 0.0743, 0.1103, -0.0001, -0.0777, -0.0129]  (2048 dims)\n",
            "\n",
            "Capa 16 completada:\n",
            "[0.0778, 0.0440, -0.0947, 0.0904, 0.0411, ..., 0.2016, -0.1886, 0.1133, 0.0932, 0.3416]  (2048 dims)\n",
            "[0.0386, -0.1048, -0.1434, 0.0679, 0.0334, ..., 0.0358, -0.0110, 0.0041, -0.0750, 0.0089]  (2048 dims)\n",
            "[-0.0275, 0.0706, -0.1718, 0.0111, -0.0104, ..., 0.0143, 0.2161, 0.0821, 0.0344, -0.1370]  (2048 dims)\n",
            "[0.0064, -0.0575, -0.1466, 0.0855, -0.0828, ..., 0.1047, 0.0328, -0.2946, -0.1338, -0.0321]  (2048 dims)\n",
            "[-0.0613, -0.0646, -0.0055, -0.0508, 0.0905, ..., 0.0358, 0.0913, 0.0122, -0.1272, 0.0347]  (2048 dims)\n",
            "\n",
            "Capa 17 completada:\n",
            "[0.0567, 0.0654, -0.0855, 0.0558, 0.0220, ..., 0.2130, -0.2015, 0.1323, 0.1083, 0.3621]  (2048 dims)\n",
            "[0.0413, -0.0901, -0.1593, 0.0448, 0.0689, ..., 0.0469, 0.0129, -0.0278, 0.0459, 0.0167]  (2048 dims)\n",
            "[-0.0392, 0.1248, -0.1760, 0.0597, 0.0355, ..., -0.0484, 0.3018, 0.0811, 0.0709, -0.1576]  (2048 dims)\n",
            "[0.0948, -0.0229, -0.1334, 0.0541, -0.0156, ..., 0.0489, 0.0159, -0.3553, -0.1496, -0.0190]  (2048 dims)\n",
            "[-0.0564, -0.0841, 0.0195, -0.0490, 0.1585, ..., 0.0700, 0.1028, -0.0020, -0.0766, 0.0563]  (2048 dims)\n",
            "\n",
            "Capa 18 completada:\n",
            "[0.0838, 0.0908, -0.1030, 0.0543, 0.0123, ..., 0.2252, -0.2244, 0.1165, 0.1193, 0.3705]  (2048 dims)\n",
            "[0.0948, -0.0855, -0.1890, -0.0017, 0.0286, ..., 0.0878, -0.0021, -0.0280, 0.0365, 0.0034]  (2048 dims)\n",
            "[0.0060, 0.0994, -0.2177, 0.0596, 0.0773, ..., -0.0340, 0.2875, 0.0197, 0.0722, -0.2175]  (2048 dims)\n",
            "[0.0118, -0.1045, -0.1325, 0.0698, -0.1072, ..., 0.0415, 0.0758, -0.3826, -0.1207, -0.0166]  (2048 dims)\n",
            "[-0.0198, -0.0568, -0.0390, -0.0500, 0.1325, ..., 0.0271, 0.0077, 0.0146, -0.0998, 0.0423]  (2048 dims)\n",
            "\n",
            "Capa 19 completada:\n",
            "[0.0886, 0.1037, -0.0838, 0.0390, 0.0123, ..., 0.2412, -0.2007, 0.1305, 0.1314, 0.3922]  (2048 dims)\n",
            "[0.1587, -0.0381, -0.2047, 0.0685, 0.0584, ..., 0.1121, 0.0200, -0.0288, 0.0170, 0.0256]  (2048 dims)\n",
            "[0.0480, 0.1764, -0.2791, 0.1301, 0.0767, ..., 0.1243, 0.4207, 0.0420, 0.0858, -0.3156]  (2048 dims)\n",
            "[-0.0357, -0.0724, -0.1461, 0.1223, -0.1265, ..., 0.1019, 0.0802, -0.2475, -0.1614, 0.0971]  (2048 dims)\n",
            "[0.0453, -0.0612, -0.0851, -0.1055, 0.1984, ..., 0.0796, -0.0408, -0.0037, -0.1432, 0.0424]  (2048 dims)\n",
            "\n",
            "Capa 20 completada:\n",
            "[0.0665, 0.1316, -0.0929, 0.0730, -0.0000, ..., 0.2776, -0.1847, 0.1415, 0.1510, 0.3898]  (2048 dims)\n",
            "[0.1399, -0.0108, -0.2143, 0.0496, 0.1082, ..., -0.0122, -0.0000, 0.0189, 0.0077, 0.0685]  (2048 dims)\n",
            "[-0.1018, 0.2152, -0.2880, 0.1107, 0.3133, ..., -0.0139, 0.4936, 0.0829, 0.1370, -0.3670]  (2048 dims)\n",
            "[0.0761, -0.0459, -0.0729, 0.0618, -0.0876, ..., 0.0122, 0.1240, -0.1662, -0.0518, 0.1404]  (2048 dims)\n",
            "[-0.0108, -0.0909, -0.0808, -0.2261, 0.2689, ..., 0.0471, 0.0222, -0.0204, -0.2124, -0.0236]  (2048 dims)\n",
            "\n",
            "Capa 21 completada:\n",
            "[0.0670, 0.1537, -0.1140, 0.0464, -0.0291, ..., 0.2745, -0.1954, 0.1596, 0.1353, 0.3859]  (2048 dims)\n",
            "[0.0880, -0.0325, -0.2440, 0.0433, 0.0660, ..., 0.0614, 0.1126, 0.1613, 0.0632, 0.0702]  (2048 dims)\n",
            "[-0.1179, 0.1774, -0.3481, 0.2139, 0.0670, ..., -0.0120, 0.9118, 0.0417, 0.0695, -0.4823]  (2048 dims)\n",
            "[0.0804, 0.0074, -0.0112, 0.0602, -0.0904, ..., -0.1117, 0.0461, -0.1517, 0.0411, 0.1542]  (2048 dims)\n",
            "[-0.1843, -0.1273, -0.1203, -0.2852, 0.2254, ..., 0.0091, 0.1873, -0.0453, -0.0738, -0.1713]  (2048 dims)\n",
            "\n",
            "Capa 22 completada:\n",
            "[-0.0869, 0.1795, 0.0161, -0.0475, 0.0100, ..., 0.1999, -0.1835, 0.3044, 0.2806, 0.4190]  (2048 dims)\n",
            "[0.2347, 0.0500, -0.4671, -0.0244, 0.0110, ..., 0.0119, 0.0758, 0.4019, 0.0425, 0.1599]  (2048 dims)\n",
            "[-0.1365, 0.3261, -0.3708, 0.1781, 0.0475, ..., -0.1386, 1.2014, 0.1080, 0.0285, -0.5826]  (2048 dims)\n",
            "[-0.0275, 0.0349, 0.1099, 0.0627, -0.1189, ..., -0.0727, -0.0790, -0.0424, -0.1420, 0.1890]  (2048 dims)\n",
            "[-0.2142, -0.1054, -0.3552, -0.4569, 0.2439, ..., -0.0674, 0.3793, -0.1261, 0.0174, -0.1954]  (2048 dims)\n",
            "\n",
            "Capa 23 completada:\n",
            "[-1.5909, -1.7375, -4.4629, -0.4475, -1.5282, ..., 1.0973, -2.6448, 0.6250, -0.4054, 0.6383]  (2048 dims)\n",
            "[1.6819, -0.9818, -3.6779, 0.2479, -1.1596, ..., 2.7471, -3.4384, 4.5534, 2.1092, -0.6639]  (2048 dims)\n",
            "[0.7666, 3.5930, -1.8386, 1.6267, -0.0512, ..., 1.3732, 2.2846, 0.4330, 1.0839, -3.3412]  (2048 dims)\n",
            "[0.7188, 2.1451, 1.3267, -0.2884, -1.1240, ..., 0.2166, -3.2900, 2.8703, -0.9791, 2.8026]  (2048 dims)\n",
            "[-1.5753, -3.9831, -3.6078, -1.3246, -2.2641, ..., 2.2254, -0.3861, -0.7341, -0.4248, -1.9969]  (2048 dims)\n",
            "\n",
            "================================================================================\n",
            "Shape final: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez salimos de las 24 capas de atención nos quedan solamente unas pocas capas para llegar al final:\n",
        "-RMSNorm final: Normaliza la salida de la última capa decoder.\n",
        "-lm_head: Proyecta de 2048 dimensiones al vocabulario completo (256000). Cada posición ahora tiene un score para cada token posible."
      ],
      "metadata": {
        "id": "A6nYo95xQMXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer_output = outputs.hidden_states[-1]\n",
        "# Paso 1: Final RMSNorm\n",
        "with torch.no_grad():\n",
        "    normalized_output = model.model.norm(last_layer_output)\n",
        "\n",
        "print(f\"\\nDespués de RMSNorm final: {normalized_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = normalized_output[0,i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({normalized_output.shape[2]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFzBGOzVNxbb",
        "outputId": "f0ccad4d-693c-4647-bd20-b0f22fb5b3ce"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de RMSNorm final: torch.Size([1, 5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-1.4676, -1.7436, -4.2174, -0.4108, -1.4097, ..., 1.1949, -2.1066, 0.5878, -0.3940, 0.6377]  (2048 dims)\n",
            "[1.5520, -0.9856, -3.4767, 0.2277, -1.0700, ..., 2.9925, -2.7396, 4.2838, 2.0507, -0.6635]  (2048 dims)\n",
            "[0.7146, 3.6436, -1.7558, 1.5091, -0.0477, ..., 1.5112, 1.8388, 0.4115, 1.0647, -3.3730]  (2048 dims)\n",
            "[0.6832, 2.2181, 1.2919, -0.2728, -1.0684, ..., 0.2430, -2.7002, 2.7816, -0.9807, 2.8849]  (2048 dims)\n",
            "[-1.4453, -3.9753, -3.3908, -1.2094, -2.0772, ..., 2.4102, -0.3059, -0.6866, -0.4106, -1.9840]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasamos ahora a 256000 dims para obtener los logits (valores asignados que corresponden 1 a 1 con los tokens del vocabulario). para ello tenemos la gigantesca matriz de pesos W_lm_head (256000, 2048)"
      ],
      "metadata": {
        "id": "ar2Rif01REp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_lm_head = model.lm_head.weight  # (256000, 2048)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MATRIZ LM_HEAD\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nW_lm_head shape: {W_lm_head.shape} \")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_lm_head[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_lm_head.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_lm_head[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_lm_head.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_lm_head.shape[0]} d) \"*11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZbIkArnRlm7",
        "outputId": "0057001d-3365-4f54-db79-5e2d734ec2ef"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MATRIZ LM_HEAD\n",
            "================================================================================\n",
            "\n",
            "W_lm_head shape: torch.Size([256000, 2048]) \n",
            "\n",
            "================================================================================\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[0.0557, 0.0265, -0.0060, 0.0295, -0.0415, ..., 0.0098, 0.0605, 0.0481, -0.0206, 0.0449]  (2048 dims)\n",
            "[-0.0036, 0.0272, 0.0037, 0.0021, 0.0128, ..., -0.0069, -0.0334, -0.0135, 0.0273, -0.0228]  (2048 dims)\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0007, 0.0010, 0.0060, 0.0011, 0.0112, ..., 0.0182, -0.0277, 0.0097, 0.0069, -0.0031]  (2048 dims)\n",
            "[0.0170, -0.0066, 0.0315, 0.0150, -0.0103, ..., 0.0146, 0.0405, -0.0033, -0.0137, 0.0142]  (2048 dims)\n",
            "[0.0211, 0.0054, 0.0060, -0.0106, -0.0054, ..., -0.0142, 0.0086, 0.0028, -0.0019, 0.0110]  (2048 dims)\n",
            "[0.0124, -0.0084, -0.0201, -0.0234, -0.0270, ..., -0.0132, -0.0018, 0.0110, -0.0072, 0.0114]  (2048 dims)\n",
            "[0.0032, -0.0082, 0.0248, -0.0067, 0.0065, ..., -0.0073, -0.0193, 0.0037, 0.0248, -0.0008]  (2048 dims)\n",
            "\n",
            " (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = normalized_output @ W_lm_head.T  # (1, 5, 2048) @ (2048, 256000) = (1, 5, 256000)\n",
        "\n",
        "# # alternativa: lm_head - proyección al vocabulario\n",
        "# with torch.no_grad():\n",
        "#     logits = model.lm_head(normalized_output)\n",
        "\n",
        "print(f\"\\nDespués de lm_head: {logits.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = logits[0,i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({logits.shape[2]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjZxyRriQu2y",
        "outputId": "71e0acc8-f4e9-479b-ed97-d95604e39845"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de lm_head: torch.Size([1, 5, 256000])\n",
            "\n",
            "================================================================================\n",
            "[-5.1194, -5.1734, 3.0396, -5.1196, -5.1196, ..., 5.0793, -1.8981, -0.2701, -0.4054, 1.1557]  (256000 dims)\n",
            "[-5.4080, -7.6178, 4.5850, -5.4066, -5.4083, ..., 2.2102, -3.9177, -1.2109, -0.0522, 1.6760]  (256000 dims)\n",
            "[-4.1653, -4.7880, 2.2707, -4.1642, -4.1657, ..., 2.8799, -2.3822, 1.6910, -0.4971, 2.6660]  (256000 dims)\n",
            "[-3.5495, -4.8614, 4.8744, -3.5508, -3.5503, ..., 4.7410, -2.1277, 0.0223, 4.7902, 1.4817]  (256000 dims)\n",
            "[-6.1523, -9.2047, 6.6189, -6.1515, -6.1524, ..., 4.2811, -2.5536, -0.5159, -0.7494, 3.2771]  (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos muy cerquita ya, para predecir el siguiente token usamos solamente la última fila de esta matriz que corresponde a la predicción para el último token de la secuencia ('...')"
      ],
      "metadata": {
        "id": "DR4aY--eTHf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PREDICCIÓN DEL SIGUIENTE TOKEN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Para predecir el siguiente token, usamos el ÚLTIMO token de la secuencia (posición 4)\n",
        "last_token_logits = logits[0, -1, :]  # (256000,)\n",
        "\n",
        "print(f\"\\nLogits del último token: {last_token_logits.shape}\")\n",
        "\n",
        "# Primeros 5 valores\n",
        "first_vals = \", \".join([f\"{v:.4f}\" for v in last_token_logits[:5]])\n",
        "\n",
        "# Últimos 5 valores\n",
        "last_vals = \", \".join([f\"{v:.4f}\" for v in last_token_logits[-5:]])\n",
        "\n",
        "print(f\"[{first_vals}, ..., {last_vals}] ({last_token_logits.shape[0]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMnW-VQ6TG4-",
        "outputId": "e096e79c-7733-4b19-ebff-681536e1dea1"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PREDICCIÓN DEL SIGUIENTE TOKEN\n",
            "================================================================================\n",
            "\n",
            "Logits del último token: torch.Size([256000])\n",
            "[-6.1523, -9.2047, 6.6189, -6.1515, -6.1524, ..., 4.2811, -2.5536, -0.5159, -0.7494, 3.2771] (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto ya sacamos las probabilidades con una softmax.\n",
        "Hemos de decir que el modelo llega hasta aquí, al usar el modelo este nos devuelve los logits. Con eso nosotros ya aplicamos softmax y la estrategia de sampleado (elección del token) que querramos. Se usan los siguiente parámetros normalmente:"
      ],
      "metadata": {
        "id": "HjuMIwZPULM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output_tokens = model.generate(\n",
        "#     tokens,\n",
        "#     max_length=50,\n",
        "#     temperature=1.0,      # ← AQUÍ se especifica\n",
        "#     top_p=0.9,           # ← AQUÍ se especifica\n",
        "#     top_k=50,            # ← AQUÍ se especifica\n",
        "#     do_sample=True       # ← True = sampling, False = greedy\n",
        "# )"
      ],
      "metadata": {
        "id": "7rhhjsdWXCkY"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explico estos valores al final del notebook como apéndice."
      ],
      "metadata": {
        "id": "kGBHLrNYX4U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar softmax para obtener probabilidades\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "print(f\"\\nProbabilidades: {probabilities.shape}\")\n",
        "print(f\"Suma de probabilidades: {probabilities.sum():.6f}  # Debe ser 1.0\")\n",
        "# Primeros 5 valores\n",
        "first_vals = \", \".join([f\"{v:.4f}\" for v in probabilities[:5]])\n",
        "\n",
        "# Últimos 5 valores\n",
        "last_vals = \", \".join([f\"{v:.4f}\" for v in probabilities[-5:]])\n",
        "\n",
        "print(f\"[{first_vals}, ..., {last_vals}] ({probabilities.shape[0]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxRrtUJGQuz-",
        "outputId": "0a8ad9dc-c074-43ab-cf14-a201786432c7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Probabilidades: torch.Size([256000])\n",
            "Suma de probabilidades: 1.000019  # Debe ser 1.0\n",
            "[0.0000, 0.0000, 0.0004, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000, 0.0000, 0.0000] (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el token con mayor probabilidad (greedy decoding)\n",
        "predicted_token_id = torch.argmax(probabilities).item()\n",
        "predicted_probability = probabilities[predicted_token_id].item()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"TOKEN PREDICHO:\")\n",
        "print(f\"  ID: {predicted_token_id}\")\n",
        "print(f\"  Texto: {tokenizer.decode([predicted_token_id])}\")\n",
        "print(f\"  Probabilidad: {predicted_probability:.4f} ({predicted_probability*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzqdbSXDQuw7",
        "outputId": "4610011e-4709-4a88-f656-34caec5da799"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TOKEN PREDICHO:\n",
            "  ID: 594\n",
            "  Texto: es\n",
            "  Probabilidad: 0.0599 (5.99%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-5 predicciones\n",
        "top5_probs, top5_ids = torch.topk(probabilities, 5)\n",
        "print(f\"\\nTop 5 predicciones:\")\n",
        "for i in range(5):\n",
        "    token_id = top5_ids[i].item()\n",
        "    prob = top5_probs[i].item()\n",
        "    text = tokenizer.decode([token_id])\n",
        "    print(f\"  {i+1}. [{token_id}] '{text}' - {prob:.4f} ({prob*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mwojcbCQuul",
        "outputId": "d8b895a2-ef80-4d93-99e1-78eb023487db"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 predicciones:\n",
            "  1. [594] 'es' - 0.0599 (5.99%)\n",
            "  2. [503] 'se' - 0.0427 (4.27%)\n",
            "  3. [1161] 'és' - 0.0409 (4.09%)\n",
            "  4. [704] 'no' - 0.0340 (3.40%)\n",
            "  5. [394] 's' - 0.0315 (3.15%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡HEMOS COMPLETADO EL RECORRIDO COMPLETO! 🎉"
      ],
      "metadata": {
        "id": "LmxOlLHhVXGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input tokens\n",
        "#     ↓\n",
        "# Embeddings (256000 → 2048)\n",
        "#     ↓\n",
        "# ┌─ CAPA 0 (y se repite 24 veces) ────┐\n",
        "# │  input_layernorm (RMSNorm)         │\n",
        "# │  Multi-Head Attention              │\n",
        "# │  Residual connection               │\n",
        "# │  post_attention_layernorm (RMSNorm)│\n",
        "# │  MLP con SwiGLU                    │\n",
        "# │  Residual connection               │\n",
        "# └────────────────────────────────────┘\n",
        "#     ↓\n",
        "# ... (capas 1-23 igual) ...\n",
        "#     ↓\n",
        "# RMSNorm FINAL (después de las 24 capas)\n",
        "#     ↓\n",
        "# lm_head (2048 → 256000)\n",
        "#     ↓\n",
        "# Softmax\n",
        "#     ↓\n",
        "# Predicción"
      ],
      "metadata": {
        "id": "mQevSwrQQuri"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de irte recordamos que esto ha sido solo para predecir el siguiente de ..., pero para predecir el siguiente hay que volver a pasar por todo el modelo incluyendo el nuevo token predicho."
      ],
      "metadata": {
        "id": "803AKrgFaF16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [<s>, açò, es, or, ...]\n",
        "#        ↓ (TODO el modelo)\n",
        "# Output: predice \"es\" como siguiente token\n",
        "\n",
        "# Para seguir generando:\n",
        "# Paso 1: Input [<s>, açò, es, or, ...] → Predice token es\n",
        "# Paso 2: Input [<s>, açò, es, or, ..., es] → Volver a pasar por TODO el modelo → Predice siguiente token\n",
        "# Paso 3: Input [<s>, açò, es, or, ..., es, nuevo_token] → Volver a pasar por TODO → Predice siguiente\n",
        "# Y así sucesivamente..."
      ],
      "metadata": {
        "id": "3fIkK62uVkDW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada vez:\n",
        "\n",
        "Se añade el token predicho al final de la secuencia\\\n",
        "Se pasa toda la secuencia por TODO el modelo de nuevo\n",
        "\n",
        "  Embeddings\\\n",
        "  24 capas completas (attention + MLP)\\\n",
        "  RMSNorm final\\\n",
        "  lm_head\\\n",
        "  Softmax\n",
        "\n",
        "\n",
        "Se toma el último logit para predecir el siguiente\n",
        "\n",
        "Por eso la generación de texto es costosa computacionalmente: cada token nuevo requiere un forward pass completo por las 24 capas.\n",
        "(Aquí es donde técnicas como KV-cache ayudan a optimizar, pero eso es otro tema)"
      ],
      "metadata": {
        "id": "I4h8C6e4aUKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APÉNDICE"
      ],
      "metadata": {
        "id": "b_-qKvnNX-ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación parámetros de model.generate():"
      ],
      "metadata": {
        "id": "Cb8dsZm3YG51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "do_sample (True/False):"
      ],
      "metadata": {
        "id": "OwBR215QYMzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do_sample=False  # Greedy decoding (siempre el token con mayor probabilidad)\n",
        "# do_sample=True   # Sampling probabilístico (introduce aleatoriedad)"
      ],
      "metadata": {
        "id": "NfwcMtRKYAXI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "temperature (float, 0.1-2.0):"
      ],
      "metadata": {
        "id": "i7ZfHp2OYVRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logits_ajustados = logits / temperature\n",
        "# probabilities = softmax(logits_ajustados)\n",
        "\n",
        "# temperature=0.5   # Más conservador, menos aleatorio\n",
        "# temperature=1.0   # Distribución original\n",
        "# temperature=1.5   # Más creativo, más aleatorio\n"
      ],
      "metadata": {
        "id": "JcUi-dD3YZV4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_k (int):"
      ],
      "metadata": {
        "id": "z2NJiB5YYm2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k=50  # Solo considera los 50 tokens más probables\n",
        "\n",
        "# Ordena tokens por probabilidad\n",
        "# Toma solo los top-k\n",
        "# Pone probabilidad 0 al resto\n",
        "# Renormaliza y samplea de esos k\n",
        "\n",
        "# Efecto: Evita tokens muy improbables/raros"
      ],
      "metadata": {
        "id": "TA8JA3rAYpkU"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_p (float entre 0 y 1, típicamente 0.9-0.95):"
      ],
      "metadata": {
        "id": "Mu3rfGWvY1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p=0.9  # Considera el conjunto mínimo de tokens que suman 90% de probabilidad\n",
        "\n",
        "# Ordena tokens por probabilidad descendente\n",
        "# Suma probabilidades acumuladas hasta llegar a p\n",
        "# Solo samplea de ese conjunto \"núcleo\"\n",
        "\n",
        "# Ejemplo:\n",
        "\n",
        "# Token A: 40%, Token B: 30%, Token C: 15%, Token D: 10%, otros: 5%\n",
        "# Con top_p=0.9 → considera A, B, C, D (suman 95% > 90%)\n",
        "# Con top_p=0.7 → considera solo A, B (suman 70%)\n",
        "\n",
        "# Ventaja sobre top_k: Es dinámico. A veces incluye 10 tokens, a veces 100, dependiendo de qué tan concentrada esté la distribución."
      ],
      "metadata": {
        "id": "UYin269RY33Z"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y por último aunque no tiene que ver con el sampleado tenemos"
      ],
      "metadata": {
        "id": "UWyjZPXnZVtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_length o max_new_tokens, eos_token_id y min_length para decidir cuándo acabar."
      ],
      "metadata": {
        "id": "DR6P2Z_DZJcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=100        # Máxima longitud TOTAL (input + generado)\n",
        "max_new_tokens=50     # Máximo número de tokens NUEVOS a generar\n",
        "eos_token_id=None         # para cuando encuentra el token de fin (ej: </s>)\n",
        "min_length=25         # longitud mínima antes de poder parar"
      ],
      "metadata": {
        "id": "VJXU6czhZAoI"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5rL0tXIQBt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}