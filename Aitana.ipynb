{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y hf-xet\n",
        "!pip install -U 'huggingface_hub[cli]'\n",
        "\n",
        "# Ahora descarga el modelo\n",
        "!huggingface-cli download gplsi/Aitana-2B-S"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RBi-6SV_uya",
        "outputId": "b8105eb2-bdda-4d46-b43c-3f93e0e572ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: hf-xet 1.1.10\n",
            "Uninstalling hf-xet-1.1.10:\n",
            "  Successfully uninstalled hf-xet-1.1.10\n",
            "Requirement already satisfied: huggingface_hub[cli] in /usr/local/lib/python3.12/dist-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub[cli]) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub[cli])\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.52)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub[cli]) (2025.10.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.14)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, hf-xet, InquirerPy\n",
            "Successfully installed InquirerPy-0.3.4 hf-xet-1.1.10 pfzy-0.3.4\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 9 files:   0% 0/9 [00:00<?, ?it/s]Downloading 'model.safetensors' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/2161af68419ee601de3bf72f0fe1fb532cee2deabb71f2e745a78e75b2e22a1b.incomplete'\n",
            "Downloading 'README.md' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/547a6c991ec1a933b8a3c17b83c378088bbe0c9d.incomplete'\n",
            "Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/1243416e65a219384cdd9100454c4c3076b37c00.incomplete'\n",
            "\n",
            "generation_config.json: 100% 224/224 [00:00<00:00, 556kB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/1243416e65a219384cdd9100454c4c3076b37c00\n",
            "Downloading 'tokenizer.model' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/ab94ddf46d14f0279254858d53770c5319c5129d47291ee2bada530271cb1292.incomplete'\n",
            "Downloading '.gitattributes' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b.incomplete'\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/22de496f6cae5f5519436a54f325368abd138882.incomplete'\n",
            "\n",
            "README.md: 0.00B [00:00, ?B/s]\u001b[ADownloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/72ecfeeb7e14d244c936169d2ed139eeae235ef1.incomplete'\n",
            "Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/c205db342a8a7321a4147629d806caac1c70a8cb1cebf4c0f3636dec7a3452e0.incomplete'\n",
            "README.md: 17.4kB [00:00, 4.15MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/547a6c991ec1a933b8a3c17b83c378088bbe0c9d\n",
            "\n",
            ".gitattributes: 1.57kB [00:00, 8.23MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/52373fe24473b1aa44333d318f578ae6bf04b49b\n",
            "Fetching 9 files:  11% 1/9 [00:00<00:02,  3.38it/s]\n",
            "config.json: 100% 722/722 [00:00<00:00, 7.40MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/22de496f6cae5f5519436a54f325368abd138882\n",
            "\n",
            "special_tokens_map.json: 100% 437/437 [00:00<00:00, 3.35MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/72ecfeeb7e14d244c936169d2ed139eeae235ef1\n",
            "\n",
            "model.safetensors:   0% 0.00/4.51G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "tokenizer.json:   0% 0.00/19.1M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/69c8945cd78d9dad80b1f2e3fba734dfc0b3fe35.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "tokenizer_config.json: 25.6kB [00:00, 105MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/69c8945cd78d9dad80b1f2e3fba734dfc0b3fe35\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model:   0% 0.00/4.81M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "tokenizer.json: 100% 19.1M/19.1M [00:01<00:00, 13.1MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/c205db342a8a7321a4147629d806caac1c70a8cb1cebf4c0f3636dec7a3452e0\n",
            "\n",
            "\n",
            "\n",
            "tokenizer.model: 100% 4.81M/4.81M [00:01<00:00, 3.32MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/ab94ddf46d14f0279254858d53770c5319c5129d47291ee2bada530271cb1292\n",
            "\n",
            "model.safetensors:   0% 746k/4.51G [00:02<4:52:33, 257kB/s]\u001b[A\n",
            "model.safetensors:   0% 2.34M/4.51G [00:04<1:59:53, 626kB/s]\u001b[A\n",
            "model.safetensors:   0% 4.66M/4.51G [00:04<48:46, 1.54MB/s] \u001b[A\n",
            "model.safetensors:   2% 71.7M/4.51G [00:08<06:08, 12.0MB/s]\u001b[A\n",
            "model.safetensors:   3% 139M/4.51G [00:09<03:07, 23.4MB/s] \u001b[A\n",
            "model.safetensors:   5% 206M/4.51G [00:10<02:04, 34.5MB/s]\u001b[A\n",
            "model.safetensors:   6% 273M/4.51G [00:10<01:32, 45.9MB/s]\u001b[A\n",
            "model.safetensors:   8% 338M/4.51G [00:14<02:17, 30.4MB/s]\u001b[A\n",
            "model.safetensors:   9% 405M/4.51G [00:17<02:27, 27.8MB/s]\u001b[A\n",
            "model.safetensors:  10% 472M/4.51G [00:17<01:40, 40.1MB/s]\u001b[A\n",
            "model.safetensors:  12% 539M/4.51G [00:21<02:24, 27.4MB/s]\u001b[A\n",
            "model.safetensors:  13% 606M/4.51G [00:25<02:51, 22.8MB/s]\u001b[A\n",
            "model.safetensors:  15% 674M/4.51G [00:26<02:07, 30.1MB/s]\u001b[A\n",
            "model.safetensors:  16% 741M/4.51G [00:35<04:10, 15.0MB/s]\u001b[A\n",
            "model.safetensors:  18% 808M/4.51G [00:35<02:57, 20.8MB/s]\u001b[A\n",
            "model.safetensors:  19% 875M/4.51G [00:40<03:11, 18.9MB/s]\u001b[A\n",
            "model.safetensors:  21% 942M/4.51G [00:40<02:17, 25.9MB/s]\u001b[A\n",
            "model.safetensors:  22% 1.01G/4.51G [00:45<02:55, 19.9MB/s]\u001b[A\n",
            "model.safetensors:  24% 1.08G/4.51G [00:51<03:35, 15.9MB/s]\u001b[A\n",
            "model.safetensors:  25% 1.14G/4.51G [00:52<02:32, 22.1MB/s]\u001b[A\n",
            "model.safetensors:  27% 1.21G/4.51G [01:02<04:10, 13.1MB/s]\u001b[A\n",
            "model.safetensors:  28% 1.28G/4.51G [01:02<02:54, 18.5MB/s]\u001b[A\n",
            "model.safetensors:  30% 1.34G/4.51G [01:10<03:53, 13.6MB/s]\u001b[A\n",
            "model.safetensors:  31% 1.41G/4.51G [01:26<06:26, 8.00MB/s]\u001b[A\n",
            "model.safetensors:  37% 1.68G/4.51G [01:27<02:11, 21.5MB/s]\u001b[A\n",
            "model.safetensors:  39% 1.75G/4.51G [01:28<01:58, 23.3MB/s]\u001b[A\n",
            "model.safetensors:  40% 1.81G/4.51G [01:31<01:49, 24.5MB/s]\u001b[A\n",
            "model.safetensors:  42% 1.88G/4.51G [01:31<01:30, 28.9MB/s]\u001b[A\n",
            "model.safetensors:  43% 1.95G/4.51G [01:37<01:55, 22.2MB/s]\u001b[A\n",
            "model.safetensors:  45% 2.01G/4.51G [01:37<01:26, 28.8MB/s]\u001b[A\n",
            "model.safetensors:  46% 2.08G/4.51G [01:38<01:08, 35.4MB/s]\u001b[A\n",
            "model.safetensors:  48% 2.15G/4.51G [01:43<01:36, 24.4MB/s]\u001b[A\n",
            "model.safetensors:  49% 2.22G/4.51G [01:43<01:13, 31.3MB/s]\u001b[A\n",
            "model.safetensors:  51% 2.28G/4.51G [01:49<01:44, 21.3MB/s]\u001b[A\n",
            "model.safetensors:  52% 2.35G/4.51G [01:49<01:13, 29.3MB/s]\u001b[A\n",
            "model.safetensors:  54% 2.42G/4.51G [01:53<01:25, 24.3MB/s]\u001b[A\n",
            "model.safetensors:  55% 2.48G/4.51G [01:55<01:18, 25.8MB/s]\u001b[A\n",
            "model.safetensors:  57% 2.55G/4.51G [01:59<01:27, 22.3MB/s]\u001b[A\n",
            "model.safetensors:  58% 2.62G/4.51G [01:59<01:01, 30.5MB/s]\u001b[A\n",
            "model.safetensors:  60% 2.68G/4.51G [02:05<01:28, 20.5MB/s]\u001b[A\n",
            "model.safetensors:  61% 2.75G/4.51G [02:05<01:00, 28.8MB/s]\u001b[A\n",
            "model.safetensors:  63% 2.82G/4.51G [02:06<00:46, 36.5MB/s]\u001b[A\n",
            "model.safetensors:  64% 2.89G/4.51G [02:06<00:34, 46.9MB/s]\u001b[A\n",
            "model.safetensors:  66% 2.95G/4.51G [02:07<00:25, 59.9MB/s]\u001b[A\n",
            "model.safetensors:  67% 3.02G/4.51G [02:09<00:30, 49.3MB/s]\u001b[A\n",
            "model.safetensors:  68% 3.09G/4.51G [02:09<00:21, 65.4MB/s]\u001b[A\n",
            "model.safetensors:  70% 3.15G/4.51G [02:10<00:18, 72.8MB/s]\u001b[A\n",
            "model.safetensors:  71% 3.22G/4.51G [02:15<00:45, 28.4MB/s]\u001b[A\n",
            "model.safetensors:  73% 3.29G/4.51G [02:16<00:30, 39.8MB/s]\u001b[A\n",
            "model.safetensors:  74% 3.36G/4.51G [02:20<00:40, 28.2MB/s]\u001b[A\n",
            "model.safetensors:  76% 3.44G/4.51G [02:20<00:27, 38.7MB/s]\u001b[A\n",
            "model.safetensors:  78% 3.50G/4.51G [02:26<00:42, 23.7MB/s]\u001b[A\n",
            "model.safetensors:  79% 3.57G/4.51G [02:26<00:28, 32.8MB/s]\u001b[A\n",
            "model.safetensors:  81% 3.64G/4.51G [02:30<00:33, 25.7MB/s]\u001b[A\n",
            "model.safetensors:  82% 3.70G/4.51G [02:30<00:22, 35.0MB/s]\u001b[A\n",
            "model.safetensors:  84% 3.77G/4.51G [02:34<00:29, 25.3MB/s]\u001b[A\n",
            "model.safetensors:  85% 3.84G/4.51G [02:35<00:19, 34.7MB/s]\u001b[A\n",
            "model.safetensors:  87% 3.91G/4.51G [02:37<00:17, 34.2MB/s]\u001b[A\n",
            "model.safetensors:  88% 3.97G/4.51G [02:40<00:18, 28.3MB/s]\u001b[A\n",
            "model.safetensors:  90% 4.04G/4.51G [02:40<00:12, 38.6MB/s]\u001b[A\n",
            "model.safetensors:  91% 4.11G/4.51G [02:41<00:08, 46.6MB/s]\u001b[A\n",
            "model.safetensors:  93% 4.17G/4.51G [02:42<00:05, 57.5MB/s]\u001b[A\n",
            "model.safetensors:  94% 4.24G/4.51G [02:42<00:03, 71.6MB/s]\u001b[A\n",
            "model.safetensors:  96% 4.31G/4.51G [02:46<00:05, 34.4MB/s]\u001b[A\n",
            "model.safetensors:  97% 4.37G/4.51G [02:47<00:02, 45.7MB/s]\u001b[A\n",
            "model.safetensors:  99% 4.44G/4.51G [02:53<00:03, 22.2MB/s]\u001b[A\n",
            "model.safetensors: 100% 4.51G/4.51G [02:56<00:00, 25.5MB/s]\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/blobs/2161af68419ee601de3bf72f0fe1fb532cee2deabb71f2e745a78e75b2e22a1b\n",
            "Fetching 9 files: 100% 9/9 [02:57<00:00, 19.71s/it]\n",
            "/root/.cache/huggingface/hub/models--gplsi--Aitana-2B-S/snapshots/6451cdb022d329cf575b2e7854f495af68d71812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcBVblstUONI",
        "outputId": "46983798-8d57-4a42-b466-4a0c76a00386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"gplsi/Aitana-2B-S\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "vocab_size = len(tokenizer)\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    tokenizer=tokenizer,\n",
        "    dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = \"Termina la frase en una sola paraula: Açò es or...\"\n",
        "input_text = \"El nom correcte de la comunitat autonoma de valencia es...\"\n",
        "generation = generator(\n",
        "    input_text,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "print(f\"Result: {generation[0]['generated_text']}\")"
      ],
      "metadata": {
        "id": "4TIGZukiYlbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8828e3fc-41b3-44ef-8219-4ff51b001d19"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: El nom correcte de la comunitat autonoma de valencia es...\n",
            "\n",
            "La senyora vicepresidenta primera: Senyor diputat, ha finalitzat el seu temps. ¡Moltes gràcies! (Veus i aplaudiments)\n",
            "\n",
            "El senyor vicepresident segon del Consell: Gràcies a vosté. No se preocupe per mi. (Aplaudiments)\n",
            "\n",
            "(Ocupa la presidència el president de les Corts Valencianes, senyor Enric Morera i Català)\n",
            "\n",
            "El senyor president de les Corts Valencianes: Moltes gràcies, senyor vicepresident segon del Consell.\n",
            "\n",
            "Per al torn d'intervencions dels grups parlamentaris té la paraula l'il·lustre diputat Josep Maria Pañella pel Grup Parlamentari Compromís. Quan vullga.\n",
            "\n",
            "El senyor Pañella Alcàcer: Moltes gràcies, senyor president. Jo també li agrade que no haja parlat en valencià. I és un fet molt curiós perquè tots els diputats i diputades que s'atrevien a parlar en valencià eren els mateixos que quan parlava en castellà ho feien amb una correcció extraordinària. En este cas, crec jo, no hi han hagut cap problema. Però bé, anem a vore si ens podem entendre.\n",
            "\n",
            "Senyor conseller, li agraïsc que vinga ací a donar compte sobre els pressupostos generals de la Generalitat Valenciana i això, com ja he dit abans\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Método 1: Ver la clase exacta del tokenizer\n",
        "print(\"Clase del tokenizer:\")\n",
        "print(type(tokenizer).__name__)\n",
        "print()\n",
        "\n",
        "# Método 2: Ver el nombre completo con módulo\n",
        "print(\"Clase completa:\")\n",
        "print(type(tokenizer))\n",
        "print()\n",
        "\n",
        "# Método 3: Información del modelo base\n",
        "print(\"Información del modelo:\")\n",
        "print(f\"Model type: {tokenizer.model_max_length}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print()\n",
        "\n",
        "# Método 4: Ver archivos de configuración\n",
        "print(\"Nombre del modelo base:\")\n",
        "if hasattr(tokenizer, 'name_or_path'):\n",
        "    print(tokenizer.name_or_path)\n",
        "print()\n",
        "\n",
        "# Método 5: Inspeccionar atributos del tokenizer\n",
        "print(\"Atributos especiales:\")\n",
        "if hasattr(tokenizer, 'model_type'):\n",
        "    print(f\"Model type: {tokenizer.model_type}\")\n",
        "\n",
        "# Método 6: Ver toda la configuración\n",
        "print(\"\\nConfiguración completa:\")\n",
        "print(tokenizer)"
      ],
      "metadata": {
        "id": "Wjj_I0Z3b6KE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5ad5c7-e546-4c1f-e689-6694ec35236f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clase del tokenizer:\n",
            "LlamaTokenizerFast\n",
            "\n",
            "Clase completa:\n",
            "<class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "\n",
            "Información del modelo:\n",
            "Model type: 1000000000000000019884624838656\n",
            "Vocab size: 256000\n",
            "\n",
            "Nombre del modelo base:\n",
            "gplsi/Aitana-2B-S\n",
            "\n",
            "Atributos especiales:\n",
            "\n",
            "Configuración completa:\n",
            "LlamaTokenizerFast(name_or_path='gplsi/Aitana-2B-S', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t3: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t4: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t5: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t6: AddedToken(\"<|reserved_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t7: AddedToken(\"<|reserved_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t8: AddedToken(\"<|reserved_token_3|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t9: AddedToken(\"<|reserved_token_4|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t10: AddedToken(\"<|reserved_token_5|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t11: AddedToken(\"<|reserved_token_6|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t12: AddedToken(\"<|reserved_token_7|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t13: AddedToken(\"<|reserved_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t14: AddedToken(\"<|reserved_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t15: AddedToken(\"<|reserved_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t16: AddedToken(\"<|reserved_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t17: AddedToken(\"<|reserved_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t18: AddedToken(\"<|reserved_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t19: AddedToken(\"<|reserved_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t20: AddedToken(\"<|reserved_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t21: AddedToken(\"<|reserved_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t22: AddedToken(\"<|reserved_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t23: AddedToken(\"<|reserved_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t24: AddedToken(\"<|reserved_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t25: AddedToken(\"<|reserved_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t26: AddedToken(\"<|reserved_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t27: AddedToken(\"<|reserved_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t28: AddedToken(\"<|reserved_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t29: AddedToken(\"<|reserved_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t30: AddedToken(\"<|reserved_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t31: AddedToken(\"<|reserved_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t32: AddedToken(\"<|reserved_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t33: AddedToken(\"<|reserved_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t34: AddedToken(\"<|reserved_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t35: AddedToken(\"<|reserved_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t36: AddedToken(\"<|reserved_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t37: AddedToken(\"<|reserved_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t38: AddedToken(\"<|reserved_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t39: AddedToken(\"<|reserved_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t40: AddedToken(\"<|reserved_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t41: AddedToken(\"<|reserved_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t42: AddedToken(\"<|reserved_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t43: AddedToken(\"<|reserved_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t44: AddedToken(\"<|reserved_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t45: AddedToken(\"<|reserved_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t46: AddedToken(\"<|reserved_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t47: AddedToken(\"<|reserved_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t48: AddedToken(\"<|reserved_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t49: AddedToken(\"<|reserved_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t50: AddedToken(\"<|reserved_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t51: AddedToken(\"<|reserved_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t52: AddedToken(\"<|reserved_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t53: AddedToken(\"<|reserved_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t54: AddedToken(\"<|reserved_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t55: AddedToken(\"<|reserved_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t56: AddedToken(\"<|reserved_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t57: AddedToken(\"<|reserved_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t58: AddedToken(\"<|reserved_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t59: AddedToken(\"<|reserved_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t60: AddedToken(\"<|reserved_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t61: AddedToken(\"<|reserved_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t62: AddedToken(\"<|reserved_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t63: AddedToken(\"<|reserved_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t64: AddedToken(\"<|reserved_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t65: AddedToken(\"<|reserved_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t66: AddedToken(\"<|reserved_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t67: AddedToken(\"<|reserved_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t68: AddedToken(\"<|reserved_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t69: AddedToken(\"<|reserved_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t70: AddedToken(\"<|reserved_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t71: AddedToken(\"<|reserved_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t72: AddedToken(\"<|reserved_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t73: AddedToken(\"<|reserved_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t74: AddedToken(\"<|reserved_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t75: AddedToken(\"<|reserved_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t76: AddedToken(\"<|reserved_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t77: AddedToken(\"<|reserved_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t78: AddedToken(\"<|reserved_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t79: AddedToken(\"<|reserved_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t80: AddedToken(\"<|reserved_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t81: AddedToken(\"<|reserved_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t82: AddedToken(\"<|reserved_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t83: AddedToken(\"<|reserved_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t84: AddedToken(\"<|reserved_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t85: AddedToken(\"<|reserved_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t86: AddedToken(\"<|reserved_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t87: AddedToken(\"<|reserved_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t88: AddedToken(\"<|reserved_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t89: AddedToken(\"<|reserved_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t90: AddedToken(\"<|reserved_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t91: AddedToken(\"<|reserved_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t92: AddedToken(\"<|reserved_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t93: AddedToken(\"<|reserved_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t94: AddedToken(\"<|reserved_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t95: AddedToken(\"<|reserved_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t96: AddedToken(\"<|reserved_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t97: AddedToken(\"<|reserved_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t98: AddedToken(\"<|reserved_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t99: AddedToken(\"<|reserved_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t100: AddedToken(\"<|reserved_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t101: AddedToken(\"<|reserved_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t102: AddedToken(\"<|reserved_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t103: AddedToken(\"<|reserved_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t104: AddedToken(\"\\r\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t105: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t106: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t107: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t108: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t109: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t110: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t111: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t112: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t113: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t114: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t115: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t116: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t117: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t118: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t119: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t120: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t121: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t122: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t123: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t124: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t125: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t126: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t127: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t128: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t129: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t130: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t131: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t132: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t133: AddedToken(\"\n",
            "\n",
            "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t134: AddedToken(\"\n",
            "\n",
            "\n",
            "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\n",
        "    '102;194;165', '252;141;98', '141;160;203',\n",
        "    '231;138;195', '166;216;84', '255;217;47'\n",
        "]\n",
        "\n",
        "def show_tokens(sentence: str, model_id: str):\n",
        "    \"\"\" Show the tokens each separated by a different color \"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(\n",
        "            f'\\x1b[0;30;48;2;{colors[idx % len(colors)]}m' +\n",
        "            tokenizer.decode(t) +\n",
        "            '\\x1b[0m', t,\n",
        "            end=' '\n",
        "        )\n",
        "\n",
        "def show_token_ids(sentence: str, model_id: str):\n",
        "    \"\"\" Show the tokens\"\"\"\n",
        "\n",
        "    # Load the tokenizer and tokenize the input\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    token_ids = tokenizer(sentence).input_ids\n",
        "\n",
        "    # Extract vocabulary length\n",
        "    print(f\"Vocab length: {len(tokenizer)}\")\n",
        "\n",
        "    # Print a colored list of tokens\n",
        "    for idx, t in enumerate(token_ids):\n",
        "        print(t)"
      ],
      "metadata": {
        "id": "Kl6oAgAoao-j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_name = tokenizer.__class__.__name__\n",
        "print(tokenizer_name)"
      ],
      "metadata": {
        "id": "mA4Uqo9jbIeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19b8797-38f0-4a9b-f949-69bbd665c8b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitectura del modelo"
      ],
      "metadata": {
        "id": "R87mRMwmhrfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración del Modelo\n",
        "Antes de empezar, verifiquemos los detalles arquitectónicos:"
      ],
      "metadata": {
        "id": "buY4dwGeRg6s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d365db13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7887df81-cc3b-4d94-bffe-9082b88fe086"
      },
      "source": [
        "# Load the model again to inspect its architecture\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(256000, 2048)\n",
            "    (layers): ModuleList(\n",
            "      (0-23): 24 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=2048, out_features=5440, bias=False)\n",
            "          (up_proj): Linear(in_features=2048, out_features=5440, bias=False)\n",
            "          (down_proj): Linear(in_features=5440, out_features=2048, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama la atención el tamaño del embedding:\n",
        "Embedding(256000, 2048)\n",
        "\n",
        "esto implica un vocabulario de 256000 tokens y una dimensión de 2048 (que se mantendrá a lo largo de las transformaciones en el transformer)."
      ],
      "metadata": {
        "id": "VHI6q6Whh1bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"CONFIGURACIÓN DEL MODELO AITANA-2B-S\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Número de capas: {model.config.num_hidden_layers}\")\n",
        "print(f\"Dimensión oculta: {model.config.hidden_size}\")\n",
        "print(f\"Attention heads: {model.config.num_attention_heads}\")\n",
        "print(f\"KV heads: {model.config.num_key_value_heads}\")\n",
        "print(f\"Vocabulario: {len(tokenizer)}\")\n",
        "print(f\"Intermediate size (MLP): {model.config.intermediate_size}\")\n",
        "\n",
        "# Verificar RoPE\n",
        "if hasattr(model.config, 'rope_theta'):\n",
        "    print(f\"Usa RoPE con theta: {model.config.rope_theta}\")\n",
        "else:\n",
        "    print(\"No usa RoPE\")\n",
        "\n",
        "print(f\"\\nParámetros totales: {model.num_parameters():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdpzqpyHRoXn",
        "outputId": "24c4f211-b400-48de-8be9-cde2a92d98b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIGURACIÓN DEL MODELO AITANA-2B-S\n",
            "================================================================================\n",
            "Número de capas: 24\n",
            "Dimensión oculta: 2048\n",
            "Attention heads: 16\n",
            "KV heads: 16\n",
            "Vocabulario: 256000\n",
            "Intermediate size (MLP): 5440\n",
            "Usa RoPE con theta: 10000.0\n",
            "\n",
            "Parámetros totales: 2,253,490,176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspeccionamos el método forward para asegurarnos del orden correcto de las capas."
      ],
      "metadata": {
        "id": "eORW6URcnjqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir el método forward\n",
        "import inspect\n",
        "print(inspect.getsource(model.model.layers[0].__class__.forward))"
      ],
      "metadata": {
        "id": "yj6ZdhtjnJLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caac629c-1632-4b82-88bb-daa1684d88c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        attention_mask: Optional[torch.Tensor] = None,\n",
            "        position_ids: Optional[torch.LongTensor] = None,\n",
            "        past_key_values: Optional[Cache] = None,\n",
            "        use_cache: Optional[bool] = False,\n",
            "        cache_position: Optional[torch.LongTensor] = None,\n",
            "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
            "        **kwargs: Unpack[TransformersKwargs],\n",
            "    ) -> torch.Tensor:\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.input_layernorm(hidden_states)\n",
            "        # Self Attention\n",
            "        hidden_states, _ = self.self_attn(\n",
            "            hidden_states=hidden_states,\n",
            "            attention_mask=attention_mask,\n",
            "            position_ids=position_ids,\n",
            "            past_key_values=past_key_values,\n",
            "            use_cache=use_cache,\n",
            "            cache_position=cache_position,\n",
            "            position_embeddings=position_embeddings,\n",
            "            **kwargs,\n",
            "        )\n",
            "        hidden_states = residual + hidden_states\n",
            "\n",
            "        # Fully Connected\n",
            "        residual = hidden_states\n",
            "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
            "        hidden_states = self.mlp(hidden_states)\n",
            "        hidden_states = residual + hidden_states\n",
            "        return hidden_states\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos (as standard in Llama models):\n",
        "\n",
        "1-Layer Normalization\n",
        "2-Self Attention\n",
        "3-Layer Normalization\n",
        "4-Multilayer Perceptron (Feed forward layer)\n"
      ],
      "metadata": {
        "id": "i9wUF3tsnysA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicio del viaje predictivo.\n",
        "Vamos a analizar por todas las transformaciones que pasa nuestro texto al meterle al modelo la frase:\n",
        "\n",
        "açò es or..."
      ],
      "metadata": {
        "id": "-9u0THzgkwn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"açò es or...\""
      ],
      "metadata": {
        "id": "nYezkiPglopZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_token_ids(text, model_id)"
      ],
      "metadata": {
        "id": "R8McFWchlP_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ccf24de-2453-46dc-e283-62a26a0d3554"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "1\n",
            "125647\n",
            "594\n",
            "785\n",
            "889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "de manera más visual:"
      ],
      "metadata": {
        "id": "1YER1PLbl-j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "show_tokens(text, model_id)"
      ],
      "metadata": {
        "id": "wtLjBx7saxcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b20bbea-2414-49c5-ecd4-2c1c1473d78c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "\u001b[0;30;48;2;102;194;165m<s>\u001b[0m 1 \u001b[0;30;48;2;252;141;98maçò\u001b[0m 125647 \u001b[0;30;48;2;141;160;203mes\u001b[0m 594 \u001b[0;30;48;2;231;138;195mor\u001b[0m 785 \u001b[0;30;48;2;166;216;84m...\u001b[0m 889 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lo que le pasamos al modelo:"
      ],
      "metadata": {
        "id": "6OFRhyk4mBJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = tokenizer(text).input_ids\n",
        "print(token_ids)\n",
        "print(len(token_ids))"
      ],
      "metadata": {
        "id": "RINLsjg_l6WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e8549a-68f8-4be7-b94a-5878d2f8ce8b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 125647, 594, 785, 889]\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ver representación de grafias específicas del valenciano"
      ],
      "metadata": {
        "id": "Tewi4sGmim5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"anxova així caparaçò col·legi l'horta mascletà perquè\"\n",
        "show_tokens(text, model_id)"
      ],
      "metadata": {
        "id": "msa19ZmbaxZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428e941c-979e-4723-a449-12e929a522fa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab length: 256000\n",
            "\u001b[0;30;48;2;102;194;165m<s>\u001b[0m 1 \u001b[0;30;48;2;252;141;98manx\u001b[0m 126722 \u001b[0;30;48;2;141;160;203mova\u001b[0m 1255 \u001b[0;30;48;2;231;138;195maixí\u001b[0m 18268 \u001b[0;30;48;2;166;216;84mcap\u001b[0m 2153 \u001b[0;30;48;2;255;217;47mara\u001b[0m 925 \u001b[0;30;48;2;102;194;165mçò\u001b[0m 14720 \u001b[0;30;48;2;252;141;98mcol\u001b[0m 1896 \u001b[0;30;48;2;141;160;203m·\u001b[0m 255740 \u001b[0;30;48;2;231;138;195mlegi\u001b[0m 39451 \u001b[0;30;48;2;166;216;84ml\u001b[0m 414 \u001b[0;30;48;2;255;217;47m'\u001b[0m 255723 \u001b[0;30;48;2;102;194;165mhorta\u001b[0m 163861 \u001b[0;30;48;2;252;141;98mmasc\u001b[0m 170285 \u001b[0;30;48;2;141;160;203mlet\u001b[0m 736 \u001b[0;30;48;2;231;138;195mà\u001b[0m 255804 \u001b[0;30;48;2;166;216;84mperquè\u001b[0m 11203 "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontramos que \"·\", \"'\" y \"à\" son tokens únicos y que están al final del vocabulario muy cerca del 256000. Vamos a imprimir por curiosidad los 100 últimos tokens"
      ],
      "metadata": {
        "id": "iyRmIFPwjOV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in range(vocab_size - 100, vocab_size):\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "    print(f\"{token_id}: {repr(token_text)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i5FP2OY2axWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04e61d7-e7b4-48a1-bccd-c88f70b6d1c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255900: 'Ν'\n",
            "255901: '•'\n",
            "255902: 'Δ'\n",
            "255903: 'љ'\n",
            "255904: 'ń'\n",
            "255905: 'ß'\n",
            "255906: 'Ž'\n",
            "255907: 'Ф'\n",
            "255908: 'É'\n",
            "255909: 'Γ'\n",
            "255910: 'Ч'\n",
            "255911: 'Х'\n",
            "255912: 'Ρ'\n",
            "255913: 'Λ'\n",
            "255914: 'І'\n",
            "255915: 'ψ'\n",
            "255916: 'ű'\n",
            "255917: 'Υ'\n",
            "255918: 'ķ'\n",
            "255919: '‘'\n",
            "255920: 'Ú'\n",
            "255921: 'Ц'\n",
            "255922: 'ģ'\n",
            "255923: 'Я'\n",
            "255924: 'Á'\n",
            "255925: 'Ü'\n",
            "255926: 'ď'\n",
            "255927: '‑'\n",
            "255928: 'ù'\n",
            "255929: 'Β'\n",
            "255930: 'Ш'\n",
            "255931: '│'\n",
            "255932: 'ђ'\n",
            "255933: 'ï'\n",
            "255934: '€'\n",
            "255935: 'Î'\n",
            "255936: 'Ä'\n",
            "255937: 'Χ'\n",
            "255938: 'Θ'\n",
            "255939: 'ź'\n",
            "255940: 'Φ'\n",
            "255941: 'Ø'\n",
            "255942: 'Ġ'\n",
            "255943: 'Ó'\n",
            "255944: 'Í'\n",
            "255945: 'Ω'\n",
            "255946: '°'\n",
            "255947: 'ё'\n",
            "255948: 'Ö'\n",
            "255949: 'º'\n",
            "255950: 'ë'\n",
            "255951: 'Ж'\n",
            "255952: 'Э'\n",
            "255953: 'Å'\n",
            "255954: '^'\n",
            "255955: '~'\n",
            "255956: '┼'\n",
            "255957: '№'\n",
            "255958: 'ì'\n",
            "255959: 'Έ'\n",
            "255960: 'Щ'\n",
            "255961: 'ϊ'\n",
            "255962: 'Ј'\n",
            "255963: 'Ю'\n",
            "255964: '¿'\n",
            "255965: 'Ś'\n",
            "255966: 'Ż'\n",
            "255967: 'Ã'\n",
            "255968: 'È'\n",
            "255969: 'Ħ'\n",
            "255970: 'Ό'\n",
            "255971: '\\u200b'\n",
            "255972: '²'\n",
            "255973: 'Ş'\n",
            "255974: 'Й'\n",
            "255975: 'ð'\n",
            "255976: '©'\n",
            "255977: 'Ά'\n",
            "255978: 'ا'\n",
            "255979: 'Į'\n",
            "255980: 'Õ'\n",
            "255981: 'Є'\n",
            "255982: 'ŵ'\n",
            "255983: 'Ā'\n",
            "255984: 'Ζ'\n",
            "255985: 'Ċ'\n",
            "255986: 'À'\n",
            "255987: 'Ė'\n",
            "255988: 'Ξ'\n",
            "255989: 'Ř'\n",
            "255990: 'Ò'\n",
            "255991: '®'\n",
            "255992: 'œ'\n",
            "255993: 'Ł'\n",
            "255994: '→'\n",
            "255995: '´'\n",
            "255996: 'Ī'\n",
            "255997: '的'\n",
            "255998: '×'\n",
            "255999: '，'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "por tanto le pasamos al modelo la secuencia\n",
        "[1, 125647, 594, 785, 889]\n",
        "\n",
        "La primera capa es el embedding, que (consultando en una tabla de (256000, 2048)nos va a devolver un vector de dimensión 2048 para cada token, es decir un tensor (5, 2048)"
      ],
      "metadata": {
        "id": "i1sDn-jivtZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = torch.tensor([[1, 125647, 594, 785, 889]])  # Shape: (1, 5)\n",
        "\n",
        "# Paso 1: Embeddings\n",
        "with torch.no_grad():\n",
        "    embeddings = model.model.embed_tokens(tokens)\n",
        "\n",
        "print(\"Después de embed_tokens:\")\n",
        "print(f\"Shape: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "YCmfnM8mjNBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28875c1-2861-41f2-9ca5-468f007c9700"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de embed_tokens:\n",
            "Shape: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualizamos los primeros 10 dígitos de cada uno."
      ],
      "metadata": {
        "id": "gZZWzX3nwoKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for embedding in embeddings[0][:]:\n",
        "    print(embedding[:8])"
      ],
      "metadata": {
        "id": "HFPZYstiwmRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1cbba97-a622-44e7-d3e2-a39818ea3ebb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0004, -0.0002, -0.0007, -0.0016, -0.0005,  0.0002,  0.0071,  0.0006])\n",
            "tensor([-0.0081, -0.0030, -0.0015,  0.0126,  0.0275, -0.0022,  0.0211, -0.0004])\n",
            "tensor([-0.0039,  0.0030,  0.0129,  0.0031,  0.0062, -0.0005, -0.0036, -0.0053])\n",
            "tensor([-0.0063,  0.0043, -0.0093,  0.0012,  0.0033,  0.0076, -0.0021,  0.0039])\n",
            "tensor([-0.0011,  0.0011,  0.0017,  0.0077,  0.0106,  0.0103, -0.0118, -0.0073])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "de manera más visual:"
      ],
      "metadata": {
        "id": "ebbWiTN6yIDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Embeddings - Shape: (1, 5, 2048)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "for i in range(5):\n",
        "    row = embeddings[0][i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"Token {i} ({tokens_names[i]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "8GF80cknw1D9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39c85e50-5ead-4117-8b59-10f10d2a0937"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings - Shape: (1, 5, 2048)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0004, -0.0002, -0.0007, -0.0016, -0.0005, ..., -0.0009, 0.0008, -0.0027, 0.0003, -0.0008]  (2048 dims)\n",
            "Token 1 ( açò): [-0.0081, -0.0030, -0.0015, 0.0126, 0.0275, ..., -0.0092, -0.0146, -0.0013, -0.0075, 0.0078]  (2048 dims)\n",
            "Token 2 (  es): [-0.0039, 0.0030, 0.0129, 0.0031, 0.0062, ..., -0.0020, 0.0165, 0.0102, -0.0143, -0.0113]  (2048 dims)\n",
            "Token 3 (  or): [-0.0063, 0.0043, -0.0093, 0.0012, 0.0033, ..., 0.0062, 0.0043, 0.0063, 0.0019, 0.0030]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0011, 0.0011, 0.0017, 0.0077, 0.0106, ..., -0.0179, 0.0047, -0.0026, 0.0027, 0.0149]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se normaliza a lo largo de las filas. En este caso se hace la RMSNorm, que calcula la raíz cuadrada del promedio de los cuadrados, divide cada valor por él y multiplica por un valor gamma aprendible. Esto es una transformación matemática que no altera la dimensión."
      ],
      "metadata": {
        "id": "UM4NQEBqyj0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized = model.model.layers[0].input_layernorm(embeddings)\n",
        "print(\"\\nDespués de input_layernorm (primera capa):\")\n",
        "print(f\"Shape: {normalized.shape}\")"
      ],
      "metadata": {
        "id": "rbw0xj95xp09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c76ed3c-39be-411c-c97c-4f5a347287dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de input_layernorm (primera capa):\n",
            "Shape: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Después de input_layernorm - Shape: (1, 5, 2048)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "for i in range(5):\n",
        "    row = normalized[0, i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "i6sWT1zly6mH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef86984-89a3-465e-beec-5ff8f7ad1158"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de input_layernorm - Shape: (1, 5, 2048)\n",
            "================================================================================\n",
            "[-0.0077, -0.0051, -0.0163, -0.0330, -0.0092, ..., -0.0267, 0.0137, -0.0585, 0.0046, -0.0162]  (2048 dims)\n",
            "[-0.1861, -0.0883, -0.0411, 0.2961, 0.5605, ..., -0.2989, -0.2932, -0.0325, -0.1344, 0.1704]  (2048 dims)\n",
            "[-0.1271, 0.1251, 0.5070, 0.1055, 0.1801, ..., -0.0951, 0.4700, 0.3511, -0.3674, -0.3536]  (2048 dims)\n",
            "[-0.2106, 0.1875, -0.3764, 0.0404, 0.0988, ..., 0.2971, 0.1256, 0.2221, 0.0514, 0.0960]  (2048 dims)\n",
            "[-0.0328, 0.0419, 0.0626, 0.2359, 0.2824, ..., -0.7634, 0.1218, -0.0827, 0.0622, 0.4264]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso es ya la famosa capa de atención. Para saber exactamente qué operaciones se aplican hemos de averiguar qué tipo de atención se usa en esta arquitectura MHA estándar (todos los heads son iguales) o GQA (K y V comparten heads entre grupos de Q)."
      ],
      "metadata": {
        "id": "-SIsz65m1Rjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Orden de operaciones en Attention\n",
        "\n",
        "1. **Proyectar Q, K, V** (usando q_proj, k_proj, v_proj)\n",
        "2. **Aplicar RoPE a Q y K** ← Codifica posición mediante rotación\n",
        "3. **Calcular scores** Q @ K.T\n",
        "4. **Aplicar máscara causal** ← Bloquea tokens futuros\n",
        "5. **Softmax** ← Convierte a probabilidades\n",
        "6. **Multiplicar por V** ← Combina información"
      ],
      "metadata": {
        "id": "y0k4VV8XTwGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proyectar Q, K, V"
      ],
      "metadata": {
        "id": "Xx_yylV3T9eL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"num_attention_heads: {model.config.num_attention_heads}\")\n",
        "print(f\"num_key_value_heads: {model.config.num_key_value_heads}\")"
      ],
      "metadata": {
        "id": "9cxJ6cA5zUyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1975715-2529-4e21-9633-97a6c2e726c6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_attention_heads: 16\n",
            "num_key_value_heads: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos 16 heads. Esto implica que cada head procesa 2048/16 = 128 dimensiones. Por tanto tenemos 16 Queries, 16 Keys y 16 Values (QKV)"
      ],
      "metadata": {
        "id": "j6doSCGf1v_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente operación en la transformación lineal Q, donde cada uno de nuestros vectores de 2048 dims se multiplica por una matriz (2048,2048), lo que nos devuelve otro vector de dim 2048. Al tener 5 de ellos no hemos sufrido ninguna transformación en las dimensiones."
      ],
      "metadata": {
        "id": "ZTvh02rd4PfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q = model.model.layers[0].self_attn.q_proj(normalized)  # Linear(2048 → 2048)\n",
        "Q.shape"
      ],
      "metadata": {
        "id": "iL-QQE9X1lQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeeefe97-9fb0-41f7-83b1-52f0f6fcd4fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 2048])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí tenemos la matriz de pesos (adquirida durante el entrenamiento) W_q."
      ],
      "metadata": {
        "id": "6f7aABsGnkjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_q = model.model.layers[0].self_attn.q_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos W_q: {W_q.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_q[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_q[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "5p5_N36pnGuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35edad37-d9d3-41a3-e627-2060a2c882d0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_q: torch.Size([2048, 2048])\n",
            "[-0.0308, 0.0061, 0.0210, -0.0179, 0.0299, ..., -0.0145, 0.0255, -0.0206, -0.0233, 0.0396]  (2048 dims)\n",
            "[-0.0152, -0.0139, 0.0317, -0.0513, 0.0030, ..., -0.0072, 0.0476, 0.0098, -0.0405, 0.0171]  (2048 dims)\n",
            "[0.0388, 0.0219, -0.0245, 0.0486, 0.0085, ..., -0.0151, -0.0471, 0.0023, 0.1196, -0.0045]  (2048 dims)\n",
            "[-0.0228, -0.0154, 0.0121, -0.0112, 0.0056, ..., -0.0258, 0.0459, -0.0466, -0.1104, -0.0054]  (2048 dims)\n",
            "[0.0168, 0.0243, -0.0151, -0.0011, -0.0320, ..., -0.0091, -0.0430, 0.0151, 0.0596, 0.0203]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0304, -0.0062, -0.0016, 0.0026, 0.0040, ..., -0.0054, -0.0045, -0.0267, 0.0132, 0.0014]  (2048 dims)\n",
            "[0.0266, 0.0106, -0.0063, 0.0041, 0.0048, ..., 0.0184, -0.0197, -0.0083, 0.0146, -0.0020]  (2048 dims)\n",
            "[-0.0182, 0.0386, 0.0042, -0.0300, -0.0192, ..., 0.0228, -0.0113, 0.0557, 0.0055, 0.0194]  (2048 dims)\n",
            "[-0.0027, -0.0091, 0.0171, 0.0221, -0.0170, ..., 0.0284, 0.0065, -0.0208, 0.0110, -0.0172]  (2048 dims)\n",
            "[-0.0284, 0.0109, 0.0142, 0.0014, -0.0170, ..., 0.0074, 0.0157, 0.0231, -0.0172, -0.0089]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora multiplicamos por nuestro tensor con los embeddings normalizados. Lo que nos dará Q con dimensión (5, 2048)"
      ],
      "metadata": {
        "id": "XOwe26nZnr0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "Q = all_tokens @ W_q.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#Q = model.model.layers[0].self_attn.q_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_q: {Q.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = Q[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "8CyOPXHL2nar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d45067-f4ac-422c-f9e7-99edd48db14d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_q: torch.Size([5, 2048])\n",
            "[0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (2048 dims)\n",
            "[-1.3560, -1.4548, 0.6690, -0.2317, -0.0264, ..., 0.0732, -0.0682, 0.6090, 0.3939, 0.7310]  (2048 dims)\n",
            "[-0.5737, -0.6154, 0.4625, -0.4684, 0.2226, ..., -0.1939, -0.4065, 0.4861, 0.9461, 0.8626]  (2048 dims)\n",
            "[-0.2960, -0.2892, 0.1795, -0.1695, 0.5004, ..., -0.2480, -0.3824, 0.9402, 0.8782, 0.7316]  (2048 dims)\n",
            "[-0.4210, -0.3547, 0.4076, -0.6729, 0.3303, ..., -0.0021, -0.2761, 0.7703, 0.5679, 0.7159]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De igual forma tenemos la matriz de pesos de las Keys W_k (2048, 2048)"
      ],
      "metadata": {
        "id": "XocLHbrMoJGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_k = model.model.layers[0].self_attn.k_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos K_q: {W_k.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_k[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_k[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "Wrjh3yD_n_Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a7c103-b0dd-4773-c7d1-b01610062d74"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos K_q: torch.Size([2048, 2048])\n",
            "[0.0065, -0.0530, 0.0045, -0.0178, 0.0410, ..., 0.0369, -0.0128, -0.0762, 0.0039, 0.0542]  (2048 dims)\n",
            "[0.0063, -0.0322, -0.0334, 0.0020, 0.0530, ..., 0.0255, -0.0110, -0.0854, -0.0042, 0.0698]  (2048 dims)\n",
            "[0.0193, 0.0154, 0.0361, 0.0168, -0.0483, ..., -0.0197, -0.0229, 0.0845, -0.0253, -0.0654]  (2048 dims)\n",
            "[-0.0138, 0.0010, -0.0510, -0.0288, 0.0559, ..., 0.0312, 0.0060, -0.0564, 0.0214, 0.0698]  (2048 dims)\n",
            "[0.0752, -0.0315, 0.0464, 0.0620, -0.0082, ..., -0.0330, -0.0435, 0.0381, 0.0037, -0.0459]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0214, -0.0229, -0.0177, -0.0608, 0.0096, ..., -0.0325, -0.0142, 0.0327, 0.0022, 0.0579]  (2048 dims)\n",
            "[-0.0092, -0.0461, 0.0295, -0.0160, 0.0105, ..., -0.0299, 0.0010, -0.0148, -0.0047, -0.0344]  (2048 dims)\n",
            "[0.0247, -0.0118, -0.0250, 0.0125, 0.0134, ..., -0.0035, 0.0085, 0.0037, -0.0204, -0.0115]  (2048 dims)\n",
            "[0.0204, -0.0157, -0.0304, 0.0057, 0.0120, ..., 0.0227, -0.0157, 0.0079, -0.0383, 0.0131]  (2048 dims)\n",
            "[0.0059, 0.0596, -0.0825, -0.0283, -0.0165, ..., 0.0270, -0.0203, 0.0188, -0.0281, -0.0076]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "K = all_tokens @ W_k.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#K_all = model.model.layers[0].self_attn.k_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_k: {K.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = K[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "42m_rweFoFqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f5ac6c-05bf-4b16-f775-3849475ffadb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_k: torch.Size([5, 2048])\n",
            "[0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (2048 dims)\n",
            "[-0.1193, -0.3046, 0.0923, -0.8474, -0.1341, ..., -0.0361, -0.6353, -0.0747, 0.4616, 0.5062]  (2048 dims)\n",
            "[-0.2077, -1.0465, 0.9611, -1.6995, 0.1201, ..., -1.6739, -0.4848, -0.7525, -0.1444, 0.1011]  (2048 dims)\n",
            "[-0.0302, -1.1034, 0.7296, -1.7983, -0.1476, ..., -2.2730, -0.1709, -0.7411, -0.2895, -0.0452]  (2048 dims)\n",
            "[1.1368, -0.8047, 1.3983, -3.4482, 4.9817, ..., -1.8034, -0.8704, -1.1216, -1.0419, -0.5522]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y por último y de igual manera los Values, con su W_v y su V"
      ],
      "metadata": {
        "id": "B6naGVQB_-Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_v = model.model.layers[0].self_attn.v_proj.weight\n",
        "\n",
        "print(f\"Shape de los pesos W_v: {W_v.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_v[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_v[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "wCGsBSLLp_-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4305aef0-1442-4297-ec6e-cf7d5009042e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_v: torch.Size([2048, 2048])\n",
            "[0.0008, 0.0009, 0.0023, -0.0012, 0.0022, ..., 0.0002, -0.0023, -0.0014, -0.0016, 0.0017]  (2048 dims)\n",
            "[-0.0010, 0.0024, 0.0036, -0.0007, -0.0024, ..., -0.0002, 0.0001, 0.0014, -0.0007, -0.0011]  (2048 dims)\n",
            "[0.0038, 0.0025, 0.0021, 0.0014, -0.0022, ..., -0.0003, 0.0001, 0.0002, -0.0019, 0.0010]  (2048 dims)\n",
            "[0.0030, -0.0009, 0.0023, -0.0026, 0.0020, ..., 0.0018, 0.0016, 0.0031, 0.0013, 0.0003]  (2048 dims)\n",
            "[-0.0011, 0.0017, 0.0050, 0.0012, -0.0045, ..., -0.0006, -0.0021, 0.0018, 0.0022, 0.0012]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0025, 0.0011, 0.0030, -0.0005, -0.0038, ..., -0.0032, 0.0005, 0.0024, -0.0002, 0.0018]  (2048 dims)\n",
            "[0.0001, -0.0010, 0.0001, 0.0009, -0.0016, ..., 0.0021, 0.0011, -0.0018, 0.0049, 0.0009]  (2048 dims)\n",
            "[-0.0003, -0.0006, -0.0012, 0.0032, 0.0029, ..., 0.0008, -0.0005, 0.0010, 0.0030, 0.0008]  (2048 dims)\n",
            "[-0.0029, -0.0015, 0.0002, -0.0017, -0.0024, ..., -0.0031, 0.0009, -0.0015, 0.0003, 0.0007]  (2048 dims)\n",
            "[0.0024, 0.0012, 0.0001, -0.0009, -0.0018, ..., -0.0011, 0.0006, -0.0020, -0.0029, -0.0003]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = normalized[0]  # Shape: (5, 2048)\n",
        "V = all_tokens @ W_v.T  # Shape: (5, 2048)\n",
        "# equivalente a\n",
        "#V = model.model.layers[0].self_attn.v_proj(normalized)\n",
        "\n",
        "print(f\"Shape de los pesos W_v: {V.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = V[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "hPfI1MtSqABM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7908475c-a8db-43f0-ac20-fddde5f390c2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape de los pesos W_v: torch.Size([5, 2048])\n",
            "[0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (2048 dims)\n",
            "[-0.0194, 0.0367, 0.0695, 0.0036, 0.0111, ..., -0.0239, -0.0067, -0.0332, 0.0050, -0.0243]  (2048 dims)\n",
            "[0.0151, 0.0223, -0.0127, 0.0137, 0.0391, ..., -0.0464, -0.0095, -0.0084, -0.0192, 0.0126]  (2048 dims)\n",
            "[0.0186, 0.0353, -0.0046, -0.0044, 0.0186, ..., -0.0092, -0.0029, -0.0215, -0.0029, 0.0147]  (2048 dims)\n",
            "[-0.0066, 0.0324, 0.0575, -0.0022, 0.0066, ..., 0.0185, -0.0176, 0.0189, -0.0021, 0.0139]  (2048 dims)\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividimos los 2048 valores en 16 heads de 128 dimensiones cada uno.\n",
        "\n",
        "En este modelo cada head tiene su propia Q, K, V completamente independiente. Los 16 heads trabajan en paralelo y luego se concatenan.\n",
        "\n",
        "Así hay más capacidad en el modelo, 16 proyecciones lineales independientes en paralelo dan más flexibilidad (en otras arquitecturas se tiene un solo Q, K y V, aunque ultimamente se agrupan por grupos como punto medio).\n",
        "\n",
        "Habrás oído decir que cada head aprende conceptos diferentes como semántica, sintaxis... pero en realidad no se sabe qué aprende cada head ni siquiera si son redundantes unos con otros."
      ],
      "metadata": {
        "id": "CpaaQbmrrf3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La operació que se realiza es simplemente dividir secuancialmente los 2048 en 16 trozos de 128."
      ],
      "metadata": {
        "id": "od1lnuu9tULL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración\n",
        "num_heads = 16\n",
        "head_dim = 2048 // 16  # = 128\n",
        "print(f'Número de heads: {num_heads}')\n",
        "print(f'Dimensión de cada head {head_dim}')\n",
        "# Reshape: dividir 2048 en (16 heads × 128 dims)\n",
        "Q_reshaped = Q.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "K_reshaped = K.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "V_reshaped = V.view(5, num_heads, head_dim)  # (5, 16, 128)\n",
        "\n",
        "print(f\"\\nDespués del reshape:\")\n",
        "print(f\"Q: {Q_reshaped.shape}  # (tokens, heads, head_dim)\")\n",
        "print(f\"K: {K_reshaped.shape}\")\n",
        "print(f\"V: {V_reshaped.shape}\")"
      ],
      "metadata": {
        "id": "Ug64bDMOp_70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263a0ac3-a564-4dea-f6fc-1b2343a4c329"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de heads: 16\n",
            "Dimensión de cada head 128\n",
            "\n",
            "Después del reshape:\n",
            "Q: torch.Size([5, 16, 128])  # (tokens, heads, head_dim)\n",
            "K: torch.Size([5, 16, 128])\n",
            "V: torch.Size([5, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transponemos de (tokens, heads, dim) a (heads, tokens, dim).\n",
        "\n",
        "Esto nos permite que cada uno de los 16 heads procese su secuencia completa de forma independiente y en paralelo. Es decir, head 0 procesa sus 5 tokens, head 1 procesa sus 5 tokens, etc."
      ],
      "metadata": {
        "id": "Pywqse0vvBXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose: intercambiar dimensiones (tokens, heads) → (heads, tokens)\n",
        "Q_heads = Q_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "K_heads = K_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "V_heads = V_reshaped.transpose(0, 1)  # (16, 5, 128)\n",
        "\n",
        "print(f\"\\nDespués del transpose:\")\n",
        "print(f\"Q: {Q_heads.shape}  # (heads, tokens, head_dim)\")\n",
        "print(f\"K: {K_heads.shape}\")\n",
        "print(f\"V: {V_heads.shape}\")"
      ],
      "metadata": {
        "id": "r3kioIDtp_1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37c7220-2266-464c-cc2c-bd711f48cf76"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después del transpose:\n",
            "Q: torch.Size([16, 5, 128])  # (heads, tokens, head_dim)\n",
            "K: torch.Size([16, 5, 128])\n",
            "V: torch.Size([16, 5, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"16 heads, cada uno con 5 tokens × 128 dimensiones:\\n\")\n",
        "\n",
        "tokens_names = [\"<s>\", \"açò\", \"es\", \"or\", \"...\"]\n",
        "\n",
        "print(f\"Q después del reshape: {Q_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = Q_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "ntWJwipLvLWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aefd027e-88e0-44f4-d82d-62373c9e0b35"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 heads, cada uno con 5 tokens × 128 dimensiones:\n",
            "\n",
            "Q después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., -2.6492, 1.4339, -1.5661, 1.1632, -0.4373]  (128 dims)\n",
            "Token 1 ( açò): [-1.3560, -1.4548, 0.6690, -0.2317, -0.0264, ..., -0.7550, 0.2729, -1.6073, 1.4798, 0.1336]  (128 dims)\n",
            "Token 2 (  es): [-0.5737, -0.6154, 0.4625, -0.4684, 0.2226, ..., -1.1480, -2.2189, 1.5698, -1.4523, 1.1900]  (128 dims)\n",
            "Token 3 (  or): [-0.2960, -0.2892, 0.1795, -0.1695, 0.5004, ..., -0.2370, -0.8527, 0.8555, -1.6374, 0.8495]  (128 dims)\n",
            "Token 4 ( ...): [-0.4210, -0.3547, 0.4076, -0.6729, 0.3303, ..., -0.1713, -1.2641, -0.1150, -0.1072, 0.6012]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0463, 0.0001, -0.1137, 0.2507, 0.3485, ..., 0.0239, 1.2501, 0.0091, -1.1293, 0.5304]  (128 dims)\n",
            "Token 1 ( açò): [0.1753, -0.0384, -0.1158, 0.6476, 0.1233, ..., -0.5869, 0.5340, -0.1435, -0.2585, -0.3561]  (128 dims)\n",
            "Token 2 (  es): [-0.2957, -0.0967, -0.3313, 0.7226, -0.4555, ..., -0.0589, 0.6326, 0.0926, -0.2679, -0.0041]  (128 dims)\n",
            "Token 3 (  or): [-0.1775, -0.1178, -0.2619, 0.8932, -0.2205, ..., -0.2287, 0.6506, -0.1512, 0.2409, -0.1348]  (128 dims)\n",
            "Token 4 ( ...): [-0.1746, -0.3620, -0.6023, 0.4919, -0.3494, ..., 0.3267, 1.4952, 0.4679, -0.1422, 0.3713]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1706, -0.0658, -0.0793, 0.2997, -0.3046, ..., 0.7790, -0.1864, -0.0850, 0.0032, -0.0692]  (128 dims)\n",
            "Token 1 ( açò): [0.9729, 1.0287, 0.0236, -0.3572, 0.2337, ..., 1.0374, 0.8098, -0.7264, 0.1974, -0.1934]  (128 dims)\n",
            "Token 2 (  es): [0.7203, 0.8356, 0.8809, 0.5975, -0.2755, ..., 2.0269, -0.5792, -0.3753, 0.1948, -0.2170]  (128 dims)\n",
            "Token 3 (  or): [-0.2117, -0.1933, 0.1695, 0.7177, -0.3620, ..., 0.9301, -0.3541, -0.2490, -0.1009, 0.1738]  (128 dims)\n",
            "Token 4 ( ...): [0.1204, -0.0434, 0.1668, 0.5531, -0.2229, ..., 1.1979, 0.1576, -0.2914, 0.0382, 0.0330]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2368, 0.0941, 0.4004, -0.7508, -0.3325, ..., -2.3852, 2.1380, 1.3856, 1.8737, 1.7969]  (128 dims)\n",
            "Token 1 ( açò): [-0.9052, -0.5434, -0.4093, 0.8376, 0.7514, ..., -2.3765, 2.1581, 1.1979, 1.8483, 1.7538]  (128 dims)\n",
            "Token 2 (  es): [-1.1278, -1.0152, 0.2929, -0.2057, 1.0354, ..., -1.3648, 1.2237, 0.6112, 1.0117, 0.9354]  (128 dims)\n",
            "Token 3 (  or): [-1.4702, -1.2488, -0.1364, -0.6977, 1.3201, ..., -1.4375, 1.2681, 0.6324, 1.0610, 0.9849]  (128 dims)\n",
            "Token 4 ( ...): [-0.8696, -0.8841, 0.1978, 0.0516, 0.4693, ..., -1.5825, 1.4624, 0.9933, 1.2366, 1.1462]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7162, -0.2681, -0.7130, 0.0637, 0.1057, ..., -1.9417, 0.2538, 0.2032, -0.0186, 0.1673]  (128 dims)\n",
            "Token 1 ( açò): [0.2423, -0.2437, -0.1857, -0.1681, -0.0926, ..., -1.3426, -0.1524, -0.1309, 0.2113, -0.0702]  (128 dims)\n",
            "Token 2 (  es): [0.0197, -0.1555, 0.2600, 0.3858, 0.3284, ..., -0.8291, 0.4543, 0.3647, -0.1662, 0.4049]  (128 dims)\n",
            "Token 3 (  or): [0.2969, 0.0060, -0.3040, 0.2633, 0.1149, ..., -1.1991, 0.6011, 0.4371, -0.2629, 0.4939]  (128 dims)\n",
            "Token 4 ( ...): [0.3715, 0.0877, -0.0236, 0.1735, 0.1603, ..., -1.1050, 0.1550, 0.0097, 0.1086, 0.0456]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0136, 0.2143, 0.0703, 0.2861, -0.2265, ..., -0.4979, -0.4998, -0.4526, 0.7031, 0.5648]  (128 dims)\n",
            "Token 1 ( açò): [-0.1877, 0.6651, 0.0684, 0.0887, -0.3455, ..., -0.2277, 0.1529, -0.3082, 0.8758, -0.0170]  (128 dims)\n",
            "Token 2 (  es): [0.0548, 0.5494, -0.0028, 0.6520, -0.4576, ..., 0.3936, 0.3902, 0.0573, 0.9557, -0.5112]  (128 dims)\n",
            "Token 3 (  or): [0.4428, 0.6817, 0.1330, 0.2191, -0.4121, ..., -0.0566, 0.2678, 0.2221, 0.4069, -0.0402]  (128 dims)\n",
            "Token 4 ( ...): [0.1323, -0.0364, 0.2997, 0.6286, -0.1115, ..., 0.3132, 0.1885, -0.0788, 0.6763, -0.1910]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4356, 0.3842, 0.4366, -0.0392, -0.6289, ..., 0.3766, 0.0753, -0.0762, 0.2500, 0.2201]  (128 dims)\n",
            "Token 1 ( açò): [-0.1512, -0.2313, 0.1226, -0.1999, -0.3211, ..., 0.2323, 0.5593, 0.0498, 0.3011, -0.1351]  (128 dims)\n",
            "Token 2 (  es): [-0.4594, 0.0002, 0.4326, 0.0167, -0.5535, ..., 0.8336, 0.1073, -0.3837, -0.1297, 0.5710]  (128 dims)\n",
            "Token 3 (  or): [-0.3310, 0.4007, 0.5494, 0.1909, -0.4913, ..., 0.5548, 0.0391, -0.0483, 0.0612, 0.4027]  (128 dims)\n",
            "Token 4 ( ...): [-0.4149, 0.6708, 0.5233, 0.5578, -0.5242, ..., 0.6875, -0.0728, -0.6530, -0.0174, 0.4924]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3952, -0.0236, -1.0263, -0.1699, -0.1340, ..., -0.0407, -0.4909, 0.6510, -0.6418, 0.1959]  (128 dims)\n",
            "Token 1 ( açò): [0.9375, 0.6714, -0.5704, 0.7097, -0.0566, ..., -0.6450, -0.9232, 0.4157, -0.5969, 0.1693]  (128 dims)\n",
            "Token 2 (  es): [0.2366, -0.4750, -1.8830, 1.3823, -0.5084, ..., -0.3023, -0.9054, 0.6343, -0.3719, 0.6219]  (128 dims)\n",
            "Token 3 (  or): [0.0621, -0.3821, -1.8959, 0.4259, -0.1472, ..., -0.3808, -1.0186, 0.7626, -0.1060, 0.6481]  (128 dims)\n",
            "Token 4 ( ...): [0.1523, -0.3061, -1.8541, -0.0910, -0.4091, ..., 0.2713, -0.1362, 0.5073, 0.2179, 0.3282]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3521, -0.4142, 0.0211, -0.3359, -0.0088, ..., -0.1404, 0.3456, 0.1993, -0.1943, -0.0363]  (128 dims)\n",
            "Token 1 ( açò): [0.1636, 0.4980, -0.4347, -0.0551, 0.0801, ..., -0.2342, 0.1049, 0.2734, -0.1037, -0.2153]  (128 dims)\n",
            "Token 2 (  es): [0.2764, 0.0915, -0.2638, -0.4732, 0.2249, ..., -0.1783, -0.2319, 0.0280, -0.0632, -0.6011]  (128 dims)\n",
            "Token 3 (  or): [0.1035, 0.1167, -0.5689, -1.0045, -0.4412, ..., 0.0426, -0.0709, -0.0715, -0.4818, -0.5342]  (128 dims)\n",
            "Token 4 ( ...): [0.3403, -0.1603, 0.0905, -0.6499, 0.3287, ..., 0.4690, -0.4103, -0.3167, -0.9564, -0.7482]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5972, -0.9489, 0.6698, -0.5446, -0.0196, ..., 0.5141, -0.4475, 0.1800, -0.7482, -0.2691]  (128 dims)\n",
            "Token 1 ( açò): [0.2297, 0.0210, 0.4059, -0.2165, -0.7175, ..., -0.4145, -0.9012, 0.6049, -0.1837, 0.2825]  (128 dims)\n",
            "Token 2 (  es): [0.6454, -0.9514, 0.1140, -0.4741, -0.6463, ..., 0.4344, -0.0290, 0.5017, -0.2086, -0.0128]  (128 dims)\n",
            "Token 3 (  or): [-0.9088, -0.1555, 1.6956, 0.0507, 0.8503, ..., 0.4027, 0.3905, 0.2509, -0.5379, 0.0452]  (128 dims)\n",
            "Token 4 ( ...): [-0.5502, -0.6060, 0.6875, -0.0558, -0.8151, ..., 0.0446, -0.2260, 1.5348, 0.7467, -0.3747]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.3898, 0.5608, -0.3478, -0.7937, -0.1874, ..., -6.1992, 0.7628, -0.9464, -2.9338, -1.4260]  (128 dims)\n",
            "Token 1 ( açò): [0.4304, 0.9547, -0.2417, 0.0601, -0.4175, ..., -4.9788, 0.5727, 0.2168, -0.8108, -1.6431]  (128 dims)\n",
            "Token 2 (  es): [-0.1053, 0.0126, -0.4970, -0.2937, 0.9077, ..., -1.7626, 1.4711, 0.4254, 1.0420, 0.2493]  (128 dims)\n",
            "Token 3 (  or): [-0.1341, 0.5412, -0.6478, -0.3714, 0.5574, ..., -2.2971, 1.8675, 0.0014, 0.7817, -0.0616]  (128 dims)\n",
            "Token 4 ( ...): [-0.1815, 0.0884, -0.2799, -0.0011, 0.7434, ..., -3.0511, 1.5577, -0.6614, -0.8876, -1.0166]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0608, 0.0167, -0.1336, -0.4315, 0.2209, ..., -0.0606, -0.7908, -0.1831, -0.5492, -0.3523]  (128 dims)\n",
            "Token 1 ( açò): [0.0203, -0.1551, 0.1141, 0.5850, 0.5510, ..., -0.0855, -0.3393, -0.4315, -0.3726, 0.0073]  (128 dims)\n",
            "Token 2 (  es): [-1.0321, 0.3264, -0.1027, -0.2935, -0.0032, ..., 0.3267, -0.0851, -0.2760, -0.7708, 0.2868]  (128 dims)\n",
            "Token 3 (  or): [-0.9770, 0.4581, 0.0160, -0.4381, 0.1748, ..., 0.1164, -0.3084, -0.2633, -0.3487, 0.0658]  (128 dims)\n",
            "Token 4 ( ...): [-1.1907, 0.0451, 0.4218, -0.1758, 0.4463, ..., 0.7321, -0.2380, -0.6604, -0.5919, 0.5151]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0172, -0.2092, -0.3587, 0.3502, -0.2233, ..., 0.5501, -0.1378, -1.5415, 1.7725, 0.4384]  (128 dims)\n",
            "Token 1 ( açò): [0.0894, 0.2693, -0.4054, 0.2954, -0.5662, ..., 0.6032, 0.3408, -1.1414, 1.8246, 0.3758]  (128 dims)\n",
            "Token 2 (  es): [1.3138, 0.7973, -1.4069, -0.4350, -0.9540, ..., -0.2088, 0.0446, -1.5589, 2.3099, 1.9920]  (128 dims)\n",
            "Token 3 (  or): [0.8274, 0.2831, -0.7696, -0.1746, -0.3887, ..., 0.1155, 0.2903, -2.1808, 2.6727, 1.0851]  (128 dims)\n",
            "Token 4 ( ...): [0.5170, 0.2818, -0.8807, -0.0281, -0.5380, ..., -0.3815, 0.7386, -3.0925, 3.3272, 1.0345]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.6629, 0.4068, -0.1787, -1.1420, -0.5572, ..., -0.5138, 0.3562, 1.0724, 0.4084, 0.2699]  (128 dims)\n",
            "Token 1 ( açò): [0.9803, 0.4912, -1.2112, -1.0546, -1.2825, ..., -0.4150, 0.2592, 0.6175, 0.4214, 0.2794]  (128 dims)\n",
            "Token 2 (  es): [1.9493, 1.5695, -0.5145, -2.0043, -0.3188, ..., -0.4167, 0.4187, 0.5706, 0.3695, 0.4126]  (128 dims)\n",
            "Token 3 (  or): [2.0152, 1.3458, -0.4484, -1.9027, -0.3098, ..., -0.6995, 0.7193, 0.8272, 0.6982, 0.6889]  (128 dims)\n",
            "Token 4 ( ...): [1.2439, 0.4431, -0.3114, -0.6376, -0.1979, ..., -0.3917, 0.3658, 0.8047, 0.2960, 0.3168]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1084, -0.6354, 0.1776, -0.3193, 0.2784, ..., -0.5032, 0.6485, 0.9932, -0.7111, 0.6745]  (128 dims)\n",
            "Token 1 ( açò): [0.4021, -0.2191, 0.3832, -0.6455, -0.0418, ..., -0.2285, 0.0733, 1.2004, -0.9589, 0.8342]  (128 dims)\n",
            "Token 2 (  es): [0.7683, -0.7091, 0.1607, -0.9199, 0.2450, ..., -0.5071, 0.3840, 0.5404, -0.9958, 0.9048]  (128 dims)\n",
            "Token 3 (  or): [0.6540, -0.1735, 0.4499, -1.4005, -0.0079, ..., 0.2469, 0.3365, 0.0369, -0.6902, 0.6522]  (128 dims)\n",
            "Token 4 ( ...): [0.2004, 0.2648, 0.0084, -0.0791, 0.4677, ..., 0.1099, 1.3578, 0.3634, -1.2804, 1.2922]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0436, 0.1843, -0.1697, 0.2867, -0.0651, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (128 dims)\n",
            "Token 1 ( açò): [-0.1741, 0.3098, -0.3025, -0.2658, -0.1033, ..., 0.0732, -0.0682, 0.6090, 0.3939, 0.7310]  (128 dims)\n",
            "Token 2 (  es): [-0.3427, 0.2213, -0.6469, 0.6139, 0.5872, ..., -0.1939, -0.4065, 0.4861, 0.9461, 0.8626]  (128 dims)\n",
            "Token 3 (  or): [-0.1758, 0.3049, -0.1614, 0.1618, 0.3866, ..., -0.2480, -0.3824, 0.9402, 0.8782, 0.7316]  (128 dims)\n",
            "Token 4 ( ...): [0.0450, -0.1547, -0.3470, 0.6334, 0.4800, ..., -0.0021, -0.2761, 0.7703, 0.5679, 0.7159]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lo mismo se hace para K:"
      ],
      "metadata": {
        "id": "EfTRxrW3drS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"K después del reshape: {K_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = K_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ByVhNxdysp",
        "outputId": "05bc99c2-4ed4-4069-9cbe-efebae1b378b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., 0.8779, 0.0834, -0.0709, 0.5904, -0.6143]  (128 dims)\n",
            "Token 1 ( açò): [-0.1193, -0.3046, 0.0923, -0.8474, -0.1341, ..., 1.1073, -1.9489, -0.2811, 0.2003, -0.7695]  (128 dims)\n",
            "Token 2 (  es): [-0.2077, -1.0465, 0.9611, -1.6995, 0.1201, ..., 0.7113, -0.9554, 0.0270, 0.4592, 0.7332]  (128 dims)\n",
            "Token 3 (  or): [-0.0302, -1.1034, 0.7296, -1.7983, -0.1476, ..., 1.1968, -0.9722, 0.8353, 0.8401, 0.6133]  (128 dims)\n",
            "Token 4 ( ...): [1.1368, -0.8047, 1.3983, -3.4482, 4.9817, ..., 1.2196, -0.5812, -1.4418, 2.4327, -0.0171]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4833, -0.3272, 0.1366, -0.1066, -0.4100, ..., 1.3725, 1.4919, -0.0843, -1.8912, 0.7427]  (128 dims)\n",
            "Token 1 ( açò): [-0.9843, -0.7069, -0.1119, -0.2753, -0.3989, ..., -0.5022, 0.4551, 0.9582, -0.0558, 1.7710]  (128 dims)\n",
            "Token 2 (  es): [0.1840, 0.4601, -0.0241, -0.0505, -0.4438, ..., 0.2203, 0.0131, 1.3834, -0.4277, 1.6438]  (128 dims)\n",
            "Token 3 (  or): [0.1848, 0.1182, 0.0871, -0.0906, -0.0562, ..., 0.8820, -0.0977, 2.2479, 0.4266, 0.5862]  (128 dims)\n",
            "Token 4 ( ...): [-0.3058, 0.4503, -0.5571, 0.0103, -0.2038, ..., 1.2746, 0.1786, 0.8937, -0.6274, 1.9437]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [1.6238, -2.1105, -0.3979, 0.6275, 0.1373, ..., 0.2037, 1.9504, -0.6677, 2.1918, -1.5709]  (128 dims)\n",
            "Token 1 ( açò): [0.0238, -0.4756, -0.2042, -0.1974, 0.4665, ..., -0.4258, -0.8895, 0.4160, -2.4309, 0.4110]  (128 dims)\n",
            "Token 2 (  es): [-0.7996, -0.2498, 0.4914, 0.6140, 1.0400, ..., -0.0836, 0.4261, 2.3701, 0.0058, -0.1875]  (128 dims)\n",
            "Token 3 (  or): [-0.0892, 0.2199, -0.2973, 0.3403, 0.4278, ..., -0.6758, 0.5817, 0.2620, 0.0922, 1.8649]  (128 dims)\n",
            "Token 4 ( ...): [0.4514, -0.1033, 0.3615, 0.8567, 0.2546, ..., -0.6950, 1.1896, 1.8943, 1.8001, 0.8314]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0515, 0.0201, -0.0272, 0.0114, 0.0579, ..., -2.8270, 2.8352, 2.2631, 2.7978, 2.7694]  (128 dims)\n",
            "Token 1 ( açò): [-1.0420, -0.7445, 0.0800, -0.2186, 0.1616, ..., -0.3243, 0.2399, -0.6898, 0.1073, 0.0460]  (128 dims)\n",
            "Token 2 (  es): [0.3531, 0.0322, 0.5005, -0.3252, 0.3856, ..., -1.1498, 1.0439, 0.6893, 0.9715, 0.9601]  (128 dims)\n",
            "Token 3 (  or): [0.0867, -0.2604, 0.6376, 0.4162, -0.1442, ..., -0.8656, 0.8486, 0.1480, 0.8099, 0.7895]  (128 dims)\n",
            "Token 4 ( ...): [-0.1245, -0.1871, 0.2050, -0.5151, 0.7792, ..., -0.4241, 0.2682, -1.1600, 0.0933, 0.0375]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1220, 0.3785, -0.1131, -0.5678, 0.6471, ..., 0.2490, 0.8485, 1.3179, -1.8046, 1.0987]  (128 dims)\n",
            "Token 1 ( açò): [0.2945, 0.1428, -0.4351, -0.2576, 0.2620, ..., -1.1779, -0.5660, -1.1386, 1.0694, -0.8756]  (128 dims)\n",
            "Token 2 (  es): [0.4192, -0.4615, -0.5852, 0.0211, -0.0741, ..., 1.3735, -0.8589, -0.4867, 0.1381, -1.2530]  (128 dims)\n",
            "Token 3 (  or): [0.8165, -0.1313, -0.4496, -0.0494, 0.2291, ..., 1.2879, -1.0247, -0.7046, 0.7663, -1.5148]  (128 dims)\n",
            "Token 4 ( ...): [-0.0258, 0.1160, -0.1007, -0.1613, -0.0815, ..., 0.8664, -1.7723, -1.0826, -0.0260, -1.9231]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7487, 0.6855, 0.4504, 0.3863, -0.1420, ..., -2.3786, -0.9175, -1.4701, 0.3072, 0.4352]  (128 dims)\n",
            "Token 1 ( açò): [0.5020, 0.3195, 0.5049, 0.0786, 0.4326, ..., -0.3988, -1.4212, 0.6783, 0.6853, 0.0632]  (128 dims)\n",
            "Token 2 (  es): [-0.5593, -0.2221, 0.0937, -0.0044, 0.4904, ..., -0.4586, -1.3943, 1.4635, 0.1738, -1.7180]  (128 dims)\n",
            "Token 3 (  or): [-0.4300, -0.7631, 0.3384, 0.3612, 0.4029, ..., -0.2063, -3.4965, 0.7733, 0.8742, 2.8823]  (128 dims)\n",
            "Token 4 ( ...): [-0.3847, 0.1615, 0.3196, -0.2129, 0.4946, ..., 0.4709, -1.7521, 0.9540, -0.2614, -2.1374]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5317, 0.1277, 0.2778, 0.0549, -0.3009, ..., 1.0020, -0.5405, -1.2072, -0.0631, 1.1323]  (128 dims)\n",
            "Token 1 ( açò): [-0.1116, -0.1610, -0.2161, -0.5303, -0.1058, ..., 0.0734, 2.3083, -0.3129, 1.0310, -0.5156]  (128 dims)\n",
            "Token 2 (  es): [-0.0617, -0.0618, 0.2237, -0.0246, -0.1845, ..., -0.3463, 0.4511, -0.5506, 0.8103, -0.5110]  (128 dims)\n",
            "Token 3 (  or): [0.3560, 0.2053, 0.3912, -0.0168, -0.2417, ..., -0.5512, 1.8206, -1.8389, 1.3271, -0.4147]  (128 dims)\n",
            "Token 4 ( ...): [-0.4183, 0.5391, 0.8800, -0.2418, -0.7980, ..., -0.4308, 0.2192, -1.4246, 1.4308, -0.1592]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0247, -0.3167, -0.0929, -0.2560, -0.1773, ..., -0.7128, -2.8245, 1.3088, -1.4605, 1.0619]  (128 dims)\n",
            "Token 1 ( açò): [-0.5393, -0.5521, -0.2546, 0.1574, -0.4869, ..., 0.3211, -0.3033, -0.0859, 0.6987, 0.7965]  (128 dims)\n",
            "Token 2 (  es): [0.5168, -2.8425, 0.1645, 1.6784, -0.3013, ..., 0.9849, -1.9401, -0.1742, 1.4260, 0.3956]  (128 dims)\n",
            "Token 3 (  or): [0.2009, -1.2342, -0.1961, 0.1410, -0.0014, ..., -0.4536, -1.2703, 1.0357, 1.5117, -0.1427]  (128 dims)\n",
            "Token 4 ( ...): [1.0174, -1.0900, -0.9469, 0.5173, 0.1574, ..., -0.9358, -0.2994, -1.7006, 0.8744, -0.1210]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1430, -0.2671, 0.2044, -0.0357, 0.1421, ..., -0.2843, 0.7301, 0.2891, 0.4284, 0.0749]  (128 dims)\n",
            "Token 1 ( açò): [0.0570, -0.0680, -0.0996, 0.0832, 0.0190, ..., -0.2270, 0.8001, 0.7047, 0.8373, 0.0580]  (128 dims)\n",
            "Token 2 (  es): [0.2984, -0.1189, 0.5522, 0.1831, 0.1569, ..., -0.0490, -1.0709, -0.1080, 1.4075, -0.8779]  (128 dims)\n",
            "Token 3 (  or): [0.0346, 0.1319, 0.3926, 1.0105, 0.0102, ..., 0.4863, -0.8422, -0.8399, 0.5786, -0.5454]  (128 dims)\n",
            "Token 4 ( ...): [-0.1579, -0.2460, 0.1591, 0.3214, 0.4382, ..., 0.5315, -1.4688, -1.2476, 1.9424, -0.0678]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8143, -1.2586, 0.4210, -1.8082, -2.6416, ..., -0.3086, 0.7203, 0.4455, 0.2583, -0.7831]  (128 dims)\n",
            "Token 1 ( açò): [0.1294, -0.4049, -0.1552, -0.2182, -0.2793, ..., -0.8341, 0.1756, -0.5772, 0.7945, -0.8796]  (128 dims)\n",
            "Token 2 (  es): [0.3906, -0.7396, -0.3970, -0.7586, -0.1186, ..., -1.1033, -0.4366, -0.2125, -0.3350, -0.0030]  (128 dims)\n",
            "Token 3 (  or): [0.6782, -1.3729, -0.2564, -0.6724, -1.1919, ..., -1.3765, -0.4069, 0.3345, -0.2209, 0.1056]  (128 dims)\n",
            "Token 4 ( ...): [0.4699, -1.7554, 0.6140, -2.3504, -2.5079, ..., -0.0787, 0.4214, 0.1861, 0.3391, -1.8276]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1141, 0.0147, 0.2306, 0.0312, -0.1102, ..., -0.4191, 1.2161, 0.5313, 0.9569, -0.1863]  (128 dims)\n",
            "Token 1 ( açò): [0.0982, 0.2287, 0.0556, -0.0174, -0.0156, ..., 0.5968, 0.8045, 1.2447, 2.1893, 0.4082]  (128 dims)\n",
            "Token 2 (  es): [-0.2486, 0.1484, -0.9535, -0.0814, 0.2539, ..., 1.2225, -0.5069, 1.8091, 1.9420, 1.8352]  (128 dims)\n",
            "Token 3 (  or): [-0.1046, 0.0773, -0.8490, 0.4064, 0.1344, ..., 1.0011, -0.7570, 1.6131, 1.2046, 2.4530]  (128 dims)\n",
            "Token 4 ( ...): [0.1643, -0.0076, -0.3240, -0.1524, -0.2411, ..., 0.5901, 1.1316, 0.9230, 1.0872, 2.2424]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.2609, -0.2117, 0.4495, 0.1503, 0.2674, ..., -0.4155, -0.3622, -0.2970, -0.0813, -0.1502]  (128 dims)\n",
            "Token 1 ( açò): [-0.3115, 0.4735, -0.1662, -0.2302, -0.3165, ..., -0.2556, -0.0115, 0.1955, 0.1563, 1.6540]  (128 dims)\n",
            "Token 2 (  es): [-0.2406, -0.2076, 0.1203, 0.0308, 0.2538, ..., 0.0764, -0.0890, 0.6060, -0.4239, 2.1300]  (128 dims)\n",
            "Token 3 (  or): [0.0227, -0.3146, 0.4548, 0.4912, 0.3396, ..., -2.0140, 0.0448, 0.3925, 0.4561, 1.5923]  (128 dims)\n",
            "Token 4 ( ...): [0.3774, -0.3489, 0.5864, 0.4881, -0.1655, ..., -1.1276, 0.4838, 0.9544, 0.3956, 1.3253]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1495, 0.2181, -0.2070, -0.1809, -0.1314, ..., 1.1907, -2.8027, -0.6817, 0.5220, -0.8140]  (128 dims)\n",
            "Token 1 ( açò): [0.6259, 0.6378, -0.6406, -0.9497, -0.4243, ..., 1.5754, -2.6734, -0.8839, 0.7212, -1.0017]  (128 dims)\n",
            "Token 2 (  es): [0.2544, 0.4739, -0.7211, -0.5653, -0.0795, ..., 0.6482, -0.5643, 0.5732, -0.7838, -0.5688]  (128 dims)\n",
            "Token 3 (  or): [0.3554, 0.3896, -0.8326, -0.7789, -0.2960, ..., -0.1536, -2.8558, -0.0678, -0.3706, -0.7340]  (128 dims)\n",
            "Token 4 ( ...): [-0.1168, 0.5561, -1.9653, -1.7089, -1.6725, ..., -0.1022, -1.8980, 0.3079, -0.5268, -0.6559]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4624, -0.1438, 0.5297, 0.3217, 0.5069, ..., -0.7191, 0.7023, 0.2106, 0.3088, 0.4109]  (128 dims)\n",
            "Token 1 ( açò): [-0.6180, -0.2305, 0.9478, 0.3609, 1.0403, ..., 0.1799, -0.2022, -0.4271, -0.9021, -0.8956]  (128 dims)\n",
            "Token 2 (  es): [0.8320, 0.6585, -0.2488, -0.2683, -0.3660, ..., -2.0417, 2.2301, 0.2474, 1.2097, 1.9695]  (128 dims)\n",
            "Token 3 (  or): [0.6514, 0.3549, -0.0753, -0.6041, -0.1810, ..., -2.0306, 2.0648, 0.3157, 1.2851, 1.7259]  (128 dims)\n",
            "Token 4 ( ...): [1.0701, 0.6271, -0.2265, -0.5105, -0.0938, ..., -1.4626, 1.4446, -0.5596, 0.1495, 0.6551]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2740, -0.0506, 0.4008, -0.1798, 0.0627, ..., 1.1809, -0.2412, -0.8631, -0.6208, 0.6555]  (128 dims)\n",
            "Token 1 ( açò): [0.2260, -0.1917, -0.0855, -0.2215, 0.0417, ..., -0.2112, -0.6327, 0.8493, 0.3805, -0.4114]  (128 dims)\n",
            "Token 2 (  es): [0.3609, -0.4110, -0.0882, -0.3340, -0.2078, ..., 1.0530, -0.8593, -1.1391, 1.0772, -1.0269]  (128 dims)\n",
            "Token 3 (  or): [-0.0013, -0.2575, -0.0742, 0.3113, -0.4680, ..., 0.3529, -1.6033, -0.4635, 0.3484, -0.4002]  (128 dims)\n",
            "Token 4 ( ...): [0.2323, -0.0319, 0.1919, -0.3285, -0.0295, ..., 1.9947, -1.7368, -1.2364, 0.6802, -0.6213]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8710, -1.0364, -0.1889, -0.3356, 0.0606, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (128 dims)\n",
            "Token 1 ( açò): [-0.1316, -0.0911, 0.2151, 0.1788, 0.1853, ..., -0.0361, -0.6353, -0.0747, 0.4616, 0.5062]  (128 dims)\n",
            "Token 2 (  es): [0.1118, 0.2098, 0.1478, 0.0366, -0.2164, ..., -1.6739, -0.4848, -0.7525, -0.1444, 0.1011]  (128 dims)\n",
            "Token 3 (  or): [0.1963, -0.0227, 0.2032, 0.2057, -0.0620, ..., -2.2730, -0.1709, -0.7411, -0.2895, -0.0452]  (128 dims)\n",
            "Token 4 ( ...): [-0.2255, 0.1096, -0.1719, -0.2426, 0.1503, ..., -1.8034, -0.8704, -1.1216, -1.0419, -0.5522]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "y para V:"
      ],
      "metadata": {
        "id": "dHrAUw9gd8ZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"V después del reshape: {V_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = V_heads[head_idx, token_idx, :]  # (128,)\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Letc7Biqd90t",
        "outputId": "8164c116-4bc0-4ab7-9c46-d7b8721c146f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "V después del reshape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0101, 0.0171, 0.0520, 0.0119, -0.0049]  (128 dims)\n",
            "Token 1 ( açò): [-0.0194, 0.0367, 0.0695, 0.0036, 0.0111, ..., 0.0025, -0.0039, -0.0263, -0.0170, -0.0196]  (128 dims)\n",
            "Token 2 (  es): [0.0151, 0.0223, -0.0127, 0.0137, 0.0391, ..., -0.0199, -0.0072, -0.0610, -0.0246, -0.0376]  (128 dims)\n",
            "Token 3 (  or): [0.0186, 0.0353, -0.0046, -0.0044, 0.0186, ..., 0.0247, 0.0039, -0.0278, -0.0000, -0.0036]  (128 dims)\n",
            "Token 4 ( ...): [-0.0066, 0.0324, 0.0575, -0.0022, 0.0066, ..., 0.0131, 0.0214, -0.0935, -0.0029, -0.0114]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0012, 0.1335, -0.0075, -0.0087, -0.0006, ..., 0.0052, -0.0055, 0.0015, 0.0052, 0.0133]  (128 dims)\n",
            "Token 1 ( açò): [0.0426, 0.0277, -0.0045, -0.0126, -0.0293, ..., 0.0589, -0.0051, 0.0251, 0.0028, -0.0148]  (128 dims)\n",
            "Token 2 (  es): [-0.0094, 0.0100, 0.0084, -0.0117, 0.0130, ..., -0.0241, -0.0061, -0.0019, 0.0277, 0.0257]  (128 dims)\n",
            "Token 3 (  or): [-0.0295, 0.0242, 0.0070, -0.0241, 0.0527, ..., -0.0092, -0.0038, 0.0096, 0.0276, 0.0201]  (128 dims)\n",
            "Token 4 ( ...): [-0.0058, -0.0625, -0.0261, -0.0466, 0.0628, ..., 0.0187, -0.0110, 0.0326, -0.0018, 0.0205]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0236, 0.0308, 0.0247, 0.0003, -0.0147, ..., -0.0319, -0.0092, -0.0265, -0.0062, 0.0050]  (128 dims)\n",
            "Token 1 ( açò): [-0.0027, -0.0061, -0.0405, 0.0461, -0.0422, ..., -0.0023, 0.0749, -0.0437, 0.0127, -0.0384]  (128 dims)\n",
            "Token 2 (  es): [0.0366, 0.0692, -0.0059, -0.0177, 0.0376, ..., 0.0167, -0.0588, 0.0036, 0.0036, 0.0340]  (128 dims)\n",
            "Token 3 (  or): [-0.0004, 0.0122, -0.0074, -0.0076, 0.0054, ..., 0.0240, -0.0061, -0.0007, -0.0063, -0.0144]  (128 dims)\n",
            "Token 4 ( ...): [0.0005, 0.0141, -0.0053, -0.0012, 0.0151, ..., 0.0161, -0.0032, 0.0260, 0.0005, 0.0045]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, ..., 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]  (128 dims)\n",
            "Token 1 ( açò): [0.0124, 0.0801, -0.0509, -0.0666, -0.1353, ..., 0.1175, -0.0220, 0.0734, 0.0029, 0.0229]  (128 dims)\n",
            "Token 2 (  es): [0.0061, -0.0187, 0.0600, -0.0887, 0.0140, ..., 0.0280, -0.0018, -0.0147, 0.0127, -0.0417]  (128 dims)\n",
            "Token 3 (  or): [-0.0441, -0.0376, -0.0135, 0.0945, 0.0198, ..., 0.0044, -0.0414, 0.1110, -0.0328, 0.0965]  (128 dims)\n",
            "Token 4 ( ...): [-0.0306, 0.1822, 0.0339, 0.0769, 0.0520, ..., -0.1586, 0.0068, 0.1036, 0.0005, -0.0832]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0221, -0.0290, 0.0401, 0.0479, 0.0276, ..., -0.0478, 0.0607, 0.0734, -0.0399, -0.0458]  (128 dims)\n",
            "Token 1 ( açò): [0.0096, 0.0368, 0.0156, 0.0479, -0.0254, ..., 0.2872, -0.0892, 0.0060, 0.0457, -0.0251]  (128 dims)\n",
            "Token 2 (  es): [0.0231, -0.0055, 0.0283, -0.0364, 0.0028, ..., 0.1219, 0.0114, -0.0420, -0.0282, -0.0114]  (128 dims)\n",
            "Token 3 (  or): [0.0437, -0.0278, 0.0092, -0.0403, 0.0063, ..., 0.1447, 0.0263, -0.0003, 0.0165, -0.0184]  (128 dims)\n",
            "Token 4 ( ...): [-0.0476, 0.0527, -0.0094, -0.0297, 0.0155, ..., -0.0083, -0.0393, -0.0186, 0.0095, 0.0267]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0429, -0.0179, -0.0109, 0.0065, -0.0191, ..., 0.0514, -0.0193, -0.0056, 0.0338, -0.0442]  (128 dims)\n",
            "Token 1 ( açò): [-0.0294, 0.0166, 0.0486, 0.0059, 0.0568, ..., -0.0196, 0.0131, -0.0123, 0.0156, -0.0071]  (128 dims)\n",
            "Token 2 (  es): [-0.0114, 0.0198, 0.0339, 0.0035, -0.0074, ..., -0.0129, 0.0233, -0.0073, -0.0093, -0.0188]  (128 dims)\n",
            "Token 3 (  or): [0.0054, 0.0167, 0.0031, 0.0001, 0.0260, ..., -0.0164, 0.0192, 0.0012, 0.0150, -0.0149]  (128 dims)\n",
            "Token 4 ( ...): [0.0368, -0.0021, 0.0069, -0.0106, 0.0356, ..., -0.0075, -0.0344, 0.0321, -0.0099, -0.0200]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0137, 0.0080, -0.0237, 0.0203, 0.0457, ..., -0.0295, -0.0021, -0.0045, -0.0225, 0.0009]  (128 dims)\n",
            "Token 1 ( açò): [0.0214, -0.0035, 0.0208, 0.0165, -0.0303, ..., -0.0156, -0.0377, 0.0177, -0.0271, 0.0035]  (128 dims)\n",
            "Token 2 (  es): [0.0166, 0.0083, -0.0105, -0.0016, 0.0297, ..., 0.0168, -0.0145, 0.0046, -0.0102, -0.0135]  (128 dims)\n",
            "Token 3 (  or): [-0.0126, -0.0075, 0.0158, -0.0152, 0.0139, ..., -0.0061, -0.0067, 0.0099, -0.0027, 0.0025]  (128 dims)\n",
            "Token 4 ( ...): [-0.0277, 0.0269, 0.0029, 0.0682, 0.0398, ..., -0.0310, -0.0324, -0.0141, -0.0079, -0.0239]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0055, 0.0139, -0.0012, -0.0055, 0.0251, ..., -0.0057, -0.0133, 0.0034, 0.0156, -0.0053]  (128 dims)\n",
            "Token 1 ( açò): [0.0439, -0.0256, -0.0133, 0.2030, 0.0612, ..., 0.0385, 0.0356, 0.0546, 0.0192, 0.0171]  (128 dims)\n",
            "Token 2 (  es): [-0.0409, -0.0017, -0.0221, -0.0363, 0.0533, ..., -0.0140, 0.0627, 0.0329, -0.0070, 0.0292]  (128 dims)\n",
            "Token 3 (  or): [-0.0369, -0.0000, -0.0337, 0.0020, 0.0267, ..., -0.0130, 0.0426, -0.0443, 0.0010, 0.0367]  (128 dims)\n",
            "Token 4 ( ...): [0.0719, 0.0152, 0.0941, -0.2367, -0.0310, ..., -0.0138, -0.2049, 0.0382, 0.0516, 0.0021]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0426, -0.0096, -0.0343, 0.0244, -0.0478, ..., 0.0463, -0.0073, -0.0316, 0.0095, 0.0244]  (128 dims)\n",
            "Token 1 ( açò): [0.0009, -0.0040, 0.0368, -0.0228, 0.0197, ..., 0.0210, -0.0071, -0.0095, -0.0007, 0.0064]  (128 dims)\n",
            "Token 2 (  es): [0.0050, 0.0272, -0.0084, 0.0115, 0.0296, ..., -0.0079, -0.0025, 0.0208, -0.0253, 0.0013]  (128 dims)\n",
            "Token 3 (  or): [-0.0353, 0.0153, -0.0183, 0.0017, 0.0001, ..., 0.0235, 0.0279, 0.0039, -0.0091, -0.0165]  (128 dims)\n",
            "Token 4 ( ...): [-0.0212, 0.0092, 0.0090, 0.0367, -0.0870, ..., -0.0468, 0.0262, -0.0201, 0.0186, -0.0431]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0082, 0.0044, -0.0388, 0.0688, -0.0155, ..., -0.0285, 0.0363, -0.0448, 0.0119, -0.0096]  (128 dims)\n",
            "Token 1 ( açò): [-0.0056, 0.0459, 0.0434, -0.0685, -0.0240, ..., 0.0780, 0.0057, -0.0755, -0.0033, 0.0095]  (128 dims)\n",
            "Token 2 (  es): [-0.0282, -0.0311, -0.0211, 0.0410, 0.0116, ..., -0.0050, -0.0147, 0.0178, -0.0063, -0.0120]  (128 dims)\n",
            "Token 3 (  or): [-0.0517, -0.0338, 0.0242, -0.0501, 0.0267, ..., -0.0091, 0.0373, 0.0824, 0.0130, 0.0101]  (128 dims)\n",
            "Token 4 ( ...): [-0.0351, 0.0232, -0.0426, -0.0012, -0.0061, ..., 0.0995, 0.0101, -0.1281, 0.0316, 0.0101]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, ..., -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]  (128 dims)\n",
            "Token 1 ( açò): [-0.0541, -0.0032, 0.0071, -0.0467, 0.0658, ..., 0.0618, 0.0090, 0.0202, -0.0576, 0.0039]  (128 dims)\n",
            "Token 2 (  es): [0.0586, -0.0087, 0.0153, -0.0023, -0.0177, ..., 0.0125, 0.0367, -0.0037, -0.0138, 0.0284]  (128 dims)\n",
            "Token 3 (  or): [0.0310, -0.0210, 0.0042, 0.0028, -0.0091, ..., 0.0202, 0.0172, 0.0222, -0.0011, 0.0154]  (128 dims)\n",
            "Token 4 ( ...): [0.1293, -0.0294, 0.0033, 0.1233, -0.0271, ..., 0.0506, -0.0043, 0.0095, -0.0196, -0.0010]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0407, 0.0040, -0.0497, -0.0331, 0.0371, ..., -0.0325, 0.0308, -0.0027, 0.0419, 0.0007]  (128 dims)\n",
            "Token 1 ( açò): [0.0029, -0.0077, -0.0035, 0.0170, 0.0249, ..., 0.0330, 0.0383, -0.0220, 0.0484, 0.0010]  (128 dims)\n",
            "Token 2 (  es): [-0.0018, -0.0358, 0.0079, 0.0293, 0.0126, ..., 0.0183, 0.0110, -0.0291, 0.0352, 0.0065]  (128 dims)\n",
            "Token 3 (  or): [0.0062, -0.0349, -0.0165, 0.0050, -0.0084, ..., -0.0046, -0.0042, 0.0011, 0.0251, -0.0360]  (128 dims)\n",
            "Token 4 ( ...): [-0.0380, -0.0100, 0.0140, 0.0530, -0.0513, ..., -0.0103, -0.0077, 0.0163, -0.0063, -0.0085]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0182, 0.0278, -0.0238, 0.0184, -0.0484, ..., -0.0921, 0.0130, -0.0359, 0.0489, -0.0099]  (128 dims)\n",
            "Token 1 ( açò): [-0.0119, -0.0057, -0.1055, -0.0029, 0.0033, ..., -0.0280, -0.0004, -0.0273, -0.0478, -0.0256]  (128 dims)\n",
            "Token 2 (  es): [-0.0196, -0.0070, -0.0155, 0.0050, -0.0083, ..., -0.0304, 0.0145, -0.0046, 0.0535, -0.0190]  (128 dims)\n",
            "Token 3 (  or): [0.0039, 0.0184, 0.0117, 0.0163, 0.0446, ..., 0.0288, -0.0302, 0.0397, -0.0756, 0.0071]  (128 dims)\n",
            "Token 4 ( ...): [0.0231, -0.0227, -0.0038, 0.0707, 0.0955, ..., 0.0040, -0.0115, -0.0261, 0.0004, 0.0255]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0174, -0.1290, -0.0275, 0.0390, -0.0243, ..., 0.0371, -0.0154, -0.0116, 0.0631, -0.0049]  (128 dims)\n",
            "Token 1 ( açò): [-0.0758, 0.0054, 0.0508, 0.0180, 0.0769, ..., 0.0187, 0.0278, -0.0476, 0.0130, -0.1136]  (128 dims)\n",
            "Token 2 (  es): [-0.0312, -0.0069, -0.0010, 0.0012, -0.0187, ..., -0.0060, -0.0030, -0.0247, -0.0279, -0.0743]  (128 dims)\n",
            "Token 3 (  or): [0.0037, 0.0041, 0.0107, -0.0344, 0.0404, ..., 0.0165, -0.0253, 0.0303, -0.0198, -0.0638]  (128 dims)\n",
            "Token 4 ( ...): [-0.0218, 0.0483, 0.0183, -0.0697, 0.0166, ..., 0.0565, -0.0097, 0.0232, 0.0310, -0.1542]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0047, -0.0257, 0.0017, -0.0183, 0.0019, ..., -0.0218, -0.0255, -0.0075, 0.0050, -0.0041]  (128 dims)\n",
            "Token 1 ( açò): [-0.0450, -0.0349, -0.0102, -0.0333, 0.0143, ..., -0.0554, 0.0109, -0.0386, -0.0383, -0.0504]  (128 dims)\n",
            "Token 2 (  es): [-0.0329, 0.0189, -0.0070, 0.0014, -0.0092, ..., 0.0283, -0.0124, -0.0035, 0.0228, -0.0141]  (128 dims)\n",
            "Token 3 (  or): [0.0076, 0.0523, -0.0147, 0.0218, 0.0518, ..., -0.0375, 0.0235, 0.0123, 0.0201, -0.0323]  (128 dims)\n",
            "Token 4 ( ...): [-0.0232, 0.0332, -0.0303, 0.0170, 0.0164, ..., -0.0340, 0.0043, 0.0434, 0.0100, 0.0008]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0001, 0.0307, 0.0271, -0.0246, -0.0394, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (128 dims)\n",
            "Token 1 ( açò): [-0.0465, 0.0419, 0.0015, 0.0234, 0.0394, ..., -0.0239, -0.0067, -0.0332, 0.0050, -0.0243]  (128 dims)\n",
            "Token 2 (  es): [-0.0306, -0.0032, 0.0080, -0.0363, 0.0078, ..., -0.0464, -0.0095, -0.0084, -0.0192, 0.0126]  (128 dims)\n",
            "Token 3 (  or): [-0.0078, 0.0034, 0.0153, -0.0016, -0.0157, ..., -0.0092, -0.0029, -0.0215, -0.0029, 0.0147]  (128 dims)\n",
            "Token 4 ( ...): [-0.0578, 0.0029, 0.0008, -0.0237, 0.0056, ..., 0.0185, -0.0176, 0.0189, -0.0021, 0.0139]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transponemos K para poder hacer la multiplicación matricial Q @ K^T.\n",
        "\n",
        "Queremos comparar cada query (128 dims) con cada key (128 dims) mediante producto escalar. Al transponer K de (16, 5, 128) a (16, 128, 5), la multiplicación Q @ K^T nos da una matriz (16, 5, 5) donde cada posición (i,j) es el score de cuánto el token i atiende al token j.\n",
        "\n",
        "De esta manera tenemos 16 matrices 5x5"
      ],
      "metadata": {
        "id": "FVRFCB9MuTSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_vec.shape"
      ],
      "metadata": {
        "id": "LT4TPoZrDbAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4403e31f-6100-431e-a231-bc7c51b65ae3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([128])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicar RoPE a Q y K"
      ],
      "metadata": {
        "id": "2NRw1mJGUIdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de implementar la máscara aplicamos un positional encoding. En esta arquitectura se usa RoPE (Rotatory Positional Encoding) que usa una fórmula basada en seno y coseno para aplicar una rotación a cada vector de la matriz. Sólo se aplica a Q y a K. Inspeccionamos el método forward para confirmar que se aplica RoPE y en qué momento."
      ],
      "metadata": {
        "id": "6okZMCSAXpCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "attention_layer = model.model.layers[0].self_attn\n",
        "print(inspect.getsource(attention_layer.forward))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj6ZF3dHZ1lR",
        "outputId": "e2031606-562e-430f-b233-c34bd2242e26"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n",
            "    def forward(\n",
            "        self,\n",
            "        hidden_states: torch.Tensor,\n",
            "        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n",
            "        attention_mask: Optional[torch.Tensor],\n",
            "        past_key_values: Optional[Cache] = None,\n",
            "        cache_position: Optional[torch.LongTensor] = None,\n",
            "        **kwargs: Unpack[TransformersKwargs],\n",
            "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
            "        input_shape = hidden_states.shape[:-1]\n",
            "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
            "\n",
            "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
            "\n",
            "        cos, sin = position_embeddings\n",
            "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
            "\n",
            "        if past_key_values is not None:\n",
            "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
            "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
            "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
            "\n",
            "        attention_interface: Callable = eager_attention_forward\n",
            "        if self.config._attn_implementation != \"eager\":\n",
            "            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
            "\n",
            "        attn_output, attn_weights = attention_interface(\n",
            "            self,\n",
            "            query_states,\n",
            "            key_states,\n",
            "            value_states,\n",
            "            attention_mask,\n",
            "            dropout=0.0 if not self.training else self.attention_dropout,\n",
            "            scaling=self.scaling,\n",
            "            **kwargs,\n",
            "        )\n",
            "\n",
            "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
            "        attn_output = self.o_proj(attn_output)\n",
            "        return attn_output, attn_weights\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n",
        "\n",
        "Q_antes_rope = Q_reshaped.clone()\n",
        "K_antes_rope = K_reshaped.clone()\n",
        "\n",
        "print(\"Dimensiones ANTES de RoPE:\")\n",
        "print(f\"Shape: {Q_antes_rope.shape}\")  # (5, 16, 128)\n",
        "\n",
        "\n",
        "Q_transposed = Q_reshaped.unsqueeze(0).transpose(1, 2)  # (1, 16, 5, 128)\n",
        "K_transposed = K_reshaped.unsqueeze(0).transpose(1, 2)\n",
        "\n",
        "positions_ids = torch.arange(5, device=Q_reshaped.device).unsqueeze(0)\n",
        "\n",
        "cos_sin_cache = model.model.rotary_emb(K_transposed, positions_ids)\n",
        "\n",
        "Q_rope, K_rope = apply_rotary_pos_emb(Q_transposed, K_transposed, *cos_sin_cache)\n",
        "\n",
        "Q_rope_reshaped = Q_rope.squeeze(0).transpose(0, 1)  # (5, 16, 128)\n",
        "K_rope_reshaped = K_rope.squeeze(0).transpose(0, 1)\n",
        "\n",
        "print(\"\\nDimensiones DESPUÉS de RoPE:\")\n",
        "print(f\"Shape: {Q_reshaped.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYz3BA9IUxg5",
        "outputId": "e02fb732-3ebd-44be-a878-89d1782ac50d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones ANTES de RoPE:\n",
            "Shape: torch.Size([5, 16, 128])\n",
            "\n",
            "Dimensiones DESPUÉS de RoPE:\n",
            "Shape: torch.Size([5, 16, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q_heads = Q_rope_reshaped.transpose(0, 1)\n",
        "\n",
        "print(f\"Q después de RoPE shape: {Q_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = Q_heads[head_idx, token_idx, :]\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tiMjNm3beUS",
        "outputId": "a14142b0-72b8-4004-95dd-a5f8dc0f6dcd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q después de RoPE shape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.8511, 0.7741, -0.8034, 1.0649, -0.5152, ..., -2.6492, 1.4339, -1.5661, 1.1632, -0.4373]  (128 dims)\n",
            "Token 1 ( açò): [-1.7761, -1.3385, 0.8938, -0.6128, 0.4115, ..., -0.7553, 0.2729, -1.6076, 1.4796, 0.1335]  (128 dims)\n",
            "Token 2 (  es): [0.1317, -0.1957, 0.4669, -0.3632, 0.5207, ..., -1.1496, -2.2193, 1.5701, -1.4527, 1.1898]  (128 dims)\n",
            "Token 3 (  or): [0.3140, 0.1664, 0.2739, -0.3544, 0.3062, ..., -0.2388, -0.8528, 0.8558, -1.6380, 0.8494]  (128 dims)\n",
            "Token 4 ( ...): [0.2094, 0.3513, -0.3691, 0.5510, -0.1598, ..., -0.1742, -1.2643, -0.1162, -0.1085, 0.6010]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.0463, 0.0001, -0.1137, 0.2507, 0.3485, ..., 0.0239, 1.2501, 0.0091, -1.1293, 0.5304]  (128 dims)\n",
            "Token 1 ( açò): [0.0427, 0.0118, 0.1495, 0.5702, -0.0794, ..., -0.5869, 0.5339, -0.1435, -0.2585, -0.3561]  (128 dims)\n",
            "Token 2 (  es): [-0.0040, -0.1699, 0.3557, -0.4858, -0.5684, ..., -0.0592, 0.6325, 0.0925, -0.2680, -0.0040]  (128 dims)\n",
            "Token 3 (  or): [0.1373, 0.0828, 0.3265, -0.6355, -0.5290, ..., -0.2292, 0.6504, -0.1514, 0.2409, -0.1348]  (128 dims)\n",
            "Token 4 ( ...): [0.2942, 0.3504, 0.6447, -0.7177, -0.0687, ..., 0.3264, 1.4946, 0.4684, -0.1424, 0.3716]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1706, -0.0658, -0.0793, 0.2997, -0.3046, ..., 0.7790, -0.1864, -0.0850, 0.0032, -0.0692]  (128 dims)\n",
            "Token 1 ( açò): [0.0704, -0.1357, 0.3921, 0.0091, 0.1753, ..., 1.0374, 0.8097, -0.7264, 0.1974, -0.1934]  (128 dims)\n",
            "Token 2 (  es): [0.1055, -0.8084, 0.4365, 0.3805, -0.0061, ..., 2.0269, -0.5795, -0.3754, 0.1947, -0.2171]  (128 dims)\n",
            "Token 3 (  or): [0.2203, 0.1889, -0.3362, -0.2650, 0.1526, ..., 0.9304, -0.3546, -0.2495, -0.1011, 0.1737]  (128 dims)\n",
            "Token 4 ( ...): [-0.1443, 0.0145, -0.1684, -0.5082, 0.5020, ..., 1.1980, 0.1567, -0.2919, 0.0381, 0.0329]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2368, 0.0941, 0.4004, -0.7508, -0.3325, ..., -2.3852, 2.1380, 1.3856, 1.8737, 1.7969]  (128 dims)\n",
            "Token 1 ( açò): [0.5956, 0.4570, -0.4761, 1.0860, 0.9890, ..., -2.3763, 2.1579, 1.1982, 1.8482, 1.7537]  (128 dims)\n",
            "Token 2 (  es): [1.0809, 0.1737, -0.4270, 1.2628, 0.9959, ..., -1.3645, 1.2235, 0.6115, 1.0115, 0.9353]  (128 dims)\n",
            "Token 3 (  or): [1.5716, 1.0128, -0.5454, 1.3509, -0.0262, ..., -1.4371, 1.2678, 0.6329, 1.0608, 0.9846]  (128 dims)\n",
            "Token 4 ( ...): [0.4602, 0.8053, -0.2909, 0.4646, 0.2952, ..., -1.5817, 1.4617, 0.9941, 1.2361, 1.1457]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7162, -0.2681, -0.7130, 0.0637, 0.1057, ..., -1.9417, 0.2538, 0.2032, -0.0186, 0.1673]  (128 dims)\n",
            "Token 1 ( açò): [-0.4656, 0.0231, -0.1858, -0.2550, 0.0352, ..., -1.3425, -0.1523, -0.1309, 0.2114, -0.0701]  (128 dims)\n",
            "Token 2 (  es): [0.1289, -0.5399, -0.4584, 0.1418, -0.1337, ..., -0.8291, 0.4546, 0.3649, -0.1664, 0.4051]  (128 dims)\n",
            "Token 3 (  or): [-0.3261, 0.0521, 0.0559, -0.1985, -0.0665, ..., -1.1989, 0.6014, 0.4376, -0.2632, 0.4944]  (128 dims)\n",
            "Token 4 ( ...): [-0.0413, -0.0980, -0.0052, -0.4299, -0.2804, ..., -1.1049, 0.1553, 0.0099, 0.1084, 0.0459]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0136, 0.2143, 0.0703, 0.2861, -0.2265, ..., -0.4979, -0.4998, -0.4526, 0.7031, 0.5648]  (128 dims)\n",
            "Token 1 ( açò): [-0.5658, 0.5893, 0.2643, -0.0389, -0.1818, ..., -0.2277, 0.1531, -0.3083, 0.8758, -0.0169]  (128 dims)\n",
            "Token 2 (  es): [-0.4581, -0.0239, 0.4948, -0.0316, 0.1622, ..., 0.3936, 0.3908, 0.0571, 0.9557, -0.5113]  (128 dims)\n",
            "Token 3 (  or): [-0.4525, -0.6596, 0.4596, -0.1392, 0.1793, ..., -0.0566, 0.2683, 0.2221, 0.4069, -0.0401]  (128 dims)\n",
            "Token 4 ( ...): [-0.0429, 0.0192, -0.2185, -0.7461, 0.5821, ..., 0.3132, 0.1897, -0.0788, 0.6763, -0.1910]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4356, 0.3842, 0.4366, -0.0392, -0.6289, ..., 0.3766, 0.0753, -0.0762, 0.2500, 0.2201]  (128 dims)\n",
            "Token 1 ( açò): [0.1684, -0.1363, 0.0872, -0.2135, -0.2655, ..., 0.2323, 0.5594, 0.0499, 0.3009, -0.1351]  (128 dims)\n",
            "Token 2 (  es): [0.8605, 0.6096, 0.2201, -0.3011, -0.1081, ..., 0.8336, 0.1072, -0.3835, -0.1300, 0.5709]  (128 dims)\n",
            "Token 3 (  or): [0.4299, -0.0627, -0.6815, -0.5573, 0.1731, ..., 0.5548, 0.0389, -0.0480, 0.0609, 0.4026]  (128 dims)\n",
            "Token 4 ( ...): [-0.1206, -0.7378, -0.4739, -0.6613, 0.4105, ..., 0.6875, -0.0733, -0.6527, -0.0179, 0.4923]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3952, -0.0236, -1.0263, -0.1699, -0.1340, ..., -0.0407, -0.4909, 0.6510, -0.6418, 0.1959]  (128 dims)\n",
            "Token 1 ( açò): [-0.6924, -0.7012, -0.5698, 1.0280, -0.7606, ..., -0.6449, -0.9232, 0.4157, -0.5968, 0.1692]  (128 dims)\n",
            "Token 2 (  es): [-0.8914, -0.3434, 0.1194, 0.3362, -2.1645, ..., -0.3021, -0.9052, 0.6343, -0.3716, 0.6218]  (128 dims)\n",
            "Token 3 (  or): [-0.0419, 0.3789, 1.1386, -0.3379, -1.2604, ..., -0.3805, -1.0183, 0.7628, -0.1057, 0.6479]  (128 dims)\n",
            "Token 4 ( ...): [0.1402, 0.3013, 1.8866, 0.2003, -0.5569, ..., 0.2710, -0.1359, 0.5077, 0.2176, 0.3281]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3521, -0.4142, 0.0211, -0.3359, -0.0088, ..., -0.1404, 0.3456, 0.1993, -0.1943, -0.0363]  (128 dims)\n",
            "Token 1 ( açò): [-0.4598, 0.6294, -0.2306, 0.0968, 0.0099, ..., -0.2342, 0.1049, 0.2734, -0.1038, -0.2153]  (128 dims)\n",
            "Token 2 (  es): [-0.2535, 0.8375, 0.2793, 0.5955, -0.0509, ..., -0.1783, -0.2319, 0.0280, -0.0634, -0.6011]  (128 dims)\n",
            "Token 3 (  or): [-0.1134, 0.0168, 0.4866, 0.6594, -0.5082, ..., 0.0426, -0.0709, -0.0715, -0.4822, -0.5342]  (128 dims)\n",
            "Token 4 ( ...): [-0.6560, -0.0983, -0.0161, 0.6661, -0.6865, ..., 0.4688, -0.4106, -0.3171, -0.9566, -0.7478]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5972, -0.9489, 0.6698, -0.5446, -0.0196, ..., 0.5141, -0.4475, 0.1800, -0.7482, -0.2691]  (128 dims)\n",
            "Token 1 ( açò): [0.2127, -0.2304, 0.1507, -0.5512, -0.6151, ..., -0.4145, -0.9011, 0.6050, -0.1837, 0.2826]  (128 dims)\n",
            "Token 2 (  es): [0.4912, 0.1979, -1.1627, -0.7679, -0.4500, ..., 0.4343, -0.0289, 0.5017, -0.2086, -0.0127]  (128 dims)\n",
            "Token 3 (  or): [0.9956, -0.7571, -0.7487, -0.3287, -0.8286, ..., 0.4023, 0.3909, 0.2510, -0.5378, 0.0453]  (128 dims)\n",
            "Token 4 ( ...): [-0.0506, 0.8090, -0.6447, -0.4299, -0.3881, ..., 0.0447, -0.2266, 1.5346, 0.7465, -0.3751]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.3898, 0.5608, -0.3478, -0.7937, -0.1874, ..., -6.1992, 0.7628, -0.9464, -2.9338, -1.4260]  (128 dims)\n",
            "Token 1 ( açò): [0.6617, 0.8825, -0.2612, 0.5629, -0.0906, ..., -4.9788, 0.5729, 0.2170, -0.8110, -1.6430]  (128 dims)\n",
            "Token 2 (  es): [0.3436, -0.5217, 0.1395, 0.2729, 0.9103, ..., -1.7630, 1.4711, 0.4260, 1.0417, 0.2490]  (128 dims)\n",
            "Token 3 (  or): [0.2547, -0.5113, 0.6322, 0.0461, 0.1949, ..., -2.2979, 1.8676, 0.0020, 0.7816, -0.0621]  (128 dims)\n",
            "Token 4 ( ...): [-0.4242, -0.0083, 0.3563, 0.0468, -0.3325, ..., -3.0515, 1.5584, -0.6611, -0.8877, -1.0170]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0608, 0.0167, -0.1336, -0.4315, 0.2209, ..., -0.0606, -0.7908, -0.1831, -0.5492, -0.3523]  (128 dims)\n",
            "Token 1 ( açò): [0.5721, -0.5225, 0.2781, 0.6381, 0.6156, ..., -0.0855, -0.3393, -0.4315, -0.3726, 0.0073]  (128 dims)\n",
            "Token 2 (  es): [0.5272, -1.0263, 0.6841, 0.8074, 1.1369, ..., 0.3267, -0.0853, -0.2759, -0.7708, 0.2867]  (128 dims)\n",
            "Token 3 (  or): [0.9435, -0.8300, 0.4123, 1.1153, 1.1493, ..., 0.1166, -0.3084, -0.2633, -0.3488, 0.0656]  (128 dims)\n",
            "Token 4 ( ...): [0.6168, 0.2131, -0.2654, 0.7652, 0.8081, ..., 0.7323, -0.2383, -0.6603, -0.5921, 0.5152]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0172, -0.2092, -0.3587, 0.3502, -0.2233, ..., 0.5501, -0.1378, -1.5415, 1.7725, 0.4384]  (128 dims)\n",
            "Token 1 ( açò): [0.5094, 0.7780, -0.6825, -0.1308, -0.4507, ..., 0.6030, 0.3410, -1.1413, 1.8245, 0.3757]  (128 dims)\n",
            "Token 2 (  es): [-0.0068, 0.8843, -0.0602, -0.6226, 0.5274, ..., -0.2086, 0.0455, -1.5586, 2.3095, 1.9919]  (128 dims)\n",
            "Token 3 (  or): [-0.7313, 0.0799, 0.4847, -0.3303, 0.6587, ..., 0.1158, 0.2915, -2.1804, 2.6721, 1.0850]  (128 dims)\n",
            "Token 4 ( ...): [-0.6549, -0.4765, 0.8604, -0.3211, 0.2338, ..., -0.3808, 0.7398, -3.0914, 3.3263, 1.0342]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.6629, 0.4068, -0.1787, -1.1420, -0.5572, ..., -0.5138, 0.3562, 1.0724, 0.4084, 0.2699]  (128 dims)\n",
            "Token 1 ( açò): [0.1513, -0.2381, -1.6466, -0.2240, -1.6521, ..., -0.4149, 0.2590, 0.6175, 0.4214, 0.2794]  (128 dims)\n",
            "Token 2 (  es): [0.2043, -0.8181, -1.9317, -0.1889, -1.2735, ..., -0.4163, 0.4184, 0.5707, 0.3694, 0.4126]  (128 dims)\n",
            "Token 3 (  or): [-1.8653, -1.4462, -1.1720, 1.0490, -1.4221, ..., -0.6988, 0.7188, 0.8275, 0.6979, 0.6886]  (128 dims)\n",
            "Token 4 ( ...): [-0.7624, -0.1609, 0.2455, 0.5485, 0.0419, ..., -0.3906, 0.3651, 0.8049, 0.2957, 0.3166]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1084, -0.6354, 0.1776, -0.3193, 0.2784, ..., -0.5032, 0.6485, 0.9932, -0.7111, 0.6745]  (128 dims)\n",
            "Token 1 ( açò): [0.2289, 0.2843, 0.4838, -0.4878, -0.3569, ..., -0.2286, 0.0734, 1.2004, -0.9589, 0.8342]  (128 dims)\n",
            "Token 2 (  es): [0.1933, 0.5065, 0.4266, -0.3545, -0.5683, ..., -0.5070, 0.3844, 0.5408, -0.9956, 0.9047]  (128 dims)\n",
            "Token 3 (  or): [-0.4895, 0.3432, -0.2220, 0.5731, -0.9055, ..., 0.2462, 0.3369, 0.0371, -0.6901, 0.6521]  (128 dims)\n",
            "Token 4 ( ...): [-0.3164, -0.3169, 0.0774, 0.0808, -0.6255, ..., 0.1095, 1.3590, 0.3640, -1.2799, 1.2918]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0436, 0.1843, -0.1697, 0.2867, -0.0651, ..., 0.1219, -0.0581, 0.9519, -0.0348, 0.9303]  (128 dims)\n",
            "Token 1 ( açò): [-0.2595, 0.0802, -0.2405, -0.3855, -0.0377, ..., 0.0733, -0.0682, 0.6090, 0.3939, 0.7309]  (128 dims)\n",
            "Token 2 (  es): [0.1520, -0.4288, -0.0207, -0.1040, -0.0648, ..., -0.1938, -0.4063, 0.4857, 0.9462, 0.8627]  (128 dims)\n",
            "Token 3 (  or): [0.1159, -0.3377, 0.1551, -0.6516, -0.3367, ..., -0.2481, -0.3822, 0.9398, 0.8782, 0.7316]  (128 dims)\n",
            "Token 4 ( ...): [0.1860, 0.0831, 0.3328, -0.8528, -0.3950, ..., -0.0021, -0.2759, 0.7701, 0.5679, 0.7159]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K_heads = K_rope_reshaped.transpose(0, 1)\n",
        "\n",
        "print(f\"Q después de RoPE shape: {K_heads.shape}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx} - Shape: (5, 128)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = K_heads[head_idx, token_idx, :]\n",
        "\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "        print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (128 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjZsGWl0dCh9",
        "outputId": "9e48e95e-514f-488e-e718-646e3fd8dc0c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q después de RoPE shape: torch.Size([16, 5, 128])\n",
            "================================================================================\n",
            "\n",
            "HEAD 0 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.3554, -0.4223, 0.2172, -0.7063, 0.2241, ..., 0.8779, 0.0834, -0.0709, 0.5904, -0.6143]  (128 dims)\n",
            "Token 1 ( açò): [0.1352, -0.0427, 0.1515, -0.5686, 0.2202, ..., 1.1067, -1.9492, -0.2812, 0.2005, -0.7695]  (128 dims)\n",
            "Token 2 (  es): [-1.1650, -1.4569, 1.2816, -1.2017, 0.5412, ..., 0.7107, -0.9561, 0.0267, 0.4596, 0.7334]  (128 dims)\n",
            "Token 3 (  or): [-0.2112, 0.1266, 0.3648, 0.1429, 0.2990, ..., 1.1964, -0.9724, 0.8349, 0.8404, 0.6139]  (128 dims)\n",
            "Token 4 ( ...): [0.5284, 1.6238, -1.0392, 2.5460, -2.4106, ..., 1.2185, -0.5807, -1.4425, 2.4332, -0.0166]  (128 dims)\n",
            "\n",
            "HEAD 1 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4833, -0.3272, 0.1366, -0.1066, -0.4100, ..., 1.3725, 1.4919, -0.0843, -1.8912, 0.7427]  (128 dims)\n",
            "Token 1 ( açò): [-0.4071, -0.9483, -0.0144, -0.5273, -0.2246, ..., -0.5024, 0.4553, 0.9582, -0.0559, 1.7710]  (128 dims)\n",
            "Token 2 (  es): [-0.0178, 0.0628, -0.1073, -0.0508, -0.3038, ..., 0.2201, 0.0134, 1.3837, -0.4277, 1.6438]  (128 dims)\n",
            "Token 3 (  or): [-0.1709, -0.0897, -0.1701, -0.3843, 0.2960, ..., 0.8818, -0.0973, 2.2482, 0.4261, 0.5858]  (128 dims)\n",
            "Token 4 ( ...): [0.2821, -0.3070, 0.5692, -0.1075, 0.0335, ..., 1.2751, 0.1795, 0.8942, -0.6271, 1.9434]  (128 dims)\n",
            "\n",
            "HEAD 2 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [1.6238, -2.1105, -0.3979, 0.6275, 0.1373, ..., 0.2037, 1.9504, -0.6677, 2.1918, -1.5709]  (128 dims)\n",
            "Token 1 ( açò): [0.2681, -0.9506, -0.0695, -0.4047, -0.0341, ..., -0.4253, -0.8897, 0.4161, -2.4312, 0.4107]  (128 dims)\n",
            "Token 2 (  es): [-0.9778, 0.0045, -0.9969, -0.8623, -0.4644, ..., -0.0836, 0.4264, 2.3705, 0.0056, -0.1877]  (128 dims)\n",
            "Token 3 (  or): [0.0849, -0.1735, 0.0828, 0.0432, 0.0200, ..., -0.6738, 0.5819, 0.2617, 0.0923, 1.8655]  (128 dims)\n",
            "Token 4 ( ...): [-0.4859, 0.3455, -0.4943, -0.8997, 0.3028, ..., -0.6929, 1.1894, 1.8942, 1.8007, 0.8317]  (128 dims)\n",
            "\n",
            "HEAD 3 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0515, 0.0201, -0.0272, 0.0114, 0.0579, ..., -2.8270, 2.8352, 2.2631, 2.7978, 2.7694]  (128 dims)\n",
            "Token 1 ( açò): [-0.5066, -0.4430, -0.3334, 0.3405, 0.8282, ..., -0.3245, 0.2401, -0.6898, 0.1074, 0.0461]  (128 dims)\n",
            "Token 2 (  es): [-0.1605, -0.2090, 0.1273, 0.5345, 0.2127, ..., -1.1495, 1.0436, 0.6896, 0.9713, 0.9600]  (128 dims)\n",
            "Token 3 (  or): [-0.1486, 0.0247, -0.7205, 0.5011, 0.2159, ..., -0.8657, 0.8485, 0.1484, 0.8098, 0.7895]  (128 dims)\n",
            "Token 4 ( ...): [0.3582, 0.2869, -0.2322, 0.7248, 0.0353, ..., -0.4252, 0.2691, -1.1599, 0.0938, 0.0379]  (128 dims)\n",
            "\n",
            "HEAD 4 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.1220, 0.3785, -0.1131, -0.5678, 0.6471, ..., 0.2490, 0.8485, 1.3179, -1.8046, 1.0987]  (128 dims)\n",
            "Token 1 ( açò): [-0.2677, 0.3490, -0.0829, -0.4258, 0.5350, ..., -1.1775, -0.5662, -1.1387, 1.0694, -0.8756]  (128 dims)\n",
            "Token 2 (  es): [-0.1622, 0.7186, -0.2128, -0.6423, 0.4046, ..., 1.3713, -0.8589, -0.4868, 0.1383, -1.2533]  (128 dims)\n",
            "Token 3 (  or): [-0.8172, 0.3901, 0.4299, -0.3735, 0.1314, ..., 1.2842, -1.0239, -0.7047, 0.7668, -1.5152]  (128 dims)\n",
            "Token 4 ( ...): [0.1500, -0.1691, 0.1159, -0.0331, 0.3129, ..., 0.8631, -1.7738, -1.0830, -0.0255, -1.9238]  (128 dims)\n",
            "\n",
            "HEAD 5 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.7487, 0.6855, 0.4504, 0.3863, -0.1420, ..., -2.3786, -0.9175, -1.4701, 0.3072, 0.4352]  (128 dims)\n",
            "Token 1 ( açò): [-0.0967, -0.3948, 0.4934, -0.1211, 0.3671, ..., -0.3988, -1.4212, 0.6783, 0.6853, 0.0632]  (128 dims)\n",
            "Token 2 (  es): [0.7013, 0.4052, -0.1027, -0.2346, 0.0487, ..., -0.4588, -1.3942, 1.4634, 0.1741, -1.7180]  (128 dims)\n",
            "Token 3 (  or): [0.4842, 1.1047, -0.8301, -0.1756, -0.6533, ..., -0.2078, -3.4968, 0.7725, 0.8735, 2.8822]  (128 dims)\n",
            "Token 4 ( ...): [0.1588, -0.1555, -0.3019, 0.0060, 0.0167, ..., 0.4696, -1.7523, 0.9540, -0.2606, -2.1376]  (128 dims)\n",
            "\n",
            "HEAD 6 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.5317, 0.1277, 0.2778, 0.0549, -0.3009, ..., 1.0020, -0.5405, -1.2072, -0.0631, 1.1323]  (128 dims)\n",
            "Token 1 ( açò): [-0.0176, -0.1216, -0.2205, -0.3061, -0.0291, ..., 0.0736, 2.3084, -0.3130, 1.0310, -0.5155]  (128 dims)\n",
            "Token 2 (  es): [0.1442, -0.0280, -0.4454, -0.4133, 0.1163, ..., -0.3456, 0.4514, -0.5508, 0.8105, -0.5107]  (128 dims)\n",
            "Token 3 (  or): [-0.2483, -0.1132, -0.6913, -0.3900, 0.2351, ..., -0.5504, 1.8211, -1.8392, 1.3272, -0.4147]  (128 dims)\n",
            "Token 4 ( ...): [-0.4238, -0.4713, -0.9206, 0.0635, 0.5957, ..., -0.4298, 0.2195, -1.4249, 1.4315, -0.1589]  (128 dims)\n",
            "\n",
            "HEAD 7 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.0247, -0.3167, -0.0929, -0.2560, -0.1773, ..., -0.7128, -2.8245, 1.3088, -1.4605, 1.0619]  (128 dims)\n",
            "Token 1 ( açò): [-0.1354, 0.1895, 0.2141, -0.4884, -0.3765, ..., 0.3208, -0.3034, -0.0857, 0.6985, 0.7967]  (128 dims)\n",
            "Token 2 (  es): [-0.7735, 0.3271, 0.3506, 0.0005, -0.4256, ..., 0.9841, -1.9406, -0.1741, 1.4258, 0.3958]  (128 dims)\n",
            "Token 3 (  or): [-0.3140, 0.9592, 0.0036, -0.1993, -0.1232, ..., -0.4538, -1.2706, 1.0367, 1.5112, -0.1430]  (128 dims)\n",
            "Token 4 ( ...): [-0.2461, 1.0915, 0.9392, -0.6029, -0.7786, ..., -0.9384, -0.3023, -1.7003, 0.8737, -0.1203]  (128 dims)\n",
            "\n",
            "HEAD 8 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1430, -0.2671, 0.2044, -0.0357, 0.1421, ..., -0.2843, 0.7301, 0.2891, 0.4284, 0.0749]  (128 dims)\n",
            "Token 1 ( açò): [0.0471, -0.0514, -0.2427, 0.1386, 0.0714, ..., -0.2269, 0.8001, 0.7047, 0.8373, 0.0580]  (128 dims)\n",
            "Token 2 (  es): [-0.1391, -0.2318, 0.0349, 0.0476, 0.2386, ..., -0.0492, -1.0711, -0.1081, 1.4077, -0.8777]  (128 dims)\n",
            "Token 3 (  or): [-0.0802, -0.2441, -0.3490, -0.4342, 0.3454, ..., 0.4856, -0.8423, -0.8401, 0.5785, -0.5454]  (128 dims)\n",
            "Token 4 ( ...): [0.1955, 0.2564, -0.1658, -0.1062, -0.0275, ..., 0.5309, -1.4692, -1.2478, 1.9430, -0.0673]  (128 dims)\n",
            "\n",
            "HEAD 9 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8143, -1.2586, 0.4210, -1.8082, -2.6416, ..., -0.3086, 0.7203, 0.4455, 0.2583, -0.7831]  (128 dims)\n",
            "Token 1 ( açò): [0.4476, -0.3554, -0.5168, -0.5509, 0.0196, ..., -0.8339, 0.1754, -0.5772, 0.7944, -0.8796]  (128 dims)\n",
            "Token 2 (  es): [0.4893, 0.8628, -0.4419, 0.3890, 0.3071, ..., -1.1030, -0.4363, -0.2121, -0.3347, -0.0030]  (128 dims)\n",
            "Token 3 (  or): [-0.4971, 1.2825, -1.1570, 0.4334, 0.2676, ..., -1.3754, -0.4064, 0.3352, -0.2206, 0.1054]  (128 dims)\n",
            "Token 4 ( ...): [-1.8373, 1.3927, -1.0386, 2.1238, 3.0756, ..., -0.0786, 0.4207, 0.1859, 0.3381, -1.8288]  (128 dims)\n",
            "\n",
            "HEAD 10 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1141, 0.0147, 0.2306, 0.0312, -0.1102, ..., -0.4191, 1.2161, 0.5313, 0.9569, -0.1863]  (128 dims)\n",
            "Token 1 ( açò): [0.0513, 0.0182, 0.1338, 0.0942, -0.0680, ..., 0.5960, 0.8046, 1.2450, 2.1893, 0.4081]  (128 dims)\n",
            "Token 2 (  es): [0.6266, 0.1957, 0.1254, 0.1590, 0.8806, ..., 1.2206, -0.5071, 1.8096, 1.9422, 1.8352]  (128 dims)\n",
            "Token 3 (  or): [0.1193, -0.1472, 0.9017, -0.0758, 0.5661, ..., 0.9985, -0.7569, 1.6135, 1.2042, 2.4529]  (128 dims)\n",
            "Token 4 ( ...): [-0.2758, -0.0058, 0.3186, 0.0076, 0.3637, ..., 0.5888, 1.1321, 0.9232, 1.0876, 2.2418]  (128 dims)\n",
            "\n",
            "HEAD 11 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.2609, -0.2117, 0.4495, 0.1503, 0.2674, ..., -0.4155, -0.3622, -0.2970, -0.0813, -0.1502]  (128 dims)\n",
            "Token 1 ( açò): [-0.1928, 0.2727, -0.2421, -0.0718, -0.1143, ..., -0.2557, -0.0117, 0.1955, 0.1562, 1.6541]  (128 dims)\n",
            "Token 2 (  es): [-0.2366, -0.0619, -0.0423, 0.1602, -0.2217, ..., 0.0766, -0.0894, 0.6059, -0.4242, 2.1303]  (128 dims)\n",
            "Token 3 (  or): [-0.0379, 0.5918, -0.6315, -0.0423, -0.4830, ..., -2.0145, 0.0443, 0.3926, 0.4551, 1.5923]  (128 dims)\n",
            "Token 4 ( ...): [-0.5018, 0.0441, -0.6300, -0.9656, -0.3748, ..., -1.1283, 0.4830, 0.9542, 0.3948, 1.3256]  (128 dims)\n",
            "\n",
            "HEAD 12 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.1495, 0.2181, -0.2070, -0.1809, -0.1314, ..., 1.1907, -2.8027, -0.6817, 0.5220, -0.8140]  (128 dims)\n",
            "Token 1 ( açò): [0.8906, 0.6045, -0.2254, -0.9356, -0.0971, ..., 1.5752, -2.6733, -0.8838, 0.7213, -1.0016]  (128 dims)\n",
            "Token 2 (  es): [0.5182, 0.5106, 0.3893, -1.1571, 0.1816, ..., 0.6484, -0.5645, 0.5728, -0.7833, -0.5686]  (128 dims)\n",
            "Token 3 (  or): [-0.2551, -0.0857, 0.6829, -0.4152, 0.3237, ..., -0.1532, -2.8561, -0.0682, -0.3700, -0.7338]  (128 dims)\n",
            "Token 4 ( ...): [-1.4134, -1.0770, 1.8907, 0.9697, 2.2998, ..., -0.1030, -1.8990, 0.3078, -0.5260, -0.6558]  (128 dims)\n",
            "\n",
            "HEAD 13 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.4624, -0.1438, 0.5297, 0.3217, 0.5069, ..., -0.7191, 0.7023, 0.2106, 0.3088, 0.4109]  (128 dims)\n",
            "Token 1 ( açò): [0.2617, 0.4184, 1.0353, -0.3006, 1.1072, ..., 0.1798, -0.2021, -0.4272, -0.9019, -0.8955]  (128 dims)\n",
            "Token 2 (  es): [0.0864, -0.7482, -0.2687, 0.0514, -0.7653, ..., -2.0417, 2.2301, 0.2480, 1.2094, 1.9694]  (128 dims)\n",
            "Token 3 (  or): [-0.5941, -0.5143, -0.5527, 0.3638, -0.9854, ..., -2.0305, 2.0647, 0.3165, 1.2847, 1.7257]  (128 dims)\n",
            "Token 4 ( ...): [-0.7267, -0.4258, 0.0954, 0.5923, -1.0001, ..., -1.4631, 1.4451, -0.5590, 0.1496, 0.6554]  (128 dims)\n",
            "\n",
            "HEAD 14 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [0.2740, -0.0506, 0.4008, -0.1798, 0.0627, ..., 1.1809, -0.2412, -0.8631, -0.6208, 0.6555]  (128 dims)\n",
            "Token 1 ( açò): [0.1704, 0.0039, -0.2749, -0.0557, 0.0241, ..., -0.2111, -0.6326, 0.8495, 0.3804, -0.4113]  (128 dims)\n",
            "Token 2 (  es): [-0.0070, 0.5695, -0.4239, 0.2107, -0.2851, ..., 1.0523, -0.8596, -1.1386, 1.0769, -1.0267]  (128 dims)\n",
            "Token 3 (  or): [-0.0819, 0.2520, -0.3294, 0.3245, 0.2684, ..., 0.3532, -1.6036, -0.4636, 0.3481, -0.4000]  (128 dims)\n",
            "Token 4 ( ...): [-0.5825, 0.0343, -0.1723, 0.5155, -0.1084, ..., 1.9940, -1.7373, -1.2353, 0.6799, -0.6210]  (128 dims)\n",
            "\n",
            "HEAD 15 - Shape: (5, 128)\n",
            "================================================================================\n",
            "Token 0 ( <s>): [-0.8710, -1.0364, -0.1889, -0.3356, 0.0606, ..., -0.2882, 0.4113, -0.1774, 0.4181, -0.3370]  (128 dims)\n",
            "Token 1 ( açò): [-0.0968, -0.6701, 0.4031, -0.0736, 0.1436, ..., -0.0360, -0.6349, -0.0746, 0.4615, 0.5063]  (128 dims)\n",
            "Token 2 (  es): [0.1469, -0.2049, 0.1913, -0.3113, -0.2789, ..., -1.6740, -0.4839, -0.7521, -0.1440, 0.1016]  (128 dims)\n",
            "Token 3 (  or): [-0.1433, 0.0179, -0.1340, -0.2052, -0.2585, ..., -2.2730, -0.1701, -0.7408, -0.2896, -0.0449]  (128 dims)\n",
            "Token 4 ( ...): [0.1461, 0.0850, 0.2447, -0.0407, -0.2090, ..., -1.8036, -0.8685, -1.1204, -1.0407, -0.5513]  (128 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcular scores Q @ K.T"
      ],
      "metadata": {
        "id": "MT7P0uQmUV3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para cada head, multiplicar Q por K transpuesta\n",
        "attention_scores = Q_heads @ K_heads.transpose(-2, -1)  # (16, 5, 5)\n",
        "\n",
        "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]  (5 dims)\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "ea04cJYtyQjX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bc13572-0c0d-40aa-b421-e7c6be5e7f59"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [57.2693, 2.6132, -13.2311, -11.5541, 4.2293]  (5 dims)\n",
            "Token 1( açò): [53.6804, 2.9718, 16.6660, 16.5238, -14.3015]  (5 dims)\n",
            "Token 2(  es): [59.8908, 45.1919, 23.7828, 36.1832, -6.7057]  (5 dims)\n",
            "Token 3(  or): [59.8662, 27.2245, 19.8011, 28.0697, -15.3594]  (5 dims)\n",
            "Token 4( ...): [57.5126, 39.2291, 20.6731, 36.6379, -0.0167]  (5 dims)\n",
            "\n",
            "HEAD 1  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [89.0011, 57.6921, 44.7067, 50.4499, 48.8925]  (5 dims)\n",
            "Token 1( açò): [59.3866, 49.7864, 42.8304, 47.4151, 42.0602]  (5 dims)\n",
            "Token 2(  es): [66.4886, 62.1783, 52.2892, 54.0402, 45.2684]  (5 dims)\n",
            "Token 3(  or): [62.0758, 57.8404, 46.0871, 49.4880, 43.9775]  (5 dims)\n",
            "Token 4( ...): [86.6364, 69.0822, 53.9880, 62.1124, 52.9171]  (5 dims)\n",
            "\n",
            "HEAD 2  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0198, 47.2791, 35.6356, 33.9745, 31.6207]  (5 dims)\n",
            "Token 1( açò): [59.4924, 44.6003, 37.8105, 36.8490, 33.7302]  (5 dims)\n",
            "Token 2(  es): [72.2944, 63.9624, 57.4797, 44.7616, 44.4898]  (5 dims)\n",
            "Token 3(  or): [58.8566, 65.3760, 59.5049, 43.9086, 47.1601]  (5 dims)\n",
            "Token 4( ...): [73.1809, 76.1332, 71.1952, 55.5038, 56.7187]  (5 dims)\n",
            "\n",
            "HEAD 3  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [192.7018, 43.3888, 51.7746, 38.9054, 17.3284]  (5 dims)\n",
            "Token 1( açò): [175.0408, 56.0152, 39.9238, 38.9786, 28.3519]  (5 dims)\n",
            "Token 2(  es): [147.0911, 45.6690, 42.6596, 38.5651, 29.2315]  (5 dims)\n",
            "Token 3(  or): [148.3764, 39.6019, 37.1139, 31.4962, 30.2535]  (5 dims)\n",
            "Token 4( ...): [159.6671, 43.7794, 36.3609, 39.4677, 27.7998]  (5 dims)\n",
            "\n",
            "HEAD 4  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [19.3079, 13.9675, 8.4543, 1.3233, 3.8772]  (5 dims)\n",
            "Token 1( açò): [26.5154, 17.9020, 15.5686, 18.0840, 12.9119]  (5 dims)\n",
            "Token 2(  es): [28.9216, 11.5564, 12.4854, 12.4551, 4.5309]  (5 dims)\n",
            "Token 3(  or): [33.9627, 13.8373, 16.8333, 13.3945, 10.4475]  (5 dims)\n",
            "Token 4( ...): [39.9414, 24.4077, 26.2512, 25.4779, 16.1474]  (5 dims)\n",
            "\n",
            "HEAD 5  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [64.3610, 42.8374, 34.8628, 49.3791, 35.4903]  (5 dims)\n",
            "Token 1( açò): [70.0041, 50.5220, 46.4126, 52.3538, 45.6203]  (5 dims)\n",
            "Token 2(  es): [65.8571, 59.5179, 53.9371, 62.2734, 51.2757]  (5 dims)\n",
            "Token 3(  or): [62.3272, 50.2687, 41.4245, 49.8743, 41.3031]  (5 dims)\n",
            "Token 4( ...): [77.8362, 65.2869, 61.1912, 74.0971, 60.2403]  (5 dims)\n",
            "\n",
            "HEAD 6  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [31.0864, 18.7663, 17.9248, 21.7025, 19.0006]  (5 dims)\n",
            "Token 1( açò): [27.9320, 34.9156, 33.3009, 38.6718, 30.1602]  (5 dims)\n",
            "Token 2(  es): [42.5088, 31.7305, 37.2235, 38.7079, 33.8231]  (5 dims)\n",
            "Token 3(  or): [37.5215, 31.6129, 33.9955, 40.6773, 35.0728]  (5 dims)\n",
            "Token 4( ...): [60.4183, 41.7473, 48.0986, 53.9661, 50.0786]  (5 dims)\n",
            "\n",
            "HEAD 7  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [98.8943, 44.8995, 35.3953, 46.2717, 36.3378]  (5 dims)\n",
            "Token 1( açò): [90.4400, 65.6321, 39.1654, 45.4194, 41.3217]  (5 dims)\n",
            "Token 2(  es): [100.6154, 130.9954, 75.7443, 60.5778, 57.6430]  (5 dims)\n",
            "Token 3(  or): [97.0408, 85.5344, 48.1636, 50.1396, 45.8447]  (5 dims)\n",
            "Token 4( ...): [137.0521, 92.8178, 58.9102, 72.8862, 68.1102]  (5 dims)\n",
            "\n",
            "HEAD 8  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [62.6750, 31.3035, 22.9930, 21.0347, 20.9002]  (5 dims)\n",
            "Token 1( açò): [57.0816, 33.5090, 26.8528, 25.3689, 22.1710]  (5 dims)\n",
            "Token 2(  es): [59.0071, 29.9352, 27.8470, 25.5398, 23.9035]  (5 dims)\n",
            "Token 3(  or): [62.1576, 31.0461, 27.8166, 21.5567, 23.8434]  (5 dims)\n",
            "Token 4( ...): [73.8060, 40.6871, 39.2399, 34.9727, 41.4608]  (5 dims)\n",
            "\n",
            "HEAD 9  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0327, 35.6129, 36.2276, 34.3824, 21.0371]  (5 dims)\n",
            "Token 1( açò): [34.2261, 32.3804, 33.4900, 40.9586, 3.4226]  (5 dims)\n",
            "Token 2(  es): [47.1491, 34.8875, 39.9387, 44.8859, 12.2260]  (5 dims)\n",
            "Token 3(  or): [49.9578, 35.7671, 34.7615, 31.8253, -4.8823]  (5 dims)\n",
            "Token 4( ...): [67.0046, 27.6512, 31.6569, 19.1158, 29.2106]  (5 dims)\n",
            "\n",
            "HEAD 10  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [114.2219, -18.6863, -26.2922, -9.2714, 6.3606]  (5 dims)\n",
            "Token 1( açò): [131.0323, 15.6294, 17.8656, 13.1725, 3.5624]  (5 dims)\n",
            "Token 2(  es): [136.8885, 31.8302, 38.6148, 43.6785, 22.4090]  (5 dims)\n",
            "Token 3(  or): [134.7263, 31.9176, 33.8395, 35.6799, 22.4003]  (5 dims)\n",
            "Token 4( ...): [127.7521, 18.3322, 6.6654, 24.1850, 1.2952]  (5 dims)\n",
            "\n",
            "HEAD 11  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [30.0089, 22.6481, 25.5079, 20.2462, 25.3582]  (5 dims)\n",
            "Token 1( açò): [31.1262, 35.4326, 37.7726, 34.6135, 35.0094]  (5 dims)\n",
            "Token 2(  es): [45.3402, 49.8131, 44.9214, 41.8860, 33.1227]  (5 dims)\n",
            "Token 3(  or): [42.2704, 43.0881, 40.4683, 35.6568, 26.7663]  (5 dims)\n",
            "Token 4( ...): [56.6694, 58.5355, 58.8203, 51.1950, 45.8525]  (5 dims)\n",
            "\n",
            "HEAD 12  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [78.7889, 63.1621, 46.6546, 52.9904, 47.3265]  (5 dims)\n",
            "Token 1( açò): [76.6355, 70.8005, 54.2441, 58.8523, 48.2235]  (5 dims)\n",
            "Token 2(  es): [54.4559, 59.3919, 46.0080, 46.9834, 44.2322]  (5 dims)\n",
            "Token 3(  or): [66.4957, 60.1235, 53.9893, 53.0188, 51.8577]  (5 dims)\n",
            "Token 4( ...): [57.5518, 52.3639, 40.4597, 49.6744, 36.4958]  (5 dims)\n",
            "\n",
            "HEAD 13  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.1164, -30.0530, 43.9225, 41.0464, 29.2171]  (5 dims)\n",
            "Token 1( açò): [7.9021, -45.6459, 47.9711, 52.4033, 42.8267]  (5 dims)\n",
            "Token 2(  es): [5.6784, -40.5031, 54.7064, 52.1317, 42.7724]  (5 dims)\n",
            "Token 3(  or): [8.0111, -52.3206, 61.6921, 61.5716, 57.6462]  (5 dims)\n",
            "Token 4( ...): [19.5443, -27.4960, 50.3061, 56.5095, 57.6692]  (5 dims)\n",
            "\n",
            "HEAD 14  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [71.2615, 43.0305, 35.6881, 32.8010, 31.5173]  (5 dims)\n",
            "Token 1( açò): [74.5795, 52.6590, 42.5869, 42.0648, 43.9312]  (5 dims)\n",
            "Token 2(  es): [90.2267, 54.3971, 47.9821, 40.4488, 47.1339]  (5 dims)\n",
            "Token 3(  or): [81.0496, 48.9448, 49.5234, 28.5561, 45.4943]  (5 dims)\n",
            "Token 4( ...): [93.9069, 55.7860, 49.7745, 42.2284, 53.4734]  (5 dims)\n",
            "\n",
            "HEAD 15  Scores Matrix - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [46.4097, 33.8967, 27.9736, 29.4411, 28.2988]  (5 dims)\n",
            "Token 1( açò): [67.9278, 47.6794, 41.3830, 41.9315, 43.6708]  (5 dims)\n",
            "Token 2(  es): [74.3707, 58.2002, 53.9741, 49.3251, 50.9968]  (5 dims)\n",
            "Token 3(  or): [64.4465, 49.0663, 45.7398, 43.9889, 43.5315]  (5 dims)\n",
            "Token 4( ...): [80.0894, 57.4186, 56.0733, 51.0150, 52.4121]  (5 dims)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicar máscara causal"
      ],
      "metadata": {
        "id": "4i8DuK1OUc2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos una máscara a los attention scores para que los tokens no tengan información de los tokens siguientes.\n",
        "\n",
        "Así cada secuencia sirve para entrenar el modelo a predecir el siguiente token (sin hacer trampa viendo el futuro)."
      ],
      "metadata": {
        "id": "XAgpSCi013l7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear máscara triangular inferior (True = permitido, False = bloqueado)\n",
        "causal_mask = torch.tril(torch.ones(5, 5)).bool()\n",
        "\n",
        "print(\"Máscara causal (5×5):\")\n",
        "print(causal_mask.int())  # 1 = puede atender, 0 = bloqueado"
      ],
      "metadata": {
        "id": "6MV1eAbWuRJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3aaedb7-b6b4-4783-fa5b-b70ebb2c7110"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Máscara causal (5×5):\n",
            "tensor([[1, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1]], dtype=torch.int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ponemos `-inf` donde no queremos que el token atienda. Tras el softmax que haremos después, `-inf` se convierte en 0 (peso nulo)."
      ],
      "metadata": {
        "id": "7H2NHTaX3sbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar máscara: poner -inf donde no puede atender\n",
        "attention_scores_masked = attention_scores.masked_fill(~causal_mask, float('-inf'))\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_scores_masked.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores_masked[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "21l9-65XyKrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4686fcfc-c99f-47f0-dc64-90bd1a36581f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [57.2693, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [53.6804, 2.9718, -inf, -inf, -inf]\n",
            "Token 2(  es): [59.8908, 45.1919, 23.7828, -inf, -inf]\n",
            "Token 3(  or): [59.8662, 27.2245, 19.8011, 28.0697, -inf]\n",
            "Token 4( ...): [57.5126, 39.2291, 20.6731, 36.6379, -0.0167]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [89.0011, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [59.3866, 49.7864, -inf, -inf, -inf]\n",
            "Token 2(  es): [66.4886, 62.1783, 52.2892, -inf, -inf]\n",
            "Token 3(  or): [62.0758, 57.8404, 46.0871, 49.4880, -inf]\n",
            "Token 4( ...): [86.6364, 69.0822, 53.9880, 62.1124, 52.9171]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0198, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [59.4924, 44.6003, -inf, -inf, -inf]\n",
            "Token 2(  es): [72.2944, 63.9624, 57.4797, -inf, -inf]\n",
            "Token 3(  or): [58.8566, 65.3760, 59.5049, 43.9086, -inf]\n",
            "Token 4( ...): [73.1809, 76.1332, 71.1952, 55.5038, 56.7187]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [192.7018, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [175.0408, 56.0152, -inf, -inf, -inf]\n",
            "Token 2(  es): [147.0911, 45.6690, 42.6596, -inf, -inf]\n",
            "Token 3(  or): [148.3764, 39.6019, 37.1139, 31.4962, -inf]\n",
            "Token 4( ...): [159.6671, 43.7794, 36.3609, 39.4677, 27.7998]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [19.3079, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [26.5154, 17.9020, -inf, -inf, -inf]\n",
            "Token 2(  es): [28.9216, 11.5564, 12.4854, -inf, -inf]\n",
            "Token 3(  or): [33.9627, 13.8373, 16.8333, 13.3945, -inf]\n",
            "Token 4( ...): [39.9414, 24.4077, 26.2512, 25.4779, 16.1474]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [64.3610, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [70.0041, 50.5220, -inf, -inf, -inf]\n",
            "Token 2(  es): [65.8571, 59.5179, 53.9371, -inf, -inf]\n",
            "Token 3(  or): [62.3272, 50.2687, 41.4245, 49.8743, -inf]\n",
            "Token 4( ...): [77.8362, 65.2869, 61.1912, 74.0971, 60.2403]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [31.0864, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [27.9320, 34.9156, -inf, -inf, -inf]\n",
            "Token 2(  es): [42.5088, 31.7305, 37.2235, -inf, -inf]\n",
            "Token 3(  or): [37.5215, 31.6129, 33.9955, 40.6773, -inf]\n",
            "Token 4( ...): [60.4183, 41.7473, 48.0986, 53.9661, 50.0786]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [98.8943, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [90.4400, 65.6321, -inf, -inf, -inf]\n",
            "Token 2(  es): [100.6154, 130.9954, 75.7443, -inf, -inf]\n",
            "Token 3(  or): [97.0408, 85.5344, 48.1636, 50.1396, -inf]\n",
            "Token 4( ...): [137.0521, 92.8178, 58.9102, 72.8862, 68.1102]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [62.6750, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [57.0816, 33.5090, -inf, -inf, -inf]\n",
            "Token 2(  es): [59.0071, 29.9352, 27.8470, -inf, -inf]\n",
            "Token 3(  or): [62.1576, 31.0461, 27.8166, 21.5567, -inf]\n",
            "Token 4( ...): [73.8060, 40.6871, 39.2399, 34.9727, 41.4608]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [51.0327, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [34.2261, 32.3804, -inf, -inf, -inf]\n",
            "Token 2(  es): [47.1491, 34.8875, 39.9387, -inf, -inf]\n",
            "Token 3(  or): [49.9578, 35.7671, 34.7615, 31.8253, -inf]\n",
            "Token 4( ...): [67.0046, 27.6512, 31.6569, 19.1158, 29.2106]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [114.2219, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [131.0323, 15.6294, -inf, -inf, -inf]\n",
            "Token 2(  es): [136.8885, 31.8302, 38.6148, -inf, -inf]\n",
            "Token 3(  or): [134.7263, 31.9176, 33.8395, 35.6799, -inf]\n",
            "Token 4( ...): [127.7521, 18.3322, 6.6654, 24.1850, 1.2952]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [30.0089, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [31.1262, 35.4326, -inf, -inf, -inf]\n",
            "Token 2(  es): [45.3402, 49.8131, 44.9214, -inf, -inf]\n",
            "Token 3(  or): [42.2704, 43.0881, 40.4683, 35.6568, -inf]\n",
            "Token 4( ...): [56.6694, 58.5355, 58.8203, 51.1950, 45.8525]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [78.7889, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [76.6355, 70.8005, -inf, -inf, -inf]\n",
            "Token 2(  es): [54.4559, 59.3919, 46.0080, -inf, -inf]\n",
            "Token 3(  or): [66.4957, 60.1235, 53.9893, 53.0188, -inf]\n",
            "Token 4( ...): [57.5518, 52.3639, 40.4597, 49.6744, 36.4958]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.1164, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [7.9021, -45.6459, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.6784, -40.5031, 54.7064, -inf, -inf]\n",
            "Token 3(  or): [8.0111, -52.3206, 61.6921, 61.5716, -inf]\n",
            "Token 4( ...): [19.5443, -27.4960, 50.3061, 56.5095, 57.6692]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [71.2615, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [74.5795, 52.6590, -inf, -inf, -inf]\n",
            "Token 2(  es): [90.2267, 54.3971, 47.9821, -inf, -inf]\n",
            "Token 3(  or): [81.0496, 48.9448, 49.5234, 28.5561, -inf]\n",
            "Token 4( ...): [93.9069, 55.7860, 49.7745, 42.2284, 53.4734]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [46.4097, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [67.9278, 47.6794, -inf, -inf, -inf]\n",
            "Token 2(  es): [74.3707, 58.2002, 53.9741, -inf, -inf]\n",
            "Token 3(  or): [64.4465, 49.0663, 45.7398, 43.9889, -inf]\n",
            "Token 4( ...): [80.0894, 57.4186, 56.0733, 51.0150, 52.4121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos observar tenemos valores grandes en las matrices (de hasta >100 en head 10).\n",
        "El siguiente paso es un escalado se divididen los scores por √(head_dim) para estabilizar los gradientes. Podemos observar valores mucho más moderados en todos los heads."
      ],
      "metadata": {
        "id": "nt1jri6_3p2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Escalar los scores\n",
        "scale = math.sqrt(head_dim)  # sqrt(128) ≈ 11.31\n",
        "attention_scores_scaled = attention_scores_masked / scale\n",
        "\n",
        "print(f\"Escalado por √{head_dim} = {scale:.2f}\")\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_scores_scaled.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_scores_scaled[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "5lfUGDYR4Rxc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "825e5d65-fd8d-40e8-c7df-076badc315a2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Escalado por √128 = 11.31\n",
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.0619, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [4.7447, 0.2627, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.2937, 3.9944, 2.1021, -inf, -inf]\n",
            "Token 3(  or): [5.2915, 2.4063, 1.7502, 2.4810, -inf]\n",
            "Token 4( ...): [5.0834, 3.4674, 1.8273, 3.2384, -0.0015]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [7.8667, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.2491, 4.4005, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.8768, 5.4958, 4.6218, -inf, -inf]\n",
            "Token 3(  or): [5.4868, 5.1124, 4.0736, 4.3742, -inf]\n",
            "Token 4( ...): [7.6576, 6.1061, 4.7719, 5.4900, 4.6773]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.5096, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.2584, 3.9421, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.3900, 5.6535, 5.0805, -inf, -inf]\n",
            "Token 3(  or): [5.2022, 5.7785, 5.2595, 3.8810, -inf]\n",
            "Token 4( ...): [6.4683, 6.7293, 6.2928, 4.9059, 5.0133]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [17.0326, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [15.4716, 4.9511, -inf, -inf, -inf]\n",
            "Token 2(  es): [13.0011, 4.0366, 3.7706, -inf, -inf]\n",
            "Token 3(  or): [13.1147, 3.5003, 3.2804, 2.7839, -inf]\n",
            "Token 4( ...): [14.1127, 3.8696, 3.2139, 3.4885, 2.4572]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.7066, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.3437, 1.5823, -inf, -inf, -inf]\n",
            "Token 2(  es): [2.5563, 1.0215, 1.1036, -inf, -inf]\n",
            "Token 3(  or): [3.0019, 1.2231, 1.4879, 1.1839, -inf]\n",
            "Token 4( ...): [3.5304, 2.1574, 2.3203, 2.2519, 1.4272]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.6888, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.1875, 4.4656, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.8210, 5.2607, 4.7674, -inf, -inf]\n",
            "Token 3(  or): [5.5090, 4.4432, 3.6614, 4.4083, -inf]\n",
            "Token 4( ...): [6.8798, 5.7706, 5.4086, 6.5493, 5.3245]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [2.7477, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.4689, 3.0861, -inf, -inf, -inf]\n",
            "Token 2(  es): [3.7573, 2.8046, 3.2901, -inf, -inf]\n",
            "Token 3(  or): [3.3165, 2.7942, 3.0048, 3.5954, -inf]\n",
            "Token 4( ...): [5.3403, 3.6900, 4.2514, 4.7700, 4.4264]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [8.7411, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [7.9938, 5.8011, -inf, -inf, -inf]\n",
            "Token 2(  es): [8.8932, 11.5785, 6.6949, -inf, -inf]\n",
            "Token 3(  or): [8.5773, 7.5602, 4.2571, 4.4318, -inf]\n",
            "Token 4( ...): [12.1138, 8.2040, 5.2070, 6.4423, 6.0201]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [5.5397, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [5.0453, 2.9618, -inf, -inf, -inf]\n",
            "Token 2(  es): [5.2155, 2.6459, 2.4614, -inf, -inf]\n",
            "Token 3(  or): [5.4940, 2.7441, 2.4587, 1.9054, -inf]\n",
            "Token 4( ...): [6.5236, 3.5963, 3.4683, 3.0912, 3.6647]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.5107, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [3.0252, 2.8620, -inf, -inf, -inf]\n",
            "Token 2(  es): [4.1674, 3.0836, 3.5301, -inf, -inf]\n",
            "Token 3(  or): [4.4157, 3.1614, 3.0725, 2.8130, -inf]\n",
            "Token 4( ...): [5.9224, 2.4440, 2.7981, 1.6896, 2.5819]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [10.0959, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [11.5817, 1.3815, -inf, -inf, -inf]\n",
            "Token 2(  es): [12.0993, 2.8134, 3.4131, -inf, -inf]\n",
            "Token 3(  or): [11.9082, 2.8211, 2.9910, 3.1537, -inf]\n",
            "Token 4( ...): [11.2918, 1.6204, 0.5891, 2.1377, 0.1145]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [2.6524, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [2.7512, 3.1318, -inf, -inf, -inf]\n",
            "Token 2(  es): [4.0075, 4.4029, 3.9705, -inf, -inf]\n",
            "Token 3(  or): [3.7362, 3.8085, 3.5769, 3.1516, -inf]\n",
            "Token 4( ...): [5.0089, 5.1739, 5.1990, 4.5250, 4.0528]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [6.9640, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.7737, 6.2579, -inf, -inf, -inf]\n",
            "Token 2(  es): [4.8133, 5.2496, 4.0666, -inf, -inf]\n",
            "Token 3(  or): [5.8774, 5.3142, 4.7720, 4.6862, -inf]\n",
            "Token 4( ...): [5.0869, 4.6284, 3.5762, 4.3906, 3.2258]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.8942, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [0.6985, -4.0346, -inf, -inf, -inf]\n",
            "Token 2(  es): [0.5019, -3.5800, 4.8354, -inf, -inf]\n",
            "Token 3(  or): [0.7081, -4.6245, 5.4529, 5.4422, -inf]\n",
            "Token 4( ...): [1.7275, -2.4303, 4.4465, 4.9948, 5.0973]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [6.2987, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.5920, 4.6544, -inf, -inf, -inf]\n",
            "Token 2(  es): [7.9750, 4.8081, 4.2411, -inf, -inf]\n",
            "Token 3(  or): [7.1638, 4.3261, 4.3773, 2.5240, -inf]\n",
            "Token 4( ...): [8.3003, 4.9308, 4.3995, 3.7325, 4.7264]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [4.1021, -inf, -inf, -inf, -inf]\n",
            "Token 1( açò): [6.0040, 4.2143, -inf, -inf, -inf]\n",
            "Token 2(  es): [6.5735, 5.1442, 4.7707, -inf, -inf]\n",
            "Token 3(  or): [5.6963, 4.3369, 4.0429, 3.8881, -inf]\n",
            "Token 4( ...): [7.0790, 5.0751, 4.9562, 4.5091, 4.6326]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax"
      ],
      "metadata": {
        "id": "wbcuGfqvUlf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos la función softmax para transformar los scores en distribuciones de probabilidad.\n",
        "\n",
        "Cada fila representa cuánto atiende ese token a cada posición (suma 1). Los `-inf` se convierten en 0 (no atiende a tokens futuros).\n"
      ],
      "metadata": {
        "id": "3gVcmiOlQvXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar softmax a lo largo de la última dimensión (sobre los keys)\n",
        "attention_weights = torch.softmax(attention_scores_scaled, dim=-1)\n",
        "# se puede acceder también haciendo un forward directo con el parámetro output_attentions = True\n",
        "# outputs = model(\n",
        "    # input_ids=tokens,\n",
        "    # output_attentions=True)\n",
        "\n",
        "# #Acceder a los pesos de atención de la primera capa\n",
        "# attention_weights_from_model = outputs.attentions[0]\n",
        "\n",
        "# print(f\"Attention weights shape: {attention_weights.shape}  # (16, 5, 5)\")\n",
        "# for i in range(16):\n",
        "#     print(f\"\\nHead {i} - Atenciones:\")\n",
        "#     print(attention_weights[i])\n",
        "\n",
        "print(f\"Attention scores masked shape: {attention_weights.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_weights[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "KstYHJsD4Thw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f8eaf0-a44b-4563-e1f3-863d1704e67d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention scores masked shape: torch.Size([16, 5, 5])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.9888, 0.0112, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.7611, 0.2076, 0.0313, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.8734, 0.0488, 0.0253, 0.0526, 0.0000]\n",
            "Token 4( ...): [0.7136, 0.1418, 0.0275, 0.1128, 0.0044]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.7003, 0.2997, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5081, 0.3471, 0.1448, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.4425, 0.3043, 0.1077, 0.1455, 0.0000]\n",
            "Token 4( ...): [0.6979, 0.1479, 0.0390, 0.0799, 0.0354]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.7886, 0.2114, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5718, 0.2738, 0.1544, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.2436, 0.4334, 0.2580, 0.0650, 0.0000]\n",
            "Token 4( ...): [0.2793, 0.3626, 0.2344, 0.0586, 0.0652]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9998, 0.0001, 0.0001, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.9998, 0.0001, 0.0001, 0.0000, 0.0000]\n",
            "Token 4( ...): [0.9999, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.6816, 0.3184, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.6899, 0.1487, 0.1614, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.6447, 0.1088, 0.1418, 0.1047, 0.0000]\n",
            "Token 4( ...): [0.5123, 0.1298, 0.1528, 0.1427, 0.0625]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8484, 0.1516, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5209, 0.2975, 0.1816, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.5450, 0.1877, 0.0859, 0.1813, 0.0000]\n",
            "Token 4( ...): [0.4017, 0.1325, 0.0923, 0.2887, 0.0848]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.3504, 0.6496, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.4969, 0.1917, 0.3114, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.2742, 0.1626, 0.2008, 0.3624, 0.0000]\n",
            "Token 4( ...): [0.4008, 0.0770, 0.1349, 0.2266, 0.1607]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8996, 0.1004, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.0634, 0.9296, 0.0070, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.7190, 0.2600, 0.0096, 0.0114, 0.0000]\n",
            "Token 4( ...): [0.9740, 0.0195, 0.0010, 0.0034, 0.0022]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8893, 0.1107, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.8770, 0.0671, 0.0558, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.8775, 0.0561, 0.0422, 0.0242, 0.0000]\n",
            "Token 4( ...): [0.8401, 0.0450, 0.0396, 0.0271, 0.0482]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.5407, 0.4593, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.5356, 0.1812, 0.2832, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.5722, 0.1632, 0.1494, 0.1152, 0.0000]\n",
            "Token 4( ...): [0.8891, 0.0274, 0.0391, 0.0129, 0.0315]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9997, 0.0001, 0.0002, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.9996, 0.0001, 0.0001, 0.0002, 0.0000]\n",
            "Token 4( ...): [0.9998, 0.0001, 0.0000, 0.0001, 0.0000]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.4060, 0.5940, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.2900, 0.4306, 0.2794, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.2869, 0.3084, 0.2447, 0.1599, 0.0000]\n",
            "Token 4( ...): [0.2278, 0.2687, 0.2755, 0.1404, 0.0876]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.6262, 0.3738, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.3310, 0.5121, 0.1569, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.4537, 0.2583, 0.1502, 0.1378, 0.0000]\n",
            "Token 4( ...): [0.3989, 0.2522, 0.0881, 0.1988, 0.0620]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.9913, 0.0087, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.0129, 0.0002, 0.9868, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.0044, 0.0000, 0.5005, 0.4952, 0.0000]\n",
            "Token 4( ...): [0.0140, 0.0002, 0.2121, 0.3670, 0.4066]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8741, 0.1259, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.9381, 0.0395, 0.0224, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.8851, 0.0518, 0.0546, 0.0085, 0.0000]\n",
            "Token 4( ...): [0.9149, 0.0315, 0.0185, 0.0095, 0.0257]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]\n",
            "Token 1( açò): [0.8569, 0.1431, 0.0000, 0.0000, 0.0000]\n",
            "Token 2(  es): [0.7121, 0.1705, 0.1174, 0.0000, 0.0000]\n",
            "Token 3(  or): [0.6203, 0.1593, 0.1187, 0.1017, 0.0000]\n",
            "Token 4( ...): [0.7054, 0.0951, 0.0844, 0.0540, 0.0611]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede observar que cada fila suma 1 por lo que es una distribución de probabilidad. Ejecuta esta celda para obtener una fila aleatoria de entre todos los heads."
      ],
      "metadata": {
        "id": "obcw4dx4E4Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "head_idx = random.randrange(0,16)\n",
        "token_idx = random.randrange(0,5)\n",
        "\n",
        "token_vec = attention_weights[head_idx, token_idx, :]\n",
        "vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "print(f\"Visualización: fila {token_idx} del Head {head_idx} [{vals}]\\n\")\n",
        "print(f\"Verificación: fila 2 del Head 0 suma = {attention_weights[head_idx, token_idx].sum():.4f}\\n\")\n"
      ],
      "metadata": {
        "id": "iKEeVGF-EuhM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb35fad-cdd0-43a1-8a9a-aa8534bb9373"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualización: fila 3 del Head 2 [0.2436, 0.4334, 0.2580, 0.0650, 0.0000]\n",
            "\n",
            "Verificación: fila 2 del Head 0 suma = 1.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplicar por V"
      ],
      "metadata": {
        "id": "Uxcn8sKlUsQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos ahora por V (Values). Usamos los pesos de atención para hacer una suma ponderada de los valores (V).\n",
        "\n",
        "Para cada token: su output es la combinación de los valores de todos los tokens a los que puede atender, ponderados por los pesos de atención.\n",
        "\n",
        "Shape: (16, 5, 5) @ (16, 5, 128) → (16, 5, 128)"
      ],
      "metadata": {
        "id": "19Uu6b0vCrM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_output = attention_weights @ V_heads  # (16, 5, 5) @ (16, 5, 128) = (16, 5, 128)\n",
        "\n",
        "print(f\"Attention output shape: {attention_output.shape}\")\n",
        "for head_idx in range(16):\n",
        "    print(f\"HEAD {head_idx}  Scores Matrix Masked - Shape: (5, 5)\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for token_idx in range(5):\n",
        "        token_vec = attention_output[head_idx, token_idx, :]  # (5,)\n",
        "        vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:]])\n",
        "        print(f\"Token {token_idx}({tokens_names[token_idx]:>4}): [{vals}]\")\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "cHd5mqDkJVTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24c103b-e1ea-4dcb-af5e-6fb5aef7d2ba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention output shape: torch.Size([16, 5, 128])\n",
            "HEAD 0  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, -0.0268, 0.0001, -0.0074, -0.0203, -0.0113, -0.0069, 0.0037, 0.0077, -0.0264, 0.0089, 0.0743, -0.0110, 0.0357, -0.0081, -0.0119, 0.0223, -0.0005, -0.0158, -0.0681, -0.0191, 0.0026, -0.0196, -0.0647, 0.0632, 0.0025, 0.0083, -0.0066, 0.0099, 0.0139, 0.0199, 0.0052, -0.0139, -0.0588, -0.0068, -0.0102, -0.0105, -0.0369, 0.0047, -0.0142, 0.0287, -0.0232, -0.0006, -0.0135, -0.0284, 0.0318, -0.0401, -0.0100, 0.0217, 0.0798, -0.0268, -0.0493, 0.0201, 0.0348, 0.0252, 0.0080, -0.0221, -0.0970, -0.0056, 0.0109, -0.0025, 0.0152, -0.0430, 0.0101, -0.0115, -0.0063, -0.0262, -0.0302, 0.0068, 0.0156, 0.0106, -0.0095, -0.0237, 0.0203, 0.0236, -0.0365, 0.0313, -0.0393, -0.0204, -0.0207, -0.0192, 0.0172, 0.0244, -0.1107, -0.0117, 0.0131, -0.0048, 0.0041, -0.0074, 0.0333, -0.0161, 0.0165, -0.0058, 0.0180, -0.0902, -0.0098, 0.0532, 0.0356, -0.0033, -0.0686, -0.0261, -0.0346, -0.0287, 0.0008, -0.0222, 0.0131, -0.0130, -0.0142, 0.0031, -0.0115, -0.0222, 0.1851, 0.0110, 0.0032, 0.0084, 0.0043, 0.0030, -0.0789, -0.0556, 0.0101, 0.0171, 0.0520, 0.0119, -0.0049]\n",
            "Token 1( açò): [0.0142, -0.0011, 0.1035, 0.0477, -0.0123, -0.0264, -0.0002, -0.0076, -0.0198, -0.0110, -0.0065, 0.0033, 0.0079, -0.0252, 0.0087, 0.0745, -0.0108, 0.0354, -0.0077, -0.0117, 0.0218, -0.0005, -0.0150, -0.0667, -0.0183, 0.0024, -0.0208, -0.0642, 0.0623, 0.0025, 0.0083, -0.0066, 0.0099, 0.0134, 0.0196, 0.0044, -0.0137, -0.0566, -0.0064, -0.0105, -0.0098, -0.0363, 0.0049, -0.0137, 0.0281, -0.0225, -0.0005, -0.0135, -0.0278, 0.0316, -0.0397, -0.0100, 0.0215, 0.0792, -0.0266, -0.0483, 0.0203, 0.0340, 0.0251, 0.0076, -0.0218, -0.0971, -0.0054, 0.0109, -0.0028, 0.0155, -0.0428, 0.0097, -0.0112, -0.0065, -0.0261, -0.0297, 0.0068, 0.0150, 0.0101, -0.0093, -0.0234, 0.0196, 0.0233, -0.0349, 0.0309, -0.0395, -0.0203, -0.0205, -0.0191, 0.0173, 0.0242, -0.1092, -0.0114, 0.0133, -0.0046, 0.0040, -0.0074, 0.0326, -0.0157, 0.0164, -0.0059, 0.0176, -0.0886, -0.0094, 0.0535, 0.0355, -0.0035, -0.0673, -0.0255, -0.0349, -0.0289, 0.0007, -0.0220, 0.0129, -0.0125, -0.0143, 0.0037, -0.0113, -0.0219, 0.1836, 0.0106, 0.0026, 0.0082, 0.0039, 0.0036, -0.0821, -0.0548, 0.0100, 0.0169, 0.0511, 0.0115, -0.0050]\n",
            "Token 2(  es): [0.0075, 0.0072, 0.0931, 0.0378, -0.0060, -0.0186, -0.0052, -0.0131, -0.0106, -0.0051, 0.0010, -0.0019, 0.0097, -0.0043, 0.0056, 0.0771, -0.0062, 0.0280, 0.0002, -0.0089, 0.0139, 0.0001, -0.0013, -0.0423, -0.0036, -0.0005, -0.0495, -0.0516, 0.0444, 0.0031, 0.0072, -0.0089, 0.0098, 0.0054, 0.0133, -0.0091, -0.0110, -0.0217, 0.0001, -0.0144, 0.0030, -0.0256, 0.0082, -0.0045, 0.0178, -0.0084, 0.0004, -0.0133, -0.0139, 0.0282, -0.0304, -0.0096, 0.0180, 0.0672, -0.0207, -0.0312, 0.0231, 0.0175, 0.0235, 0.0003, -0.0185, -0.0965, -0.0015, 0.0113, -0.0074, 0.0188, -0.0372, 0.0017, -0.0070, -0.0097, -0.0236, -0.0183, 0.0066, 0.0040, -0.0001, -0.0061, -0.0186, 0.0070, 0.0165, -0.0084, 0.0245, -0.0404, -0.0218, -0.0155, -0.0185, 0.0175, 0.0203, -0.0801, -0.0063, 0.0147, -0.0010, 0.0020, -0.0083, 0.0201, -0.0067, 0.0141, -0.0084, 0.0092, -0.0616, -0.0031, 0.0587, 0.0318, -0.0049, -0.0446, -0.0134, -0.0393, -0.0336, -0.0008, -0.0177, 0.0081, -0.0038, -0.0144, 0.0153, -0.0077, -0.0162, 0.1590, 0.0029, -0.0080, 0.0052, -0.0033, 0.0145, -0.1400, -0.0407, 0.0076, 0.0120, 0.0322, 0.0047, -0.0089]\n",
            "Token 3(  or): [0.0131, 0.0029, 0.0935, 0.0424, -0.0084, -0.0226, -0.0010, -0.0083, -0.0159, -0.0096, -0.0054, 0.0012, 0.0069, -0.0184, 0.0073, 0.0751, -0.0081, 0.0328, -0.0066, -0.0126, 0.0182, 0.0007, -0.0124, -0.0597, -0.0201, 0.0016, -0.0376, -0.0591, 0.0572, 0.0023, 0.0081, -0.0084, 0.0097, 0.0116, 0.0202, 0.0018, -0.0125, -0.0500, -0.0055, -0.0095, -0.0104, -0.0321, 0.0059, -0.0115, 0.0232, -0.0181, 0.0011, -0.0109, -0.0210, 0.0281, -0.0341, -0.0083, 0.0190, 0.0721, -0.0247, -0.0420, 0.0184, 0.0244, 0.0247, 0.0053, -0.0234, -0.0943, -0.0046, 0.0117, -0.0032, 0.0139, -0.0412, 0.0056, -0.0114, -0.0082, -0.0240, -0.0261, 0.0046, 0.0119, 0.0063, -0.0091, -0.0218, 0.0148, 0.0185, -0.0321, 0.0276, -0.0354, -0.0257, -0.0174, -0.0191, 0.0181, 0.0241, -0.0960, -0.0089, 0.0101, -0.0030, 0.0014, -0.0084, 0.0289, -0.0118, 0.0153, -0.0064, 0.0147, -0.0801, -0.0083, 0.0530, 0.0319, -0.0017, -0.0615, -0.0203, -0.0301, -0.0274, 0.0004, -0.0202, 0.0089, -0.0106, -0.0120, 0.0064, -0.0105, -0.0187, 0.1815, 0.0089, -0.0003, 0.0083, 0.0028, 0.0056, -0.0850, -0.0505, 0.0098, 0.0148, 0.0411, 0.0089, -0.0063]\n",
            "Token 4( ...): [0.0101, 0.0089, 0.0834, 0.0348, -0.0041, -0.0183, -0.0033, -0.0095, -0.0095, -0.0062, -0.0030, -0.0035, 0.0068, -0.0058, 0.0041, 0.0768, -0.0044, 0.0289, -0.0037, -0.0115, 0.0125, 0.0015, -0.0064, -0.0437, -0.0173, -0.0000, -0.0573, -0.0548, 0.0491, 0.0014, 0.0080, -0.0097, 0.0105, 0.0080, 0.0205, -0.0061, -0.0111, -0.0292, -0.0028, -0.0098, -0.0079, -0.0254, 0.0081, -0.0073, 0.0157, -0.0099, 0.0036, -0.0080, -0.0117, 0.0245, -0.0271, -0.0076, 0.0164, 0.0624, -0.0231, -0.0288, 0.0176, 0.0122, 0.0232, 0.0013, -0.0211, -0.0925, -0.0034, 0.0121, -0.0048, 0.0142, -0.0384, -0.0002, -0.0090, -0.0118, -0.0223, -0.0202, 0.0025, 0.0054, 0.0010, -0.0074, -0.0185, 0.0065, 0.0137, -0.0197, 0.0219, -0.0331, -0.0277, -0.0144, -0.0185, 0.0186, 0.0230, -0.0772, -0.0056, 0.0090, -0.0003, -0.0026, -0.0103, 0.0218, -0.0072, 0.0141, -0.0071, 0.0106, -0.0609, -0.0048, 0.0523, 0.0281, -0.0012, -0.0489, -0.0128, -0.0276, -0.0272, -0.0001, -0.0164, 0.0040, -0.0063, -0.0098, 0.0122, -0.0086, -0.0149, 0.1673, 0.0045, -0.0069, 0.0071, -0.0001, 0.0100, -0.1001, -0.0435, 0.0099, 0.0120, 0.0281, 0.0054, -0.0077]\n",
            "\n",
            "HEAD 1  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0012, 0.1335, -0.0075, -0.0087, -0.0006, 0.0005, -0.0229, -0.0121, 0.0100, -0.0023, -0.0038, 0.0155, -0.0052, 0.0117, -0.0194, 0.0149, -0.0115, 0.0175, -0.0108, 0.0035, -0.0009, -0.0090, -0.0023, -0.0008, -0.0082, -0.0201, 0.0219, 0.0187, 0.0121, -0.0175, 0.0056, -0.0124, -0.0064, -0.0108, -0.0018, 0.0265, 0.0129, 0.0160, 0.0102, 0.0108, 0.0160, -0.0102, 0.0072, 0.0076, -0.0174, -0.0030, -0.0074, 0.0082, -0.0057, -0.0010, 0.0045, -0.0182, 0.0144, -0.0050, 0.0073, 0.0205, 0.0158, 0.0072, -0.0245, -0.0097, 0.0237, 0.0227, 0.0092, 0.0187, -0.0324, -0.0199, -0.0113, -0.0069, -0.0184, -0.0117, 0.0202, -0.0199, 0.0051, -0.0040, 0.0163, 0.0134, 0.0148, -0.0318, 0.0111, -0.0055, -0.0060, -0.0199, -0.0313, -0.0209, -0.0004, 0.0061, -0.0073, 0.0019, 0.0120, 0.0266, -0.0017, -0.0083, 0.0130, -0.0143, -0.0164, -0.1361, 0.0236, 0.0235, 0.0094, -0.0067, 0.0073, -0.0145, -0.0118, 0.0230, -0.0041, 0.0061, -0.0005, 0.0053, -0.3107, -0.0061, 0.0079, -0.0214, -0.0108, 0.0005, -0.0144, 0.0206, 0.0088, -0.0122, -0.0166, -0.0069, -0.0326, -0.0126, 0.0135, 0.0052, -0.0055, 0.0015, 0.0052, 0.0133]\n",
            "Token 1( açò): [0.0119, 0.1018, -0.0066, -0.0099, -0.0092, 0.0083, -0.0143, -0.0046, 0.0075, 0.0016, 0.0084, 0.0192, -0.0207, -0.0002, -0.0146, 0.0151, -0.0001, -0.0012, -0.0035, -0.0053, -0.0035, -0.0142, 0.0106, 0.0027, 0.0084, -0.0164, 0.0123, -0.0021, -0.0123, -0.0178, -0.0043, 0.0069, 0.0322, -0.0108, -0.0017, 0.0281, -0.0036, 0.0157, -0.0008, 0.0181, 0.0033, 0.0649, 0.0194, 0.0211, -0.0026, -0.0090, -0.0010, 0.0040, -0.0057, -0.0000, 0.0017, -0.0100, 0.0044, 0.0010, 0.0135, 0.0053, 0.0165, 0.0050, -0.0038, -0.0064, 0.0294, 0.0154, -0.0035, 0.0026, -0.0292, -0.0110, -0.0138, -0.0048, -0.0137, 0.0142, 0.0048, -0.0210, 0.0141, -0.0007, 0.0160, 0.0004, 0.0038, -0.0269, -0.0112, -0.0086, 0.0023, -0.0240, -0.0128, -0.0083, -0.0000, -0.0003, -0.0194, -0.0073, 0.0048, 0.0110, -0.0095, -0.0173, 0.0140, -0.0082, -0.0193, -0.0943, 0.0199, 0.0028, 0.0300, -0.0023, -0.0045, 0.0081, 0.0020, 0.0207, -0.0019, -0.0251, 0.0053, -0.0035, -0.2200, -0.0069, 0.0007, -0.0159, -0.0013, 0.0056, -0.0025, 0.0075, 0.0114, -0.0127, 0.0000, -0.0164, -0.0109, -0.0173, 0.0002, 0.0213, -0.0054, 0.0085, 0.0045, 0.0049]\n",
            "Token 2(  es): [0.0128, 0.0789, -0.0041, -0.0105, -0.0086, 0.0083, -0.0127, -0.0008, 0.0020, 0.0023, 0.0116, 0.0179, -0.0202, -0.0021, -0.0118, 0.0091, 0.0066, -0.0040, 0.0014, -0.0112, -0.0044, -0.0162, 0.0098, 0.0043, 0.0142, -0.0113, 0.0105, -0.0062, -0.0167, -0.0145, -0.0052, 0.0154, 0.0429, -0.0087, 0.0010, 0.0198, -0.0108, 0.0129, -0.0000, 0.0194, 0.0032, 0.0994, 0.0243, 0.0181, 0.0052, -0.0083, -0.0016, 0.0017, -0.0044, -0.0005, -0.0005, -0.0089, 0.0007, 0.0001, 0.0106, 0.0055, 0.0122, 0.0063, 0.0050, -0.0056, 0.0266, 0.0110, -0.0013, -0.0065, -0.0193, -0.0045, -0.0101, -0.0045, -0.0112, 0.0173, -0.0023, -0.0211, 0.0142, 0.0030, 0.0099, -0.0050, 0.0048, -0.0211, -0.0171, -0.0041, 0.0063, -0.0305, -0.0071, -0.0070, 0.0046, 0.0012, -0.0218, -0.0066, 0.0030, 0.0027, -0.0110, -0.0199, 0.0160, -0.0075, -0.0152, -0.0696, 0.0132, -0.0099, 0.0337, -0.0004, -0.0078, 0.0118, 0.0088, 0.0198, -0.0022, -0.0322, 0.0026, -0.0061, -0.1565, -0.0049, -0.0032, -0.0155, 0.0041, 0.0100, -0.0009, 0.0016, 0.0078, -0.0071, 0.0039, -0.0210, -0.0071, -0.0164, -0.0033, 0.0196, -0.0055, 0.0092, 0.0076, 0.0053]\n",
            "Token 3(  or): [0.0071, 0.0721, -0.0028, -0.0125, -0.0001, 0.0067, -0.0108, 0.0008, 0.0024, 0.0029, 0.0103, 0.0171, -0.0182, 0.0014, -0.0109, 0.0037, 0.0089, -0.0042, 0.0019, -0.0095, -0.0054, -0.0151, 0.0099, 0.0047, 0.0153, -0.0109, 0.0063, -0.0087, -0.0179, -0.0156, 0.0013, 0.0138, 0.0389, -0.0096, 0.0005, 0.0175, -0.0099, 0.0127, 0.0031, 0.0201, 0.0039, 0.1207, 0.0264, 0.0183, 0.0007, -0.0085, -0.0035, 0.0037, -0.0049, 0.0014, 0.0017, -0.0105, -0.0024, -0.0022, 0.0074, 0.0042, 0.0112, 0.0016, 0.0067, -0.0014, 0.0230, 0.0108, -0.0015, -0.0083, -0.0176, -0.0037, -0.0085, -0.0063, -0.0106, 0.0138, 0.0017, -0.0205, 0.0092, 0.0033, 0.0103, -0.0004, 0.0056, -0.0226, -0.0104, -0.0127, 0.0008, -0.0294, -0.0109, -0.0061, 0.0044, 0.0040, -0.0159, -0.0054, 0.0074, 0.0061, -0.0109, -0.0164, 0.0156, -0.0050, -0.0125, -0.0605, 0.0138, -0.0073, 0.0298, -0.0048, -0.0060, 0.0064, 0.0036, 0.0183, 0.0012, -0.0270, 0.0062, -0.0058, -0.1307, -0.0034, -0.0077, -0.0162, 0.0047, 0.0043, -0.0008, -0.0038, 0.0092, -0.0084, 0.0036, -0.0155, -0.0089, -0.0144, -0.0001, 0.0163, -0.0052, 0.0095, 0.0102, 0.0071]\n",
            "Token 4( ...): [0.0025, 0.0974, -0.0059, -0.0120, 0.0022, 0.0021, -0.0159, -0.0065, 0.0064, -0.0006, 0.0035, 0.0163, -0.0109, 0.0076, -0.0140, 0.0089, -0.0026, 0.0062, -0.0062, -0.0034, -0.0034, -0.0122, 0.0040, 0.0010, 0.0035, -0.0138, 0.0134, 0.0048, -0.0025, -0.0179, 0.0054, 0.0007, 0.0149, -0.0095, -0.0011, 0.0203, 0.0003, 0.0145, 0.0063, 0.0157, 0.0090, 0.0659, 0.0167, 0.0128, -0.0107, -0.0031, -0.0051, 0.0054, -0.0041, 0.0014, 0.0045, -0.0140, 0.0065, -0.0025, 0.0087, 0.0112, 0.0129, 0.0039, -0.0082, -0.0062, 0.0228, 0.0169, 0.0046, 0.0038, -0.0258, -0.0118, -0.0097, -0.0075, -0.0146, 0.0011, 0.0126, -0.0197, 0.0062, 0.0002, 0.0138, 0.0053, 0.0101, -0.0282, 0.0019, -0.0098, -0.0031, -0.0237, -0.0217, -0.0131, 0.0026, 0.0039, -0.0107, -0.0033, 0.0095, 0.0171, -0.0050, -0.0132, 0.0138, -0.0090, -0.0150, -0.0952, 0.0176, 0.0088, 0.0195, -0.0072, 0.0028, -0.0049, -0.0047, 0.0184, -0.0011, -0.0103, 0.0037, -0.0001, -0.2124, -0.0052, -0.0010, -0.0177, -0.0039, 0.0005, -0.0090, 0.0070, 0.0091, -0.0108, -0.0058, -0.0079, -0.0210, -0.0126, 0.0072, 0.0114, -0.0056, 0.0066, 0.0073, 0.0104]\n",
            "\n",
            "HEAD 2  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0236, 0.0308, 0.0247, 0.0003, -0.0147, -0.0312, 0.0550, 0.0086, -0.0274, 0.0342, -0.0096, 0.0584, 0.0084, 0.0174, 0.0022, 0.0344, -0.0254, -0.0255, 0.0147, -0.0068, -0.0180, -0.0221, -0.0280, -0.0238, -0.0153, 0.0116, -0.0280, -0.0048, -0.0186, 0.0164, 0.0284, -0.0507, 0.0667, -0.0321, -0.0313, -0.0008, 0.0558, 0.0023, 0.0042, -0.0332, -0.0056, -0.0238, 0.0336, 0.0053, -0.0498, 0.0188, 0.0039, -0.0216, -0.0039, 0.0271, 0.0520, -0.0211, -0.0073, 0.0067, -0.0047, 0.0392, 0.0272, 0.0081, -0.0352, 0.0057, -0.0338, 0.0036, 0.0144, -0.0150, -0.0680, 0.0115, 0.0086, 0.0428, -0.0103, -0.0305, 0.0323, -0.0095, 0.0125, 0.0240, -0.0388, 0.0368, 0.0578, 0.0346, -0.0078, 0.0148, -0.0256, -0.0003, -0.0308, 0.0177, 0.0304, -0.0237, 0.0452, 0.0181, -0.0392, 0.0036, 0.0022, 0.0253, 0.0149, -0.0104, -0.0237, -0.0180, -0.0086, 0.0032, 0.0461, 0.0240, 0.0008, 0.0463, 0.0173, -0.0139, -0.0276, -0.0321, 0.0268, 0.0005, -0.0448, -0.0102, 0.0062, -0.0446, -0.0137, 0.0014, -0.0325, 0.0752, 0.0415, -0.0472, -0.0143, -0.0047, 0.0013, 0.0084, 0.0744, -0.0319, -0.0092, -0.0265, -0.0062, 0.0050]\n",
            "Token 1( açò): [-0.0192, 0.0230, 0.0109, 0.0100, -0.0205, -0.0136, 0.0409, 0.0055, -0.0316, 0.0264, -0.0092, 0.0475, 0.0083, 0.0071, -0.0070, 0.0309, -0.0219, -0.0176, 0.0057, -0.0110, -0.0243, -0.0224, -0.0226, -0.0167, -0.0158, 0.0040, -0.0221, -0.0127, -0.0131, 0.0219, 0.0185, -0.0388, 0.0599, -0.0285, -0.0215, 0.0133, 0.0430, 0.0020, -0.0034, -0.0307, -0.0126, -0.0293, 0.0315, -0.0029, -0.0411, 0.0094, 0.0046, -0.0138, -0.0075, 0.0210, 0.0365, -0.0118, -0.0045, 0.0016, -0.0067, 0.0331, 0.0235, 0.0138, -0.0296, 0.0124, -0.0237, -0.0055, 0.0157, -0.0222, -0.0657, 0.0120, 0.0093, 0.0019, -0.0126, -0.0201, 0.0275, -0.0103, 0.0083, 0.0111, -0.0314, 0.0311, 0.0550, 0.0297, -0.0154, 0.0159, -0.0268, -0.0039, -0.0328, 0.0122, 0.0223, -0.0163, 0.0298, 0.0209, -0.0284, -0.0009, 0.0034, 0.0248, 0.0235, -0.0107, -0.0170, -0.0184, 0.0031, -0.0008, 0.0247, 0.0073, 0.0058, 0.0319, 0.0170, -0.0175, -0.0223, -0.0208, 0.0216, 0.0067, -0.0397, -0.0131, 0.0022, -0.0436, -0.0175, -0.0070, -0.0320, 0.0659, 0.0368, -0.0300, -0.0211, -0.0081, 0.0001, 0.0080, 0.0653, -0.0256, 0.0086, -0.0301, -0.0022, -0.0041]\n",
            "Token 2(  es): [-0.0086, 0.0266, 0.0021, 0.0101, -0.0142, -0.0099, 0.0242, 0.0050, -0.0279, 0.0200, -0.0056, 0.0328, 0.0061, 0.0015, -0.0065, 0.0254, -0.0196, -0.0139, 0.0035, -0.0041, -0.0232, -0.0264, -0.0202, -0.0154, -0.0123, -0.0038, -0.0187, -0.0092, -0.0131, 0.0242, 0.0065, -0.0216, 0.0494, -0.0140, -0.0153, 0.0261, 0.0254, -0.0038, -0.0021, -0.0248, -0.0031, -0.0232, 0.0265, 0.0001, -0.0274, 0.0046, 0.0088, -0.0102, -0.0134, 0.0136, 0.0204, -0.0059, -0.0053, -0.0005, -0.0023, 0.0218, 0.0198, 0.0159, -0.0217, 0.0075, -0.0189, -0.0034, 0.0182, -0.0263, -0.0563, 0.0093, 0.0037, -0.0500, -0.0166, -0.0143, 0.0262, -0.0063, 0.0035, 0.0089, -0.0204, 0.0250, 0.0418, 0.0209, -0.0168, 0.0128, -0.0173, -0.0031, -0.0249, 0.0092, 0.0103, -0.0143, 0.0185, 0.0158, -0.0152, -0.0037, -0.0015, 0.0187, 0.0196, -0.0048, -0.0132, -0.0112, -0.0010, 0.0035, 0.0124, -0.0030, 0.0147, 0.0251, 0.0089, -0.0158, -0.0177, -0.0196, 0.0125, 0.0059, -0.0261, -0.0176, 0.0012, -0.0340, -0.0132, -0.0082, -0.0272, 0.0533, 0.0280, -0.0184, -0.0153, -0.0068, -0.0049, 0.0037, 0.0504, -0.0163, 0.0062, -0.0266, 0.0005, -0.0024]\n",
            "Token 3(  or): [0.0025, 0.0235, -0.0135, 0.0150, -0.0118, 0.0028, 0.0020, 0.0041, -0.0244, 0.0093, -0.0016, 0.0117, 0.0035, -0.0087, -0.0109, 0.0159, -0.0154, -0.0049, -0.0047, -0.0019, -0.0260, -0.0290, -0.0142, -0.0098, -0.0096, -0.0153, -0.0114, -0.0092, -0.0083, 0.0263, -0.0080, 0.0008, 0.0331, -0.0007, -0.0027, 0.0412, 0.0035, -0.0086, -0.0045, -0.0156, 0.0006, -0.0220, 0.0211, -0.0026, -0.0092, -0.0040, 0.0115, -0.0015, -0.0184, 0.0027, -0.0004, 0.0061, -0.0031, -0.0055, -0.0007, 0.0089, 0.0139, 0.0216, -0.0127, 0.0089, -0.0084, -0.0084, 0.0194, -0.0314, -0.0449, 0.0059, -0.0009, -0.1219, -0.0192, -0.0009, 0.0197, -0.0041, -0.0020, -0.0008, -0.0070, 0.0157, 0.0269, 0.0108, -0.0206, 0.0091, -0.0107, -0.0034, -0.0170, 0.0025, -0.0045, -0.0083, -0.0026, 0.0124, 0.0021, -0.0070, -0.0050, 0.0127, 0.0207, -0.0002, -0.0051, -0.0051, 0.0038, 0.0036, -0.0098, -0.0202, 0.0232, 0.0096, 0.0029, -0.0151, -0.0096, -0.0099, 0.0030, 0.0074, -0.0103, -0.0204, -0.0017, -0.0251, -0.0114, -0.0121, -0.0207, 0.0357, 0.0179, 0.0035, -0.0134, -0.0075, -0.0084, -0.0016, 0.0313, -0.0029, 0.0147, -0.0245, 0.0045, -0.0076]\n",
            "Token 4( ...): [0.0010, 0.0242, -0.0100, 0.0122, -0.0093, 0.0007, 0.0067, 0.0053, -0.0224, 0.0172, -0.0006, 0.0127, 0.0020, -0.0064, -0.0085, 0.0149, -0.0141, -0.0067, -0.0028, -0.0031, -0.0231, -0.0266, -0.0134, -0.0102, -0.0085, -0.0123, -0.0142, -0.0097, -0.0085, 0.0250, -0.0052, -0.0026, 0.0353, -0.0005, -0.0035, 0.0337, 0.0078, -0.0081, -0.0038, -0.0150, 0.0034, -0.0192, 0.0206, 0.0011, -0.0107, -0.0013, 0.0099, -0.0014, -0.0156, 0.0041, 0.0021, 0.0034, -0.0026, -0.0043, -0.0006, 0.0105, 0.0122, 0.0208, -0.0132, 0.0068, -0.0093, -0.0041, 0.0174, -0.0303, -0.0424, 0.0051, -0.0019, -0.1143, -0.0166, -0.0029, 0.0180, -0.0041, -0.0009, 0.0019, -0.0094, 0.0160, 0.0260, 0.0130, -0.0188, 0.0098, -0.0054, -0.0021, -0.0163, 0.0012, -0.0032, -0.0092, -0.0060, 0.0097, -0.0007, -0.0050, -0.0037, 0.0117, 0.0180, 0.0023, -0.0077, -0.0041, 0.0027, 0.0028, -0.0042, -0.0137, 0.0197, 0.0104, 0.0008, -0.0139, -0.0078, -0.0096, 0.0025, 0.0067, -0.0081, -0.0141, 0.0013, -0.0230, -0.0091, -0.0116, -0.0176, 0.0351, 0.0170, -0.0008, -0.0096, -0.0035, -0.0103, -0.0005, 0.0323, -0.0034, 0.0102, -0.0207, 0.0034, -0.0051]\n",
            "\n",
            "HEAD 3  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0085, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0007, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0013, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0005, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0045, 0.0072, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0034, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 1( açò): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0085, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0007, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0013, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0072, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0034, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 2(  es): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0014, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0029, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0071, -0.0142, -0.0132, -0.0052, -0.0021, -0.0038, 0.0090, -0.0059, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0121, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0082, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 3(  or): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0006, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0014, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0072, -0.0142, -0.0132, -0.0052, -0.0021, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0121, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "Token 4( ...): [0.0004, 0.0032, 0.0086, -0.0058, -0.0165, 0.0016, -0.0011, 0.0002, 0.0021, -0.0063, -0.0001, -0.0045, -0.0084, 0.0113, -0.0049, 0.0021, 0.0106, 0.0101, -0.0007, 0.0010, -0.0048, -0.0021, 0.0008, 0.0010, 0.0057, 0.0125, 0.0119, -0.0002, -0.0092, 0.0063, -0.0052, 0.0188, 0.0062, 0.0104, 0.0127, 0.0037, 0.0057, -0.0054, 0.0029, -0.0128, -0.0019, -0.0002, -0.0053, -0.0026, 0.0068, -0.0134, -0.0007, -0.0007, -0.0145, -0.0105, -0.0122, 0.0007, -0.0009, -0.0089, -0.0009, -0.0030, 0.0083, 0.0029, -0.0004, 0.0076, 0.0090, 0.0047, 0.0056, -0.0126, 0.0068, 0.0015, 0.0089, -0.0026, -0.0043, -0.0079, 0.0112, 0.0070, 0.0013, -0.0004, -0.0094, 0.0113, -0.0101, -0.0049, 0.0264, 0.0028, 0.0028, -0.0004, -0.0076, 0.0015, 0.0064, -0.0043, -0.0107, -0.0063, -0.0045, -0.0094, -0.0029, -0.0089, -0.0024, 0.0044, 0.0072, -0.0142, -0.0132, -0.0052, -0.0022, -0.0038, 0.0090, -0.0060, -0.0157, -0.0032, -0.0086, 0.0022, -0.0035, -0.0122, -0.0084, -0.0082, 0.0183, -0.0129, 0.0108, 0.0096, 0.0059, 0.0019, 0.0081, -0.0057, -0.0086, -0.0099, -0.0037, 0.0128, -0.0015, 0.0062, 0.0037, 0.0009, -0.0147, 0.0098]\n",
            "\n",
            "HEAD 4  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0221, -0.0290, 0.0401, 0.0479, 0.0276, 0.0309, 0.0906, 0.0031, -0.0404, -0.0427, 0.0062, 0.0359, 0.0170, -0.0042, -0.0872, 0.0440, 0.0865, -0.0061, -0.0461, -0.0415, 0.0904, 0.0210, 0.0223, 0.1286, 0.0484, -0.0855, -0.0082, -0.0447, -0.0681, 0.0004, -0.0011, 0.0295, -0.0151, 0.0046, 0.0309, -0.1582, -0.0474, -0.0266, -0.0904, -0.0449, -0.0281, -0.0195, 0.0308, -0.0697, -0.0333, -0.0543, 0.0311, -0.0130, -0.0112, 0.0463, 0.0396, 0.0115, -0.0087, 0.0505, 0.0553, -0.0155, 0.0315, 0.0621, 0.0014, -0.0402, 0.0272, 0.0318, 0.0193, -0.0305, 0.0043, 0.0270, 0.0710, -0.0147, 0.0131, -0.0003, -0.0824, -0.0185, -0.0809, 0.0021, -0.0470, 0.0892, 0.0431, 0.0617, -0.0667, -0.0013, -0.0217, 0.0576, 0.0497, 0.0382, 0.0172, 0.0054, 0.1304, 0.0131, -0.0242, -0.0039, -0.0233, 0.0146, 0.0184, 0.0437, -0.0384, -0.0255, -0.1129, 0.0313, -0.0287, -0.0150, 0.0842, -0.0510, 0.0243, 0.0378, 0.0348, 0.0708, 0.0397, -0.0221, -0.0228, -0.0396, -0.0156, 0.1222, 0.0136, 0.0417, 0.0412, 0.0504, 0.0310, 0.0525, -0.0372, -0.0014, -0.0098, 0.0982, 0.0736, -0.0478, 0.0607, 0.0734, -0.0399, -0.0458]\n",
            "Token 1( açò): [0.0181, -0.0080, 0.0323, 0.0479, 0.0107, 0.0143, 0.0490, 0.0141, -0.0183, -0.0287, 0.0018, 0.0231, 0.0186, -0.0072, -0.0540, 0.0126, 0.0535, 0.0068, -0.0330, -0.0511, 0.0694, 0.0095, 0.0138, 0.0942, 0.0239, -0.0267, -0.0138, -0.0349, -0.0424, -0.0016, -0.0018, 0.0240, -0.0145, 0.0051, 0.0086, -0.1057, -0.0492, -0.0270, -0.0673, -0.0352, -0.0218, -0.0189, 0.0233, -0.0689, -0.0293, -0.0465, 0.0059, 0.0108, 0.0012, 0.0260, 0.0159, 0.0028, -0.0040, 0.0150, 0.0675, 0.0037, 0.0384, 0.0673, 0.0029, -0.0331, 0.0369, 0.0136, 0.0176, -0.0148, -0.0072, 0.0070, 0.0327, -0.0165, -0.0044, 0.0053, -0.0395, 0.0113, -0.0670, -0.0001, -0.0275, 0.0560, 0.0268, 0.0602, -0.0430, 0.0089, -0.0237, 0.0421, 0.0302, 0.0145, 0.0033, 0.0139, 0.0949, -0.0079, -0.0157, -0.0074, -0.0159, -0.0038, 0.0145, 0.0388, -0.0260, -0.0010, -0.0951, 0.0023, -0.0236, -0.0312, 0.0619, -0.0426, 0.0329, 0.0048, 0.0058, 0.0630, 0.0634, -0.0331, -0.0028, -0.0400, -0.0218, 0.0915, -0.0098, 0.0277, 0.0131, 0.0283, 0.0178, 0.0551, -0.0163, 0.0056, -0.0192, 0.0473, 0.0683, 0.0589, 0.0130, 0.0520, -0.0127, -0.0392]\n",
            "Token 2(  es): [0.0204, -0.0154, 0.0346, 0.0343, 0.0157, 0.0193, 0.0554, 0.0131, -0.0247, -0.0332, 0.0077, 0.0210, 0.0146, -0.0010, -0.0607, 0.0257, 0.0589, 0.0036, -0.0330, -0.0418, 0.0686, 0.0144, 0.0175, 0.0920, 0.0319, -0.0634, -0.0102, -0.0349, -0.0442, -0.0029, -0.0037, 0.0274, -0.0141, 0.0006, 0.0144, -0.1004, -0.0501, -0.0239, -0.0638, -0.0384, -0.0199, -0.0171, 0.0216, -0.0557, -0.0290, -0.0392, 0.0107, 0.0080, -0.0015, 0.0249, 0.0190, 0.0061, -0.0093, 0.0232, 0.0509, -0.0016, 0.0288, 0.0551, 0.0084, -0.0283, 0.0287, 0.0140, 0.0165, -0.0198, 0.0046, 0.0175, 0.0360, -0.0117, 0.0028, 0.0044, -0.0478, -0.0010, -0.0596, -0.0001, -0.0334, 0.0571, 0.0250, 0.0524, -0.0433, 0.0044, -0.0115, 0.0438, 0.0296, 0.0232, 0.0134, 0.0087, 0.0907, 0.0045, -0.0132, -0.0026, -0.0158, 0.0022, 0.0170, 0.0324, -0.0260, -0.0110, -0.0818, 0.0090, -0.0216, -0.0197, 0.0623, -0.0376, 0.0248, 0.0170, 0.0112, 0.0544, 0.0419, -0.0274, -0.0053, -0.0226, -0.0145, 0.0845, 0.0010, 0.0337, 0.0202, 0.0339, 0.0153, 0.0474, -0.0230, -0.0006, -0.0138, 0.0633, 0.0603, 0.0294, 0.0304, 0.0448, -0.0253, -0.0371]\n",
            "Token 3(  or): [0.0231, -0.0184, 0.0326, 0.0267, 0.0161, 0.0174, 0.0516, 0.0130, -0.0271, -0.0326, 0.0037, 0.0120, 0.0145, -0.0000, -0.0600, 0.0234, 0.0557, -0.0000, -0.0250, -0.0361, 0.0647, 0.0151, 0.0195, 0.0851, 0.0263, -0.0705, -0.0125, -0.0281, -0.0374, -0.0058, -0.0066, 0.0260, -0.0125, -0.0016, 0.0165, -0.0891, -0.0406, -0.0233, -0.0563, -0.0321, -0.0255, -0.0167, 0.0141, -0.0533, -0.0305, -0.0329, 0.0104, 0.0103, -0.0025, 0.0283, 0.0160, 0.0097, -0.0042, 0.0258, 0.0482, -0.0069, 0.0229, 0.0486, 0.0119, -0.0271, 0.0182, 0.0144, 0.0161, -0.0196, 0.0100, 0.0183, 0.0328, -0.0131, 0.0072, 0.0046, -0.0477, -0.0038, -0.0573, -0.0037, -0.0267, 0.0548, 0.0278, 0.0419, -0.0424, 0.0026, -0.0159, 0.0399, 0.0260, 0.0253, 0.0188, 0.0084, 0.0783, 0.0064, -0.0082, 0.0018, -0.0143, 0.0085, 0.0109, 0.0279, -0.0227, -0.0163, -0.0770, 0.0072, -0.0141, -0.0235, 0.0496, -0.0310, 0.0231, 0.0187, 0.0168, 0.0550, 0.0385, -0.0237, -0.0104, -0.0169, -0.0110, 0.0793, 0.0038, 0.0346, 0.0215, 0.0307, 0.0175, 0.0445, -0.0272, 0.0064, -0.0121, 0.0598, 0.0536, 0.0329, 0.0338, 0.0420, -0.0230, -0.0358]\n",
            "Token 4( ...): [0.0193, -0.0116, 0.0276, 0.0176, 0.0131, 0.0152, 0.0361, 0.0204, -0.0232, -0.0275, -0.0002, 0.0017, 0.0130, 0.0037, -0.0474, 0.0193, 0.0556, 0.0068, -0.0139, -0.0344, 0.0553, 0.0105, 0.0215, 0.0722, 0.0179, -0.0688, -0.0142, -0.0225, -0.0255, -0.0064, -0.0058, 0.0240, -0.0138, -0.0043, 0.0188, -0.0666, -0.0348, -0.0178, -0.0430, -0.0257, -0.0249, -0.0145, 0.0131, -0.0469, -0.0284, -0.0192, 0.0036, 0.0173, 0.0054, 0.0229, 0.0086, 0.0067, -0.0014, 0.0125, 0.0458, -0.0028, 0.0170, 0.0410, 0.0188, -0.0211, 0.0151, 0.0130, 0.0184, -0.0182, 0.0081, 0.0107, 0.0190, -0.0155, 0.0075, 0.0052, -0.0325, 0.0063, -0.0492, -0.0053, -0.0196, 0.0456, 0.0209, 0.0330, -0.0314, 0.0058, -0.0164, 0.0352, 0.0234, 0.0182, 0.0192, 0.0145, 0.0543, 0.0059, -0.0043, 0.0003, -0.0121, 0.0139, 0.0095, 0.0198, -0.0232, -0.0152, -0.0613, 0.0039, -0.0084, -0.0264, 0.0409, -0.0284, 0.0231, 0.0143, 0.0098, 0.0451, 0.0370, -0.0214, -0.0035, -0.0115, -0.0097, 0.0668, 0.0007, 0.0241, 0.0116, 0.0234, 0.0144, 0.0381, -0.0235, 0.0070, -0.0096, 0.0456, 0.0448, 0.0515, 0.0225, 0.0308, -0.0159, -0.0294]\n",
            "\n",
            "HEAD 5  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0429, -0.0179, -0.0109, 0.0065, -0.0191, 0.0015, -0.0085, -0.0321, 0.0119, 0.0308, 0.0113, 0.0059, -0.0289, 0.0423, 0.0151, 0.0074, -0.0384, 0.0037, -0.0305, 0.0015, -0.0040, 0.0103, -0.0560, 0.0235, 0.0191, -0.0373, -0.0335, -0.0556, -0.0582, 0.0301, -0.0319, 0.0070, -0.0166, 0.0305, 0.0567, 0.0362, 0.0125, 0.0276, -0.0410, -0.0074, 0.0592, 0.0183, -0.0192, -0.0051, 0.0118, 0.0172, -0.0361, 0.0137, -0.0272, 0.0263, 0.0320, -0.0083, -0.0086, -0.0123, -0.0059, 0.0086, 0.0198, -0.0217, 0.0067, -0.0164, -0.0423, 0.0467, 0.0434, 0.0203, -0.0385, 0.0447, -0.0076, -0.0229, -0.0403, 0.0496, 0.0166, -0.0098, 0.0369, -0.0623, -0.0457, -0.0143, -0.0256, -0.0392, 0.0264, -0.0000, 0.0254, 0.0334, -0.0010, 0.0080, -0.0341, -0.0262, 0.0178, 0.0157, -0.0243, 0.0067, 0.0442, 0.0192, 0.0506, -0.0183, 0.0356, 0.0331, 0.0869, 0.0204, -0.0065, 0.0286, -0.0079, 0.0066, -0.0190, -0.0041, -0.0381, -0.0184, -0.0527, 0.0158, 0.0007, 0.0062, 0.0185, 0.0118, 0.0339, 0.0244, 0.0137, -0.0148, 0.0298, 0.0210, 0.0185, 0.0420, 0.0194, -0.0270, -0.0173, 0.0514, -0.0193, -0.0056, 0.0338, -0.0442]\n",
            "Token 1( açò): [0.0320, -0.0126, -0.0018, 0.0064, -0.0076, 0.0104, -0.0167, -0.0267, 0.0146, 0.0361, 0.0096, 0.0054, -0.0198, 0.0377, 0.0143, 0.0124, -0.0257, 0.0013, -0.0222, 0.0122, -0.0056, 0.0120, -0.0426, 0.0189, 0.0137, -0.0267, -0.0264, -0.0504, -0.0437, 0.0279, -0.0293, 0.0051, -0.0177, 0.0228, 0.0545, 0.0289, 0.0130, 0.0234, -0.0302, -0.0115, 0.0531, 0.0126, -0.0196, -0.0022, 0.0054, 0.0104, -0.0261, 0.0096, -0.0295, 0.0215, 0.0209, -0.0111, -0.0069, -0.0110, -0.0083, 0.0083, 0.0192, -0.0131, 0.0065, -0.0062, -0.0326, 0.0423, 0.0404, 0.0167, -0.0348, 0.0360, -0.0035, -0.0194, -0.0254, 0.0413, 0.0095, -0.0096, 0.0377, -0.0555, -0.0466, -0.0114, -0.0203, -0.0230, 0.0214, -0.0009, 0.0250, 0.0297, -0.0006, 0.0100, -0.0229, -0.0223, 0.0152, 0.0077, -0.0207, 0.0025, 0.0344, 0.0169, 0.0442, -0.0185, 0.0254, 0.0344, 0.0143, 0.0192, -0.0001, 0.0279, -0.0103, 0.0046, -0.0140, -0.0001, -0.0288, -0.0169, -0.0405, 0.0117, -0.0035, 0.0007, 0.0200, 0.0106, 0.0289, 0.0188, 0.0121, -0.0184, 0.0193, 0.0122, 0.0147, 0.0363, 0.0129, -0.0228, -0.0139, 0.0406, -0.0144, -0.0066, 0.0311, -0.0386]\n",
            "Token 2(  es): [0.0116, -0.0008, 0.0150, 0.0058, 0.0056, 0.0175, -0.0124, -0.0168, 0.0168, 0.0386, 0.0043, 0.0084, -0.0054, 0.0221, 0.0093, 0.0195, -0.0093, -0.0020, -0.0131, 0.0186, -0.0064, 0.0149, -0.0233, 0.0059, 0.0065, -0.0074, -0.0144, -0.0310, -0.0159, 0.0158, -0.0221, 0.0014, -0.0205, 0.0067, 0.0370, 0.0160, 0.0122, 0.0126, -0.0107, -0.0159, 0.0342, -0.0012, -0.0153, -0.0019, -0.0062, -0.0062, -0.0080, 0.0001, -0.0224, 0.0123, 0.0023, -0.0159, -0.0027, -0.0048, -0.0031, 0.0087, 0.0159, -0.0033, 0.0022, 0.0063, -0.0190, 0.0241, 0.0299, 0.0107, -0.0240, 0.0186, 0.0033, -0.0127, -0.0107, 0.0232, -0.0010, -0.0135, 0.0334, -0.0362, -0.0429, -0.0090, -0.0091, -0.0061, 0.0070, 0.0003, 0.0184, 0.0185, -0.0073, 0.0112, -0.0007, -0.0164, 0.0044, 0.0005, -0.0175, -0.0053, 0.0183, 0.0072, 0.0281, -0.0147, 0.0115, 0.0319, -0.1168, 0.0134, 0.0087, 0.0235, -0.0161, -0.0044, -0.0012, 0.0032, -0.0095, -0.0107, -0.0183, 0.0059, -0.0076, -0.0031, 0.0159, 0.0115, 0.0145, 0.0112, 0.0170, -0.0180, 0.0060, 0.0002, 0.0035, 0.0179, 0.0016, -0.0194, -0.0035, 0.0186, -0.0019, -0.0079, 0.0206, -0.0285]\n",
            "Token 3(  or): [0.0179, -0.0019, 0.0067, 0.0050, 0.0043, 0.0141, -0.0087, -0.0122, 0.0092, 0.0316, 0.0039, 0.0067, -0.0065, 0.0257, 0.0084, 0.0135, -0.0175, -0.0051, -0.0091, 0.0125, -0.0063, 0.0111, -0.0263, 0.0089, 0.0059, -0.0162, -0.0137, -0.0384, -0.0274, 0.0172, -0.0201, 0.0001, -0.0181, 0.0151, 0.0380, 0.0149, 0.0137, 0.0136, -0.0184, -0.0114, 0.0384, 0.0057, -0.0148, -0.0108, -0.0026, -0.0019, -0.0118, 0.0102, -0.0198, 0.0154, 0.0060, -0.0125, -0.0039, -0.0106, 0.0043, 0.0108, 0.0151, -0.0045, 0.0023, -0.0040, -0.0232, 0.0224, 0.0281, 0.0144, -0.0237, 0.0252, 0.0023, -0.0119, -0.0168, 0.0236, 0.0051, -0.0071, 0.0265, -0.0334, -0.0330, -0.0072, -0.0064, -0.0167, 0.0135, -0.0010, 0.0171, 0.0161, -0.0012, 0.0116, -0.0082, -0.0196, 0.0095, 0.0035, -0.0162, 0.0058, 0.0224, 0.0075, 0.0284, -0.0080, 0.0204, 0.0236, -0.0753, 0.0132, 0.0131, 0.0214, -0.0074, 0.0002, -0.0055, -0.0041, -0.0097, -0.0098, -0.0261, -0.0017, -0.0064, 0.0023, 0.0162, 0.0101, 0.0241, 0.0136, 0.0157, -0.0137, 0.0131, 0.0133, 0.0087, 0.0177, -0.0034, -0.0167, -0.0062, 0.0203, -0.0026, -0.0058, 0.0233, -0.0297]\n",
            "Token 4( ...): [0.0170, 0.0015, 0.0067, 0.0029, 0.0097, 0.0077, -0.0002, -0.0039, 0.0029, 0.0230, -0.0008, 0.0044, -0.0049, 0.0171, 0.0060, 0.0064, -0.0169, -0.0087, -0.0043, 0.0109, -0.0065, 0.0065, -0.0208, 0.0071, -0.0001, -0.0155, -0.0062, -0.0350, -0.0261, 0.0101, -0.0131, -0.0008, -0.0154, 0.0106, 0.0285, 0.0087, 0.0140, 0.0111, -0.0135, -0.0083, 0.0285, 0.0034, -0.0109, -0.0153, -0.0033, -0.0059, -0.0067, 0.0104, -0.0129, 0.0121, 0.0046, -0.0111, -0.0063, -0.0088, 0.0133, 0.0107, 0.0138, -0.0025, -0.0012, -0.0107, -0.0260, 0.0127, 0.0235, 0.0121, -0.0160, 0.0210, 0.0065, -0.0073, -0.0154, 0.0154, 0.0052, -0.0068, 0.0189, -0.0239, -0.0228, -0.0059, -0.0003, -0.0199, 0.0099, -0.0004, 0.0128, 0.0112, 0.0003, 0.0124, -0.0033, -0.0215, 0.0057, 0.0060, -0.0104, 0.0112, 0.0195, 0.0004, 0.0164, -0.0014, 0.0209, 0.0163, -0.1067, 0.0083, 0.0187, 0.0159, -0.0026, -0.0026, -0.0010, -0.0045, -0.0021, -0.0050, -0.0211, -0.0088, -0.0065, 0.0065, 0.0116, 0.0095, 0.0256, 0.0128, 0.0139, -0.0128, 0.0116, 0.0159, 0.0087, 0.0076, -0.0093, -0.0134, -0.0049, 0.0115, -0.0013, -0.0015, 0.0183, -0.0264]\n",
            "\n",
            "HEAD 6  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0137, 0.0080, -0.0237, 0.0203, 0.0457, -0.0550, -0.0337, -0.0210, 0.0034, 0.0230, -0.0106, 0.0489, 0.0458, 0.0026, -0.0006, -0.0319, -0.0073, -0.0139, -0.0402, 0.0432, 0.0131, -0.0196, 0.0178, 0.0868, -0.0280, 0.0066, -0.0154, -0.0950, -0.0445, -0.0329, 0.0157, -0.0258, -0.0374, -0.0414, 0.0055, 0.0018, 0.0316, 0.0247, 0.0244, 0.0332, 0.0334, -0.0201, -0.0257, -0.0182, -0.0344, 0.0531, 0.0240, -0.0031, -0.0365, 0.0279, 0.0039, -0.0460, -0.0138, -0.0035, -0.0246, -0.0191, -0.0275, -0.0409, -0.0339, -0.0039, 0.0118, 0.0337, 0.0427, 0.0922, -0.0504, 0.0117, 0.0087, -0.0035, 0.0444, 0.0249, -0.0034, -0.0242, 0.0166, -0.0017, -0.0351, -0.0057, -0.0211, 0.0220, 0.0111, 0.0040, -0.0161, -0.0115, -0.0214, 0.0154, -0.0330, 0.0168, 0.0002, -0.0092, 0.0007, 0.0167, 0.0224, 0.0023, 0.0288, -0.0171, 0.0161, -0.0218, -0.0303, 0.0090, 0.0009, -0.0059, 0.0047, -0.0149, -0.0364, -0.0213, 0.0151, 0.0148, 0.0186, -0.0135, 0.0105, 0.0293, 0.1226, 0.0057, -0.0366, 0.0019, -0.0199, 0.0339, 0.0305, -0.0020, -0.0159, 0.0146, -0.0081, 0.0179, -0.0184, -0.0295, -0.0021, -0.0045, -0.0225, 0.0009]\n",
            "Token 1( açò): [0.0091, 0.0006, 0.0052, 0.0179, -0.0037, -0.0319, -0.0023, 0.0118, 0.0274, 0.0156, 0.0241, 0.0069, 0.0423, -0.0051, 0.0025, -0.0230, 0.0096, -0.0289, -0.0046, 0.0220, -0.0098, 0.0136, 0.0140, 0.0459, 0.0132, -0.0079, 0.0060, 0.0064, -0.0649, 0.0013, 0.0266, -0.0320, -0.0089, -0.0282, -0.0070, 0.0151, 0.0292, -0.0267, -0.0057, 0.0156, 0.0061, 0.0041, 0.0233, 0.0007, -0.0093, 0.0349, 0.0139, 0.0189, -0.0091, 0.0071, 0.0374, -0.0152, -0.0266, 0.0049, -0.0026, 0.0288, 0.0087, -0.0048, -0.0273, 0.0156, -0.0141, -0.0061, 0.0348, 0.0691, -0.0043, 0.0155, 0.0515, 0.0285, 0.0304, 0.0315, -0.0004, 0.0016, 0.0022, -0.0118, 0.0068, -0.0087, 0.0230, -0.1010, -0.0154, -0.0004, -0.0014, -0.0023, 0.0005, -0.0099, -0.0472, 0.0095, -0.0072, -0.0002, -0.0142, 0.0014, 0.0228, 0.0342, 0.0202, 0.0397, -0.0109, 0.0158, 0.0445, -0.0005, -0.0429, -0.0292, -0.0177, 0.0116, 0.0084, 0.0074, 0.0031, 0.0070, -0.0393, -0.0053, 0.0050, 0.0307, -0.1398, -0.0048, 0.0065, -0.0192, 0.0439, -0.0048, 0.0172, 0.0067, 0.0138, -0.0090, -0.0331, 0.0164, 0.0206, -0.0205, -0.0252, 0.0099, -0.0255, 0.0026]\n",
            "Token 2(  es): [0.0025, 0.0059, -0.0111, 0.0127, 0.0261, -0.0243, -0.0105, 0.0059, 0.0152, 0.0215, 0.0042, 0.0221, 0.0377, 0.0109, 0.0017, -0.0214, 0.0039, -0.0065, -0.0212, 0.0251, 0.0010, -0.0083, 0.0069, 0.0503, -0.0100, -0.0051, -0.0021, -0.0087, -0.0411, -0.0184, 0.0308, -0.0283, -0.0219, -0.0125, 0.0034, 0.0043, 0.0425, 0.0074, 0.0048, 0.0115, 0.0119, -0.0155, -0.0052, 0.0008, -0.0168, 0.0320, 0.0127, -0.0030, -0.0118, 0.0289, 0.0282, -0.0220, -0.0015, -0.0017, -0.0081, 0.0009, -0.0231, -0.0126, -0.0168, 0.0025, 0.0034, 0.0190, 0.0180, 0.0624, -0.0120, 0.0130, 0.0134, 0.0011, 0.0213, 0.0117, -0.0097, -0.0233, 0.0062, -0.0121, -0.0172, 0.0038, -0.0035, -0.0517, 0.0048, -0.0068, -0.0047, -0.0052, -0.0036, 0.0048, -0.0299, 0.0017, -0.0059, 0.0010, -0.0053, 0.0115, 0.0027, 0.0013, 0.0138, 0.0121, -0.0032, 0.0015, -0.0078, 0.0010, -0.0085, -0.0065, -0.0043, 0.0034, -0.0093, -0.0053, 0.0109, 0.0017, 0.0032, -0.0118, 0.0045, 0.0152, -0.0093, 0.0089, -0.0220, -0.0001, 0.0016, 0.0001, 0.0102, -0.0032, -0.0032, 0.0053, -0.0112, 0.0116, -0.0091, -0.0124, -0.0128, 0.0026, -0.0196, -0.0031]\n",
            "Token 3(  or): [-0.0015, 0.0006, 0.0005, 0.0024, 0.0186, -0.0068, -0.0018, -0.0086, 0.0029, 0.0126, -0.0008, 0.0089, 0.0289, -0.0027, 0.0050, -0.0197, 0.0013, -0.0088, -0.0076, 0.0214, 0.0035, -0.0015, -0.0015, 0.0336, -0.0036, -0.0200, -0.0092, 0.0340, -0.0170, -0.0134, 0.0106, -0.0139, -0.0054, -0.0036, -0.0031, -0.0018, 0.0292, 0.0052, 0.0057, 0.0178, 0.0091, -0.0048, 0.0110, -0.0004, -0.0095, 0.0250, 0.0109, 0.0010, -0.0021, 0.0183, 0.0145, -0.0195, -0.0081, 0.0003, -0.0019, 0.0037, -0.0165, -0.0032, -0.0066, 0.0049, -0.0068, 0.0147, 0.0083, 0.0286, -0.0024, 0.0059, 0.0077, -0.0018, 0.0163, 0.0019, -0.0081, -0.0214, 0.0001, -0.0042, -0.0103, 0.0057, -0.0077, -0.0318, 0.0007, -0.0019, 0.0050, -0.0007, -0.0045, -0.0013, -0.0194, 0.0036, -0.0061, 0.0012, 0.0001, 0.0054, 0.0159, 0.0015, 0.0189, 0.0109, -0.0065, 0.0016, -0.0065, -0.0068, -0.0118, -0.0000, -0.0091, 0.0044, -0.0124, -0.0016, -0.0053, -0.0065, -0.0094, -0.0055, 0.0011, 0.0083, -0.0822, 0.0051, -0.0073, -0.0024, 0.0018, 0.0034, 0.0086, -0.0117, 0.0022, 0.0020, -0.0120, 0.0084, -0.0086, -0.0095, -0.0120, 0.0062, -0.0136, -0.0010]\n",
            "Token 4( ...): [-0.0089, 0.0067, -0.0053, 0.0167, 0.0295, -0.0167, -0.0100, -0.0206, 0.0001, 0.0130, -0.0053, 0.0226, 0.0227, -0.0009, 0.0037, -0.0195, -0.0033, -0.0060, -0.0121, 0.0175, 0.0085, -0.0157, 0.0080, 0.0407, -0.0093, -0.0105, -0.0087, -0.0084, -0.0147, -0.0141, 0.0040, -0.0198, -0.0141, -0.0073, -0.0040, -0.0008, 0.0327, 0.0097, 0.0102, 0.0152, 0.0150, -0.0143, -0.0002, -0.0031, -0.0165, 0.0247, 0.0111, -0.0032, -0.0094, 0.0193, 0.0089, -0.0305, -0.0114, -0.0053, -0.0106, -0.0007, -0.0167, -0.0088, -0.0145, 0.0050, -0.0006, 0.0250, 0.0136, 0.0345, -0.0102, 0.0072, 0.0119, -0.0048, 0.0188, 0.0038, -0.0064, -0.0228, 0.0054, -0.0005, -0.0174, 0.0080, -0.0098, 0.0149, 0.0062, 0.0018, -0.0008, -0.0022, -0.0113, 0.0004, -0.0178, 0.0061, -0.0041, -0.0036, 0.0039, 0.0084, 0.0101, 0.0026, 0.0175, -0.0020, -0.0103, -0.0057, -0.0142, -0.0058, -0.0042, -0.0028, 0.0017, -0.0072, -0.0124, -0.0015, 0.0036, 0.0064, -0.0010, -0.0072, -0.0008, 0.0097, -0.0545, 0.0087, -0.0204, -0.0046, -0.0134, 0.0100, 0.0129, -0.0116, -0.0127, 0.0050, -0.0126, 0.0004, -0.0094, -0.0171, -0.0124, 0.0001, -0.0144, -0.0045]\n",
            "\n",
            "HEAD 7  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0055, 0.0139, -0.0012, -0.0055, 0.0251, -0.0036, 0.0015, -0.0022, -0.0732, 0.0079, -0.0148, 0.0687, 0.0036, 0.0005, 0.0201, 0.0075, -0.0117, 0.0190, 0.0304, -0.0223, 0.0071, -0.0002, 0.0273, -0.0095, -0.0012, 0.0283, 0.0040, 0.0132, -0.0066, 0.0220, 0.0036, 0.0009, 0.0129, 0.0181, -0.0057, -0.0031, 0.0045, 0.0143, 0.0093, -0.0031, 0.0114, 0.0489, -0.0122, 0.0292, -0.0012, 0.0085, 0.0165, -0.0084, 0.0140, 0.0126, -0.0089, -0.0227, -0.0112, -0.0071, 0.0103, -0.0059, 0.0055, 0.0089, -0.0063, -0.0031, -0.0033, -0.0002, -0.0066, -0.0092, -0.0054, -0.0083, 0.0177, -0.0101, 0.0079, 0.0222, 0.0193, 0.0034, 0.0088, -0.0045, 0.0040, 0.0077, 0.0002, 0.0144, -0.0081, -0.0013, -0.0100, -0.0006, -0.0259, -0.0131, 0.0034, 0.0045, 0.0063, 0.1089, -0.0027, -0.0056, -0.0120, -0.0134, 0.0021, 0.0010, -0.0113, 0.0281, -0.0135, -0.0065, 0.0098, 0.0206, -0.0175, 0.0132, 0.0058, 0.0153, 0.0041, 0.0014, 0.0211, -0.0076, 0.0025, 0.0004, 0.0026, 0.0061, 0.0317, 0.0028, -0.0218, -0.0232, 0.0152, 0.0148, -0.0182, -0.0057, 0.0162, -0.0056, 0.0038, -0.0057, -0.0133, 0.0034, 0.0156, -0.0053]\n",
            "Token 1( açò): [-0.0005, 0.0099, -0.0024, 0.0154, 0.0287, 0.0005, -0.0013, -0.0032, -0.0664, 0.0068, -0.0234, 0.0588, 0.0042, -0.0018, 0.0190, 0.0147, -0.0079, 0.0174, 0.0318, -0.0191, 0.0067, -0.0009, 0.0263, -0.0110, -0.0015, 0.0233, 0.0078, 0.0159, -0.0061, 0.0199, 0.0044, 0.0003, 0.0156, 0.0171, -0.0051, -0.0032, 0.0060, 0.0061, 0.0069, -0.0004, 0.0122, 0.0443, -0.0092, 0.0455, 0.0027, 0.0089, 0.0162, -0.0044, 0.0099, 0.0100, -0.0018, -0.0198, -0.0092, -0.0016, 0.0101, -0.0027, 0.0098, 0.0045, -0.0055, -0.0061, -0.0064, -0.0013, -0.0098, -0.0060, -0.0066, -0.0057, 0.0195, -0.0118, 0.0094, 0.0218, 0.0204, -0.0003, 0.0086, 0.0010, 0.0088, 0.0065, 0.0001, 0.0139, -0.0094, 0.0009, -0.0044, -0.0002, -0.0299, -0.0161, 0.0053, 0.0091, -0.0011, 0.0971, 0.0001, -0.0018, -0.0156, -0.0104, -0.0002, 0.0042, -0.0055, 0.0266, -0.0049, -0.0105, 0.0148, 0.0216, -0.0131, 0.0129, 0.0060, 0.0161, 0.0070, 0.0023, 0.0196, 0.0008, -0.0001, -0.0007, 0.0045, 0.0078, 0.0363, 0.0088, -0.0211, -0.0158, 0.0161, 0.0070, -0.0197, -0.0076, 0.0169, -0.0049, -0.0012, -0.0013, -0.0083, 0.0086, 0.0159, -0.0031]\n",
            "Token 2(  es): [0.0402, -0.0230, -0.0126, 0.1881, 0.0589, 0.0342, -0.0245, -0.0116, -0.0098, -0.0023, -0.0949, -0.0239, 0.0088, -0.0211, 0.0095, 0.0735, 0.0229, 0.0039, 0.0432, 0.0073, 0.0038, -0.0068, 0.0180, -0.0229, -0.0035, -0.0185, 0.0386, 0.0378, -0.0016, 0.0032, 0.0108, -0.0047, 0.0382, 0.0091, -0.0003, -0.0043, 0.0181, -0.0622, -0.0131, 0.0213, 0.0184, 0.0065, 0.0158, 0.1805, 0.0352, 0.0121, 0.0135, 0.0280, -0.0243, -0.0114, 0.0566, 0.0042, 0.0076, 0.0439, 0.0081, 0.0234, 0.0450, -0.0317, 0.0008, -0.0309, -0.0313, -0.0112, -0.0362, 0.0201, -0.0163, 0.0159, 0.0344, -0.0262, 0.0214, 0.0174, 0.0297, -0.0311, 0.0063, 0.0470, 0.0484, -0.0038, -0.0007, 0.0102, -0.0199, 0.0188, 0.0418, 0.0040, -0.0633, -0.0411, 0.0205, 0.0469, -0.0617, -0.0010, 0.0232, 0.0303, -0.0448, 0.0143, -0.0186, 0.0305, 0.0420, 0.0135, 0.0664, -0.0439, 0.0562, 0.0292, 0.0238, 0.0106, 0.0077, 0.0217, 0.0315, 0.0099, 0.0064, 0.0712, -0.0215, -0.0101, 0.0195, 0.0223, 0.0745, 0.0579, -0.0155, 0.0459, 0.0228, -0.0569, -0.0327, -0.0234, 0.0229, 0.0010, -0.0422, 0.0353, 0.0327, 0.0512, 0.0188, 0.0158]\n",
            "Token 3(  or): [0.0067, 0.0033, -0.0049, 0.0485, 0.0348, 0.0071, -0.0057, -0.0050, -0.0537, 0.0050, -0.0386, 0.0413, 0.0049, -0.0053, 0.0165, 0.0257, -0.0024, 0.0141, 0.0331, -0.0141, 0.0063, -0.0013, 0.0241, -0.0126, -0.0016, 0.0150, 0.0134, 0.0195, -0.0052, 0.0164, 0.0050, -0.0001, 0.0201, 0.0156, -0.0044, -0.0031, 0.0086, -0.0075, 0.0029, 0.0033, 0.0132, 0.0359, -0.0044, 0.0721, 0.0089, 0.0097, 0.0154, 0.0020, 0.0027, 0.0063, 0.0095, -0.0145, -0.0061, 0.0074, 0.0099, 0.0024, 0.0163, -0.0026, -0.0040, -0.0109, -0.0114, -0.0033, -0.0146, -0.0010, -0.0082, -0.0009, 0.0226, -0.0144, 0.0111, 0.0202, 0.0221, -0.0060, 0.0082, 0.0096, 0.0158, 0.0047, 0.0001, 0.0130, -0.0113, 0.0043, 0.0047, 0.0008, -0.0358, -0.0208, 0.0083, 0.0164, -0.0125, 0.0759, 0.0048, 0.0045, -0.0210, -0.0054, -0.0036, 0.0091, 0.0036, 0.0236, 0.0086, -0.0168, 0.0226, 0.0229, -0.0052, 0.0119, 0.0057, 0.0160, 0.0116, 0.0036, 0.0162, 0.0147, -0.0046, -0.0028, 0.0072, 0.0101, 0.0420, 0.0182, -0.0199, -0.0035, 0.0169, -0.0056, -0.0222, -0.0104, 0.0181, -0.0037, -0.0088, 0.0056, 0.0008, 0.0165, 0.0161, 0.0013]\n",
            "Token 4( ...): [-0.0045, 0.0130, -0.0013, -0.0019, 0.0257, -0.0027, 0.0008, -0.0024, -0.0711, 0.0076, -0.0178, 0.0663, 0.0037, 0.0001, 0.0197, 0.0086, -0.0110, 0.0185, 0.0302, -0.0216, 0.0070, -0.0003, 0.0269, -0.0097, -0.0012, 0.0273, 0.0046, 0.0137, -0.0067, 0.0216, 0.0035, 0.0010, 0.0136, 0.0178, -0.0057, -0.0029, 0.0049, 0.0125, 0.0089, -0.0025, 0.0115, 0.0473, -0.0115, 0.0326, -0.0005, 0.0086, 0.0163, -0.0074, 0.0131, 0.0122, -0.0076, -0.0220, -0.0108, -0.0060, 0.0102, -0.0054, 0.0062, 0.0080, -0.0060, -0.0037, -0.0040, -0.0004, -0.0073, -0.0085, -0.0055, -0.0078, 0.0181, -0.0104, 0.0081, 0.0220, 0.0194, 0.0028, 0.0087, -0.0035, 0.0048, 0.0076, 0.0002, 0.0143, -0.0083, -0.0010, -0.0088, -0.0005, -0.0263, -0.0137, 0.0039, 0.0052, 0.0048, 0.1061, -0.0021, -0.0047, -0.0125, -0.0126, 0.0015, 0.0017, -0.0101, 0.0278, -0.0118, -0.0075, 0.0106, 0.0206, -0.0165, 0.0131, 0.0056, 0.0151, 0.0048, 0.0014, 0.0208, -0.0061, 0.0018, 0.0001, 0.0030, 0.0063, 0.0323, 0.0042, -0.0216, -0.0216, 0.0152, 0.0131, -0.0184, -0.0060, 0.0164, -0.0054, 0.0031, -0.0049, -0.0125, 0.0044, 0.0157, -0.0047]\n",
            "\n",
            "HEAD 8  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0426, -0.0096, -0.0343, 0.0244, -0.0478, -0.0356, 0.0070, -0.0133, 0.0226, 0.0381, 0.0267, -0.0337, 0.0052, -0.0321, 0.0465, 0.0408, -0.0201, -0.0082, 0.1777, 0.0133, -0.0273, 0.0305, -0.0170, -0.0181, 0.0003, -0.0178, -0.0150, 0.0299, 0.0174, 0.0020, 0.0276, 0.0131, -0.0018, 0.0175, 0.0361, -0.0124, 0.0980, 0.0267, 0.0003, 0.0670, 0.0405, -0.0489, -0.0274, -0.0258, 0.0312, -0.0348, -0.0237, 0.0058, -0.0261, 0.0438, 0.0029, -0.0107, 0.0158, 0.0279, -0.0304, -0.0152, 0.0388, -0.0024, 0.0142, -0.0005, 0.0241, 0.0438, -0.0350, 0.0179, 0.0080, 0.0016, 0.0113, -0.0008, 0.0144, 0.0534, -0.0531, -0.0081, 0.0383, -0.0376, 0.0016, 0.1004, -0.0007, -0.0236, 0.0716, -0.0640, 0.0194, 0.0254, 0.0323, 0.0060, -0.0135, -0.0411, -0.0492, 0.0072, -0.0336, -0.0192, 0.0542, 0.0818, -0.0420, 0.0201, -0.0037, -0.0138, -0.0026, 0.0002, -0.0119, -0.0106, -0.0442, -0.0095, 0.0305, -0.0138, -0.0141, 0.0267, 0.0279, -0.0399, 0.0395, -0.0810, -0.0254, -0.0475, -0.0272, 0.0032, 0.0134, -0.0181, -0.0056, -0.0121, 0.0068, 0.0228, -0.0353, 0.0179, -0.0057, 0.0463, -0.0073, -0.0316, 0.0095, 0.0244]\n",
            "Token 1( açò): [0.0380, -0.0089, -0.0264, 0.0192, -0.0403, -0.0312, 0.0082, -0.0110, 0.0237, 0.0326, 0.0235, -0.0292, 0.0041, -0.0317, 0.0398, 0.0381, -0.0239, -0.0068, 0.1285, 0.0144, -0.0225, 0.0222, -0.0142, -0.0204, 0.0019, -0.0163, -0.0090, 0.0247, 0.0119, -0.0018, 0.0200, 0.0103, -0.0050, 0.0184, 0.0301, -0.0157, 0.0831, 0.0238, 0.0005, 0.0597, 0.0351, -0.0454, -0.0266, -0.0228, 0.0261, -0.0267, -0.0196, 0.0053, -0.0229, 0.0383, 0.0009, -0.0095, 0.0128, 0.0286, -0.0259, -0.0117, 0.0339, -0.0012, 0.0152, -0.0030, 0.0201, 0.0400, -0.0352, 0.0136, 0.0084, -0.0001, 0.0077, -0.0051, 0.0090, 0.0489, -0.0448, -0.0111, 0.0381, -0.0343, 0.0003, 0.0948, -0.0027, -0.0236, 0.0639, -0.0525, 0.0183, 0.0211, 0.0299, 0.0042, -0.0119, -0.0328, -0.0472, 0.0008, -0.0290, -0.0197, 0.0479, 0.0712, -0.0392, 0.0172, -0.0041, -0.0132, 0.0001, 0.0006, -0.0083, -0.0090, -0.0366, -0.0093, 0.0266, -0.0151, -0.0139, 0.0199, 0.0191, -0.0365, 0.0389, -0.0708, -0.0238, -0.0440, -0.0209, 0.0071, 0.0083, -0.0192, 0.0011, -0.0097, 0.0015, 0.0208, -0.0327, 0.0154, -0.0011, 0.0435, -0.0072, -0.0292, 0.0084, 0.0224]\n",
            "Token 2(  es): [0.0377, -0.0071, -0.0281, 0.0205, -0.0389, -0.0323, 0.0075, -0.0093, 0.0215, 0.0330, 0.0232, -0.0296, 0.0058, -0.0300, 0.0417, 0.0372, -0.0213, -0.0058, 0.1282, 0.0130, -0.0218, 0.0254, -0.0127, -0.0180, 0.0009, -0.0147, -0.0125, 0.0251, 0.0137, -0.0005, 0.0214, 0.0090, -0.0029, 0.0176, 0.0299, -0.0121, 0.0848, 0.0246, -0.0002, 0.0581, 0.0347, -0.0435, -0.0274, -0.0222, 0.0270, -0.0279, -0.0200, 0.0035, -0.0220, 0.0373, 0.0012, -0.0097, 0.0120, 0.0258, -0.0259, -0.0122, 0.0331, -0.0010, 0.0133, -0.0038, 0.0188, 0.0379, -0.0336, 0.0135, 0.0083, -0.0013, 0.0075, -0.0021, 0.0100, 0.0472, -0.0442, -0.0100, 0.0362, -0.0338, 0.0012, 0.0918, -0.0008, -0.0251, 0.0627, -0.0523, 0.0196, 0.0227, 0.0276, 0.0032, -0.0118, -0.0348, -0.0446, 0.0032, -0.0315, -0.0175, 0.0487, 0.0698, -0.0385, 0.0179, -0.0052, -0.0145, -0.0021, -0.0005, -0.0098, -0.0088, -0.0378, -0.0085, 0.0264, -0.0133, -0.0136, 0.0234, 0.0222, -0.0353, 0.0367, -0.0702, -0.0234, -0.0424, -0.0231, 0.0072, 0.0093, -0.0159, -0.0017, -0.0090, 0.0026, 0.0199, -0.0297, 0.0177, -0.0028, 0.0416, -0.0070, -0.0272, 0.0069, 0.0219]\n",
            "Token 3(  or): [0.0368, -0.0071, -0.0288, 0.0207, -0.0396, -0.0320, 0.0066, -0.0095, 0.0224, 0.0323, 0.0228, -0.0292, 0.0060, -0.0298, 0.0418, 0.0366, -0.0212, -0.0064, 0.1283, 0.0135, -0.0223, 0.0266, -0.0134, -0.0187, 0.0001, -0.0149, -0.0130, 0.0260, 0.0138, 0.0005, 0.0221, 0.0095, -0.0022, 0.0181, 0.0297, -0.0111, 0.0851, 0.0237, 0.0003, 0.0588, 0.0347, -0.0437, -0.0272, -0.0221, 0.0263, -0.0289, -0.0210, 0.0037, -0.0234, 0.0372, 0.0018, -0.0095, 0.0122, 0.0256, -0.0264, -0.0124, 0.0334, -0.0014, 0.0136, -0.0016, 0.0189, 0.0380, -0.0333, 0.0142, 0.0076, -0.0005, 0.0077, -0.0020, 0.0108, 0.0476, -0.0460, -0.0091, 0.0364, -0.0332, 0.0006, 0.0916, -0.0006, -0.0236, 0.0634, -0.0524, 0.0191, 0.0225, 0.0276, 0.0025, -0.0120, -0.0351, -0.0438, 0.0029, -0.0321, -0.0183, 0.0486, 0.0708, -0.0381, 0.0182, -0.0056, -0.0146, -0.0027, -0.0001, -0.0099, -0.0092, -0.0377, -0.0083, 0.0264, -0.0134, -0.0131, 0.0249, 0.0224, -0.0354, 0.0369, -0.0704, -0.0233, -0.0420, -0.0233, 0.0061, 0.0098, -0.0149, -0.0011, -0.0097, 0.0035, 0.0194, -0.0301, 0.0176, -0.0033, 0.0420, -0.0062, -0.0273, 0.0070, 0.0214]\n",
            "Token 4( ...): [0.0341, -0.0063, -0.0275, 0.0217, -0.0423, -0.0317, 0.0054, -0.0089, 0.0213, 0.0330, 0.0229, -0.0273, 0.0060, -0.0283, 0.0395, 0.0358, -0.0204, -0.0051, 0.1134, 0.0123, -0.0218, 0.0248, -0.0125, -0.0174, -0.0019, -0.0127, -0.0178, 0.0244, 0.0133, -0.0000, 0.0219, 0.0106, -0.0026, 0.0188, 0.0295, -0.0096, 0.0819, 0.0234, -0.0015, 0.0583, 0.0330, -0.0422, -0.0258, -0.0203, 0.0257, -0.0292, -0.0211, 0.0028, -0.0246, 0.0365, 0.0006, -0.0092, 0.0118, 0.0241, -0.0252, -0.0133, 0.0321, 0.0007, 0.0120, -0.0021, 0.0208, 0.0303, -0.0321, 0.0142, 0.0080, 0.0003, 0.0067, -0.0028, 0.0144, 0.0453, -0.0461, -0.0068, 0.0347, -0.0294, 0.0009, 0.0888, -0.0019, -0.0240, 0.0610, -0.0508, 0.0190, 0.0216, 0.0252, 0.0020, -0.0121, -0.0357, -0.0402, 0.0047, -0.0314, -0.0171, 0.0460, 0.0667, -0.0358, 0.0189, -0.0070, -0.0166, -0.0009, 0.0005, -0.0103, -0.0095, -0.0341, -0.0078, 0.0282, -0.0121, -0.0124, 0.0248, 0.0214, -0.0335, 0.0362, -0.0657, -0.0225, -0.0430, -0.0233, 0.0050, 0.0082, -0.0129, -0.0024, -0.0101, 0.0025, 0.0177, -0.0286, 0.0181, -0.0030, 0.0379, -0.0045, -0.0270, 0.0076, 0.0183]\n",
            "\n",
            "HEAD 9  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0082, 0.0044, -0.0388, 0.0688, -0.0155, -0.0115, 0.0115, 0.0654, -0.0244, 0.0053, 0.0018, -0.0656, -0.2784, -0.0140, -0.0084, -0.0213, -0.0501, -0.1128, -0.0044, -0.0102, 0.0006, 0.0030, -0.0028, -0.0827, 0.0802, -0.0160, 0.0056, 0.0223, -0.0234, 0.1145, 0.0095, 0.0337, 0.0387, 0.0022, 0.0051, 0.0031, 0.0525, -0.0164, -0.0345, 0.0089, -0.0272, -0.0330, -0.0132, 0.0019, 0.0195, 0.0224, -0.1362, 0.0201, 0.0272, 0.0109, 0.1376, 0.0112, -0.0089, 0.0290, 0.0076, -0.0390, 0.0390, -0.1032, -0.0040, -0.0001, -0.0326, 0.0029, 0.0285, -0.0119, -0.0068, 0.0038, -0.0639, 0.0073, -0.0255, 0.0892, -0.0094, -0.0070, -0.1912, 0.0324, 0.0044, 0.0051, -0.0011, 0.0104, 0.0325, 0.0066, -0.0412, 0.0335, 0.0107, 0.3033, -0.0347, 0.3077, 0.0069, 0.0240, -0.0177, -0.0010, 0.0330, 0.0083, -0.0235, -0.0332, 0.0038, -0.0553, -0.0496, -0.0037, -0.0268, -0.0617, -0.0153, -0.0212, 0.0197, 0.0494, -0.0094, -0.0758, -0.0527, -0.0053, 0.0151, 0.0351, 0.0119, 0.0151, 0.0172, -0.0358, 0.0389, 0.0642, 0.0308, 0.0001, -0.0109, 0.0588, -0.0568, 0.0008, -0.0115, -0.0285, 0.0363, -0.0448, 0.0119, -0.0096]\n",
            "Token 1( açò): [-0.0070, 0.0234, -0.0010, 0.0058, -0.0194, 0.0007, 0.0721, 0.0292, -0.0426, 0.0064, 0.0008, -0.0193, -0.1553, -0.0032, -0.0088, -0.0101, -0.0134, -0.0794, -0.0017, -0.0255, -0.0053, -0.0102, 0.0007, -0.0857, 0.0277, -0.0151, -0.0171, 0.0090, -0.0180, 0.0384, -0.0054, 0.0259, 0.0203, -0.0021, 0.0125, 0.0040, 0.0608, -0.0223, 0.0018, -0.0188, -0.0231, -0.0300, 0.0008, 0.0107, -0.0067, 0.0200, -0.0717, 0.0194, 0.0018, -0.0067, 0.0710, 0.0129, -0.0128, 0.0232, -0.0055, -0.0037, 0.0761, -0.0569, -0.0071, -0.0068, -0.0467, -0.0027, 0.0352, -0.0050, 0.0217, -0.0020, -0.0485, -0.0104, 0.0114, 0.0384, -0.0140, 0.0151, -0.0929, 0.0251, 0.0172, 0.0280, 0.0128, 0.0094, 0.0301, -0.0321, -0.0224, -0.0002, 0.0250, 0.1677, 0.0012, 0.1271, -0.0120, 0.0230, -0.0074, -0.0062, 0.0308, 0.0102, -0.0210, -0.0190, -0.0085, -0.0189, -0.0224, 0.0163, 0.0440, 0.0146, -0.0434, -0.0185, 0.0009, 0.0125, -0.0167, -0.0279, -0.0104, -0.0145, 0.0320, -0.0071, -0.0065, 0.0029, 0.0339, -0.0259, 0.0295, 0.0378, 0.0146, 0.0134, -0.0136, 0.0384, -0.0349, 0.0202, 0.0054, 0.0204, 0.0222, -0.0589, 0.0049, -0.0008]\n",
            "Token 2(  es): [-0.0134, 0.0018, -0.0189, 0.0361, -0.0094, 0.0060, 0.0418, 0.0337, -0.0217, 0.0011, 0.0013, -0.0217, -0.1423, -0.0033, -0.0078, -0.0244, -0.0239, -0.0699, -0.0084, -0.0038, 0.0019, -0.0056, 0.0009, -0.0387, 0.0357, -0.0170, 0.0027, 0.0127, -0.0065, 0.0540, 0.0024, 0.0204, 0.0243, -0.0010, 0.0210, 0.0026, 0.0513, -0.0116, 0.0047, -0.0153, -0.0184, -0.0198, -0.0059, 0.0066, 0.0042, 0.0159, -0.0764, 0.0243, 0.0114, 0.0025, 0.0656, 0.0166, -0.0120, 0.0075, -0.0053, -0.0193, 0.0310, -0.0431, 0.0002, -0.0209, -0.0346, -0.0037, 0.0235, -0.0140, 0.0119, -0.0027, -0.0568, -0.0091, -0.0031, 0.0544, -0.0175, -0.0080, -0.1012, 0.0185, -0.0089, 0.0280, 0.0051, 0.0207, 0.0201, -0.0086, -0.0292, 0.0143, 0.0233, 0.1628, -0.0223, 0.1455, -0.0084, 0.0283, -0.0075, -0.0004, 0.0181, 0.0098, -0.0226, -0.0245, 0.0088, -0.0202, -0.0326, 0.0068, 0.0146, -0.0173, -0.0195, -0.0056, 0.0054, 0.0087, -0.0125, -0.0697, -0.0211, 0.0013, 0.0049, 0.0031, 0.0079, 0.0015, 0.0304, -0.0171, 0.0232, 0.0442, 0.0248, 0.0105, -0.0058, 0.0370, -0.0283, 0.0123, 0.0037, -0.0025, 0.0163, -0.0326, 0.0040, -0.0068]\n",
            "Token 3(  or): [-0.0158, 0.0015, -0.0155, 0.0286, -0.0080, 0.0012, 0.0360, 0.0337, -0.0220, 0.0022, 0.0014, -0.0265, -0.1549, -0.0015, -0.0081, -0.0195, -0.0252, -0.0782, -0.0086, -0.0013, -0.0003, -0.0049, 0.0039, -0.0332, 0.0371, -0.0116, 0.0032, 0.0102, -0.0183, 0.0582, 0.0001, 0.0274, 0.0269, -0.0004, 0.0115, 0.0021, 0.0491, -0.0168, -0.0041, -0.0057, -0.0145, -0.0253, -0.0069, 0.0106, 0.0084, 0.0097, -0.0827, 0.0149, 0.0099, 0.0092, 0.0704, 0.0136, -0.0061, 0.0170, -0.0001, -0.0183, 0.0357, -0.0549, -0.0029, -0.0094, -0.0353, 0.0013, 0.0215, -0.0159, 0.0126, -0.0031, -0.0518, -0.0090, -0.0104, 0.0576, -0.0151, -0.0008, -0.1107, 0.0110, 0.0017, 0.0175, 0.0067, 0.0129, 0.0239, -0.0058, -0.0257, 0.0128, 0.0149, 0.1728, -0.0232, 0.1564, -0.0006, 0.0239, -0.0021, -0.0004, 0.0186, 0.0044, -0.0120, -0.0263, 0.0050, -0.0276, -0.0361, 0.0058, 0.0112, -0.0246, -0.0230, -0.0111, 0.0073, 0.0159, -0.0112, -0.0752, -0.0208, -0.0006, 0.0115, 0.0119, 0.0065, 0.0114, 0.0328, -0.0233, 0.0272, 0.0386, 0.0199, 0.0082, -0.0060, 0.0385, -0.0325, 0.0098, 0.0022, -0.0054, 0.0238, -0.0258, 0.0069, -0.0046]\n",
            "Token 4( ...): [-0.0103, 0.0042, -0.0351, 0.0602, -0.0139, -0.0080, 0.0153, 0.0596, -0.0228, 0.0051, 0.0021, -0.0585, -0.2471, -0.0132, -0.0082, -0.0209, -0.0447, -0.1050, -0.0051, -0.0081, 0.0009, 0.0004, -0.0022, -0.0666, 0.0684, -0.0146, 0.0059, 0.0189, -0.0203, 0.1043, 0.0068, 0.0307, 0.0369, 0.0001, 0.0065, 0.0012, 0.0489, -0.0151, -0.0276, 0.0067, -0.0243, -0.0294, -0.0092, 0.0012, 0.0167, 0.0198, -0.1170, 0.0207, 0.0226, 0.0099, 0.1193, 0.0126, -0.0082, 0.0262, 0.0058, -0.0342, 0.0351, -0.0892, -0.0038, -0.0029, -0.0314, 0.0022, 0.0260, -0.0117, -0.0028, 0.0017, -0.0446, 0.0043, -0.0213, 0.0814, -0.0100, -0.0067, -0.1705, 0.0285, 0.0027, 0.0077, 0.0003, 0.0108, 0.0305, 0.0032, -0.0383, 0.0302, 0.0123, 0.2675, -0.0314, 0.2803, 0.0050, 0.0239, -0.0145, -0.0010, 0.0301, 0.0078, -0.0214, -0.0311, 0.0048, -0.0486, -0.0509, -0.0022, -0.0202, -0.0501, -0.0152, -0.0189, 0.0161, 0.0398, -0.0085, -0.0793, -0.0435, -0.0045, 0.0125, 0.0298, 0.0098, 0.0135, 0.0173, -0.0309, 0.0357, 0.0597, 0.0261, 0.0006, -0.0091, 0.0534, -0.0488, 0.0031, -0.0090, -0.0204, 0.0326, -0.0441, 0.0114, -0.0083]\n",
            "\n",
            "HEAD 10  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0358, 0.0062, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0192, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 1( açò): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0062, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0192, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0069, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0085, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 2(  es): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0019, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0223, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0085, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0072, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0165, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0092, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 3(  or): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0359, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0186, 0.0019, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0027, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0223, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0068, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0071, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0165, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0092, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "Token 4( ...): [0.0114, -0.0048, 0.0015, 0.0257, 0.0059, -0.0027, 0.0297, -0.0001, -0.0054, 0.0060, -0.0096, 0.0082, -0.0082, 0.0212, -0.0358, 0.0063, 0.0083, -0.0186, 0.0280, -0.0064, 0.0092, -0.0178, -0.0019, 0.0099, 0.0066, 0.0100, 0.0099, -0.1057, 0.0040, -0.0008, 0.0031, -0.0132, 0.0122, 0.0187, 0.0018, -0.0482, -0.0114, -0.0026, -0.0004, -0.0118, -0.0180, -0.0013, 0.0028, 0.0191, -0.0111, 0.0041, -0.0159, -0.0040, -0.0110, 0.0050, 0.0104, 0.0029, -0.0224, -0.0015, -0.0100, -0.0152, 0.0233, -0.0002, -0.0026, -0.0001, 0.0126, -0.0196, 0.0069, 0.0012, -0.0244, -0.0015, 0.0039, 0.0241, -0.0054, 0.0060, -0.0089, -0.0173, 0.0084, 0.0058, 0.0131, 0.0084, -0.0145, 0.0012, 0.0127, 0.0285, 0.0036, -0.0042, 0.0066, 0.0066, -0.0038, -0.0071, 0.0135, -0.0055, -0.0174, -0.0042, -0.0090, -0.0061, 0.0260, 0.0068, 0.0090, -0.0243, 0.0195, -0.0106, -0.0073, -0.0073, 0.0036, 0.0056, -0.0032, 0.0164, 0.0055, -0.0055, 0.0056, -0.0302, 0.0077, -0.0148, 0.0167, 0.0180, 0.0074, 0.0014, -0.0068, 0.0069, -0.0093, -0.0007, -0.0161, 0.0163, 0.0159, -0.0114, -0.0167, -0.0144, 0.0072, 0.0161, 0.0223, -0.0146]\n",
            "\n",
            "HEAD 11  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0407, 0.0040, -0.0497, -0.0331, 0.0371, -0.0452, -0.0056, 0.0236, -0.0285, 0.0027, -0.0325, -0.0657, -0.0058, 0.0110, 0.0444, 0.0132, 0.0108, 0.0178, 0.0607, -0.0131, 0.0115, 0.0075, 0.0325, -0.0135, 0.0385, 0.0438, 0.0078, 0.0472, 0.1044, 0.0118, 0.0202, -0.0145, -0.0153, -0.0284, -0.0293, -0.0131, 0.1406, 0.0315, -0.0288, -0.0296, -0.0457, 0.0093, -0.0108, 0.0410, -0.0025, 0.0497, -0.0323, 0.0217, 0.0047, 0.0358, -0.0100, 0.0260, 0.0499, -0.0337, -0.0065, 0.0330, -0.0308, -0.0198, -0.0612, -0.0367, -0.0313, 0.0033, -0.0048, -0.0335, -0.0262, -0.0835, -0.0458, 0.0185, 0.0034, 0.0003, 0.0006, -0.0087, -0.0207, -0.0105, -0.0033, 0.0349, -0.0087, 0.0042, 0.0083, -0.0081, 0.0115, 0.0034, 0.0122, -0.0185, -0.0353, -0.0268, 0.0150, -0.0414, 0.0096, 0.0236, -0.0289, -0.0392, 0.0180, 0.0320, -0.0395, 0.0245, 0.0237, -0.0432, 0.0076, 0.0048, -0.0359, -0.0407, -0.0110, -0.0209, 0.0097, -0.0135, -0.0082, 0.0389, -0.0310, 0.0027, 0.0046, 0.0221, 0.0085, -0.0060, 0.1247, -0.0413, -0.0394, -0.0230, -0.0387, 0.0891, 0.0645, -0.0384, -0.0269, -0.0325, 0.0308, -0.0027, 0.0419, 0.0007]\n",
            "Token 1( açò): [0.0182, -0.0030, -0.0223, -0.0033, 0.0298, -0.0307, 0.0130, 0.0186, -0.0031, 0.0115, -0.0192, -0.0391, -0.0031, 0.0068, 0.0386, 0.0362, -0.0130, 0.0280, 0.0080, -0.0174, -0.0053, -0.0197, 0.0154, -0.0231, -0.0094, -0.0004, -0.0133, 0.0247, 0.0474, 0.0399, 0.0300, -0.0021, 0.0100, -0.0306, -0.0117, 0.0153, 0.0201, 0.0141, -0.0265, 0.0001, -0.0309, -0.0048, 0.0145, 0.0292, 0.1648, 0.0342, -0.0437, -0.0000, -0.0091, 0.0139, -0.0234, 0.0125, 0.0281, -0.0039, 0.0028, 0.0142, -0.0243, 0.0133, -0.0176, -0.0185, 0.0019, -0.0109, -0.0088, -0.0087, -0.0022, -0.0381, -0.0106, 0.0212, 0.0239, 0.0295, 0.0284, -0.0164, -0.0251, -0.0012, 0.0020, 0.0406, 0.0209, 0.0202, -0.0051, -0.0151, -0.0130, 0.0119, 0.0219, -0.0117, -0.0255, -0.0265, 0.0254, -0.0488, 0.0022, 0.0020, -0.0108, -0.0239, 0.0161, 0.0186, 0.0041, 0.0022, 0.0169, -0.0026, -0.0020, -0.0040, -0.0076, -0.0194, -0.0130, 0.0073, 0.0154, -0.0080, 0.0308, 0.0118, -0.0182, -0.0179, 0.0292, 0.0221, 0.0045, -0.0207, 0.3167, -0.0046, 0.0018, -0.0122, -0.0277, 0.1742, 0.0215, -0.0435, 0.0103, 0.0064, 0.0353, -0.0142, 0.0458, 0.0009]\n",
            "Token 2(  es): [0.0125, -0.0122, -0.0137, 0.0059, 0.0250, -0.0270, 0.0093, 0.0164, 0.0006, 0.0027, -0.0137, -0.0206, -0.0096, 0.0040, 0.0260, 0.0237, -0.0072, 0.0320, 0.0080, -0.0172, -0.0148, -0.0143, 0.0038, -0.0135, -0.0065, -0.0041, -0.0155, 0.0213, 0.0346, 0.0224, 0.0207, 0.0045, 0.0097, -0.0238, -0.0103, 0.0048, 0.0088, 0.0098, -0.0208, 0.0039, -0.0200, 0.0031, 0.0082, 0.0193, 0.1015, 0.0293, -0.0349, -0.0022, -0.0061, -0.0054, -0.0105, 0.0116, 0.0182, -0.0063, 0.0036, 0.0084, -0.0105, 0.0013, -0.0100, -0.0008, -0.0005, -0.0208, 0.0042, -0.0045, -0.0042, -0.0225, -0.0152, 0.0133, 0.0225, 0.0281, 0.0164, -0.0094, -0.0115, -0.0006, 0.0005, 0.0294, 0.0253, 0.0159, -0.0059, -0.0019, -0.0143, 0.0096, 0.0128, 0.0033, -0.0177, -0.0153, 0.0187, -0.0385, 0.0030, -0.0019, -0.0221, -0.0134, 0.0070, 0.0154, 0.0045, -0.0039, 0.0135, 0.0009, -0.0062, -0.0022, -0.0121, -0.0168, -0.0160, 0.0139, 0.0165, 0.0053, 0.0246, 0.0105, -0.0184, -0.0145, 0.0142, 0.0175, 0.0138, -0.0041, 0.2931, -0.0081, 0.0027, -0.0061, -0.0172, 0.1539, 0.0193, -0.0346, 0.0105, 0.0099, 0.0285, -0.0184, 0.0428, 0.0025]\n",
            "Token 3(  or): [0.0131, -0.0156, -0.0160, 0.0037, 0.0201, -0.0263, 0.0095, 0.0163, -0.0030, 0.0026, -0.0125, -0.0173, -0.0053, 0.0059, 0.0257, 0.0173, -0.0074, 0.0238, 0.0112, -0.0098, -0.0131, -0.0111, 0.0058, -0.0097, -0.0029, -0.0041, -0.0086, 0.0168, 0.0286, -0.0025, 0.0170, 0.0066, 0.0039, -0.0218, -0.0135, -0.0026, 0.0147, 0.0077, -0.0219, -0.0017, -0.0216, 0.0041, 0.0018, 0.0167, 0.0522, 0.0287, -0.0308, 0.0007, -0.0018, -0.0013, -0.0099, 0.0099, 0.0176, -0.0064, -0.0006, 0.0102, -0.0090, -0.0045, -0.0166, 0.0056, -0.0059, -0.0161, 0.0046, -0.0055, -0.0085, -0.0256, -0.0161, 0.0057, 0.0142, 0.0215, 0.0159, -0.0064, -0.0074, 0.0003, 0.0007, 0.0235, 0.0187, 0.0074, -0.0038, -0.0009, -0.0137, 0.0069, 0.0080, 0.0016, -0.0187, -0.0094, 0.0145, -0.0293, 0.0036, 0.0004, -0.0192, -0.0107, 0.0103, 0.0117, -0.0010, -0.0054, 0.0097, -0.0032, -0.0065, -0.0009, -0.0135, -0.0154, -0.0115, 0.0059, 0.0167, 0.0036, 0.0193, 0.0109, -0.0182, -0.0125, 0.0170, 0.0137, 0.0100, -0.0036, 0.2691, -0.0113, -0.0010, -0.0073, -0.0119, 0.1471, 0.0205, -0.0257, 0.0073, 0.0046, 0.0227, -0.0145, 0.0396, -0.0036]\n",
            "Token 4( ...): [0.0071, -0.0168, -0.0112, 0.0104, 0.0129, -0.0210, 0.0088, 0.0157, -0.0048, 0.0049, -0.0073, -0.0107, -0.0042, 0.0042, 0.0198, 0.0114, -0.0080, 0.0226, 0.0052, -0.0101, -0.0150, -0.0083, 0.0045, -0.0104, -0.0058, -0.0017, -0.0069, 0.0115, 0.0238, -0.0187, 0.0156, 0.0095, 0.0046, -0.0198, -0.0099, -0.0034, 0.0123, 0.0048, -0.0187, -0.0028, -0.0172, 0.0068, -0.0009, 0.0137, 0.0116, 0.0274, -0.0265, -0.0015, -0.0012, -0.0083, 0.0011, 0.0100, 0.0156, -0.0054, 0.0005, 0.0086, -0.0070, -0.0038, -0.0106, 0.0084, -0.0068, -0.0156, 0.0096, -0.0061, -0.0027, -0.0191, -0.0154, 0.0066, 0.0138, 0.0208, 0.0077, -0.0055, -0.0064, 0.0049, -0.0045, 0.0254, 0.0211, 0.0056, -0.0051, 0.0009, -0.0129, 0.0093, 0.0064, 0.0054, -0.0175, -0.0025, 0.0127, -0.0266, 0.0070, -0.0018, -0.0194, -0.0078, 0.0072, 0.0101, 0.0001, -0.0081, 0.0050, -0.0023, -0.0083, 0.0007, -0.0126, -0.0112, -0.0083, 0.0091, 0.0158, 0.0053, 0.0198, 0.0094, -0.0158, -0.0091, 0.0105, 0.0116, 0.0114, -0.0016, 0.2455, -0.0108, 0.0005, -0.0053, -0.0063, 0.1356, 0.0166, -0.0216, 0.0052, 0.0050, 0.0191, -0.0130, 0.0352, -0.0036]\n",
            "\n",
            "HEAD 12  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0182, 0.0278, -0.0238, 0.0184, -0.0484, -0.0129, -0.0744, -0.0377, 0.0210, 0.0746, -0.0314, -0.2231, -0.0435, -0.0089, -0.0150, -0.0390, -0.0371, -0.0254, -0.0239, -0.0260, 0.0078, -0.0231, 0.0109, 0.0994, -0.0080, 0.4786, -0.0564, -0.0219, -0.0073, -0.0100, 0.0057, -0.0898, -0.0445, 0.0153, 0.0170, -0.0438, -0.0153, -0.0398, -0.0501, 0.0140, -0.0388, 0.0280, 0.1396, 0.0470, -0.0258, 0.0049, 0.0089, -0.0369, -0.0166, -0.0142, -0.0206, 0.0254, -0.0579, 0.0036, 0.0057, -0.0407, 0.0027, -0.0016, 0.0671, 0.0408, -0.1123, 0.0057, -0.0251, -0.0340, -0.0063, 0.0677, 0.0105, 0.0189, 0.0103, 0.0971, 0.0243, 0.0232, -0.0005, -0.0012, -0.0069, -0.2391, 0.0272, -0.0959, 0.0056, 0.0062, -0.0495, 0.1669, 0.0021, 0.0177, -0.0288, 0.0043, -0.0005, 0.0352, -0.0418, 0.1269, 0.0270, -0.0283, -0.0291, -0.0242, -0.0116, -0.0298, -0.0332, 0.0427, -0.0112, 0.0025, -0.0091, 0.0133, -0.0210, 0.0138, -0.0229, 0.1473, -0.0059, 0.0106, 0.0180, 0.0127, -0.1283, -0.0005, 0.1513, 0.0197, -0.0378, -0.0330, 0.0355, -0.0313, 0.0141, -0.0028, 0.2614, 0.0045, 0.0306, -0.0921, 0.0130, -0.0359, 0.0489, -0.0099]\n",
            "Token 1( açò): [0.0070, 0.0153, -0.0543, 0.0104, -0.0291, 0.0074, -0.0257, -0.0195, 0.0106, 0.0368, -0.0053, -0.0823, -0.0307, 0.0023, -0.0053, -0.0143, -0.0200, -0.0172, -0.0179, -0.0104, -0.0012, -0.0201, 0.0083, 0.0586, -0.0097, 0.4538, -0.0255, -0.0061, -0.0132, -0.0123, 0.0100, -0.0523, -0.0185, 0.0199, 0.0154, -0.0251, -0.0017, -0.0392, -0.0266, -0.0184, -0.0011, 0.0370, 0.0882, 0.0119, -0.0292, 0.0021, 0.0159, -0.0201, 0.0082, -0.0180, -0.0243, 0.0307, -0.0220, 0.0122, 0.0137, -0.0166, -0.0199, -0.0110, 0.0532, 0.0265, -0.0716, -0.0119, 0.0072, -0.0397, -0.0263, 0.0789, -0.0078, 0.0189, -0.0075, 0.0710, 0.0142, 0.0262, -0.0058, 0.0142, -0.0087, -0.0817, 0.0065, -0.0724, 0.0128, -0.0105, -0.0488, 0.0967, 0.0106, -0.0258, 0.0020, -0.0098, 0.0154, 0.0136, -0.0470, 0.0659, -0.0135, -0.0296, -0.0307, 0.0021, -0.0246, -0.0456, -0.0162, 0.0330, -0.0028, -0.0257, -0.0157, 0.0095, 0.0020, 0.0200, -0.0291, 0.1319, -0.0047, 0.0090, 0.0078, 0.0038, -0.1738, -0.0093, 0.0902, 0.0121, -0.0168, -0.0068, 0.0070, 0.0048, 0.0259, -0.0105, 0.1714, 0.0119, 0.0180, -0.0681, 0.0080, -0.0327, 0.0127, -0.0158]\n",
            "Token 2(  es): [-0.0031, 0.0052, -0.0643, 0.0054, -0.0156, 0.0180, 0.0055, -0.0112, 0.0025, 0.0120, 0.0124, -0.0269, -0.0170, 0.0135, 0.0083, -0.0076, -0.0051, -0.0147, -0.0188, 0.0011, -0.0093, -0.0131, 0.0024, 0.0233, -0.0078, 0.4408, -0.0088, -0.0009, -0.0238, -0.0072, 0.0115, -0.0146, -0.0005, 0.0267, 0.0093, -0.0072, 0.0095, -0.0240, -0.0085, -0.0240, 0.0327, 0.0351, 0.0247, -0.0067, -0.0177, -0.0011, 0.0155, -0.0155, 0.0195, -0.0141, -0.0267, 0.0362, -0.0190, 0.0059, 0.0161, -0.0013, -0.0267, -0.0076, 0.0236, 0.0054, -0.0281, -0.0197, 0.0197, -0.0360, -0.0269, 0.0710, -0.0095, 0.0152, -0.0097, 0.0545, 0.0115, 0.0166, -0.0046, 0.0189, -0.0053, -0.0246, -0.0057, -0.0453, 0.0121, -0.0159, -0.0477, 0.0561, 0.0159, -0.0478, 0.0073, -0.0135, 0.0263, 0.0004, -0.0436, 0.0187, -0.0282, -0.0214, -0.0284, 0.0115, -0.0255, -0.0427, -0.0037, 0.0228, -0.0025, -0.0423, -0.0148, 0.0052, 0.0277, 0.0215, -0.0191, 0.1057, -0.0057, 0.0066, 0.0023, 0.0011, -0.1647, -0.0064, 0.0822, 0.0077, 0.0016, 0.0142, -0.0044, 0.0229, 0.0298, -0.0103, 0.0989, 0.0108, 0.0036, -0.0496, 0.0064, -0.0266, 0.0001, -0.0194]\n",
            "Token 3(  or): [0.0028, 0.0126, -0.0387, 0.0106, -0.0162, 0.0025, -0.0116, -0.0118, 0.0108, 0.0314, -0.0051, -0.1083, -0.0200, 0.0055, 0.0031, -0.0204, -0.0117, -0.0174, -0.0220, -0.0077, -0.0026, -0.0111, 0.0034, 0.0354, -0.0042, 0.4459, -0.0220, -0.0054, -0.0199, -0.0138, 0.0147, -0.0279, -0.0084, 0.0201, 0.0043, -0.0161, 0.0088, -0.0145, -0.0138, 0.0062, 0.0154, 0.0210, 0.0395, 0.0107, -0.0097, 0.0029, 0.0101, -0.0192, 0.0166, -0.0127, -0.0212, 0.0256, -0.0386, 0.0028, 0.0096, -0.0152, -0.0119, -0.0046, 0.0242, 0.0121, -0.0411, -0.0089, 0.0021, -0.0281, -0.0174, 0.0550, 0.0011, 0.0168, 0.0040, 0.0742, 0.0150, 0.0099, -0.0078, 0.0040, -0.0077, -0.1150, 0.0066, -0.0504, 0.0057, 0.0012, -0.0494, 0.0917, 0.0107, -0.0252, -0.0257, -0.0047, 0.0162, 0.0116, -0.0312, 0.0539, -0.0052, -0.0181, -0.0285, -0.0025, -0.0095, -0.0292, -0.0131, 0.0248, -0.0058, -0.0212, -0.0146, 0.0012, 0.0114, 0.0121, -0.0154, 0.1039, -0.0014, 0.0059, 0.0071, 0.0008, -0.1247, -0.0045, 0.1073, 0.0072, -0.0086, -0.0013, 0.0123, -0.0046, 0.0214, -0.0012, 0.1318, 0.0057, 0.0080, -0.0496, 0.0038, -0.0185, 0.0074, -0.0130]\n",
            "Token 4( ...): [0.0047, 0.0113, -0.0354, 0.0147, -0.0044, 0.0017, -0.0045, -0.0043, 0.0086, 0.0285, -0.0102, -0.0714, -0.0199, 0.0072, 0.0001, -0.0166, -0.0149, -0.0156, -0.0221, -0.0080, 0.0022, -0.0108, 0.0011, 0.0319, -0.0057, 0.4350, -0.0214, -0.0018, -0.0174, -0.0379, 0.0156, -0.0216, -0.0059, 0.0161, 0.0024, -0.0169, 0.0121, -0.0129, -0.0096, 0.0098, 0.0136, 0.0179, 0.0528, 0.0159, -0.0094, 0.0036, 0.0145, -0.0141, 0.0245, -0.0150, -0.0178, 0.0225, -0.0178, 0.0079, 0.0076, -0.0122, -0.0116, -0.0071, 0.0280, 0.0096, -0.0425, -0.0065, 0.0104, -0.0305, -0.0221, 0.0666, -0.0033, 0.0208, 0.0030, 0.0612, 0.0118, 0.0102, -0.0092, 0.0030, -0.0110, -0.1065, 0.0075, -0.0400, 0.0064, 0.0039, -0.0434, 0.0996, 0.0115, -0.0257, -0.0269, -0.0042, 0.0116, 0.0123, -0.0281, 0.0557, -0.0097, -0.0184, -0.0321, -0.0029, -0.0082, -0.0282, -0.0136, 0.0251, -0.0057, -0.0168, -0.0155, -0.0017, 0.0014, 0.0094, -0.0065, 0.0946, 0.0019, 0.0056, 0.0030, 0.0000, -0.1172, -0.0113, 0.0700, -0.0001, -0.0092, -0.0030, 0.0057, -0.0041, 0.0222, -0.0041, 0.1242, 0.0033, 0.0055, -0.0405, -0.0004, -0.0153, -0.0029, -0.0091]\n",
            "\n",
            "HEAD 13  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0174, -0.1290, -0.0275, 0.0390, -0.0243, -0.0411, 0.1415, -0.0819, -0.0737, 0.1021, -0.2416, 0.0862, 0.0627, -0.0187, 0.0868, 0.0286, 0.0697, -0.0132, 0.0228, 0.0469, 0.0987, 0.1704, -0.0052, 0.1327, -0.1521, 0.0016, 0.0844, 0.0589, 0.0819, 0.0113, -0.1218, -0.0122, -0.1546, 0.0135, 0.0594, -0.0307, 0.0543, 0.0743, 0.0812, -0.1029, 0.0549, -0.0187, -0.0453, 0.0048, 0.0856, -0.0043, 0.0479, 0.0066, 0.0117, 0.1035, -0.0402, -0.0406, -0.0410, -0.0960, 0.0719, -0.1367, -0.0420, 0.0475, -0.1495, 0.0632, -0.1488, 0.0732, -0.0100, -0.0732, -0.0740, 0.0335, -0.0052, 0.0140, 0.0049, 0.0566, 0.0908, -0.1148, 0.0772, -0.2123, -0.0500, 0.0275, -0.0897, 0.0328, 0.0599, 0.0228, -0.1711, -0.0188, 0.1615, -0.0410, 0.0348, -0.0388, -0.0342, -0.0094, -0.0558, 0.1272, 0.1143, -0.0472, 0.2274, 0.0234, -0.0762, 0.0199, -0.0847, 0.0246, 0.0474, -0.0429, 0.0770, -0.0558, 0.0554, -0.0315, -0.0858, 0.0613, -0.1620, -0.1234, -0.1278, -0.0133, -0.0333, -0.0997, -0.0709, -0.0608, -0.0352, 0.0190, 0.0251, -0.0354, 0.0087, 0.0099, 0.1437, -0.0669, -0.1836, 0.0371, -0.0154, -0.0116, 0.0631, -0.0049]\n",
            "Token 1( açò): [0.0166, -0.1279, -0.0268, 0.0388, -0.0234, -0.0406, 0.1396, -0.0824, -0.0731, 0.1011, -0.2445, 0.0850, 0.0629, -0.0183, 0.0858, 0.0289, 0.0702, -0.0128, 0.0226, 0.0466, 0.0977, 0.1691, -0.0057, 0.1311, -0.1504, 0.0016, 0.0829, 0.0589, 0.0811, 0.0108, -0.1205, -0.0123, -0.1534, 0.0133, 0.0596, -0.0305, 0.0533, 0.0742, 0.0805, -0.1020, 0.0542, -0.0188, -0.0454, 0.0040, 0.0843, -0.0038, 0.0475, 0.0062, 0.0119, 0.1030, -0.0398, -0.0401, -0.0402, -0.0949, 0.0712, -0.1355, -0.0417, 0.0467, -0.1483, 0.0629, -0.1475, 0.0725, -0.0103, -0.0722, -0.0731, 0.0330, -0.0056, 0.0141, 0.0053, 0.0568, 0.0896, -0.1142, 0.0760, -0.2106, -0.0499, 0.0277, -0.0891, 0.0327, 0.0590, 0.0224, -0.1693, -0.0189, 0.1602, -0.0404, 0.0353, -0.0382, -0.0343, -0.0094, -0.0550, 0.1261, 0.1138, -0.0469, 0.2263, 0.0230, -0.0760, 0.0190, -0.0837, 0.0243, 0.0473, -0.0424, 0.0762, -0.0550, 0.0548, -0.0313, -0.0847, 0.0602, -0.1606, -0.1226, -0.1264, -0.0134, -0.0329, -0.0989, -0.0701, -0.0603, -0.0353, 0.0187, 0.0247, -0.0356, 0.0094, 0.0095, 0.1423, -0.0663, -0.1820, 0.0369, -0.0151, -0.0119, 0.0627, -0.0059]\n",
            "Token 2(  es): [-0.0306, -0.0084, -0.0014, 0.0017, -0.0187, -0.0262, -0.0073, 0.0246, 0.0264, 0.0098, 0.2079, 0.0188, -0.0235, -0.0050, -0.0423, -0.0119, -0.0479, -0.0244, -0.0382, -0.0073, 0.0287, 0.0157, 0.0017, -0.0167, -0.0102, 0.0151, -0.0044, 0.0210, -0.0281, -0.0105, 0.0077, 0.0113, -0.0180, -0.0033, 0.0367, -0.0442, -0.0053, 0.0237, 0.0043, 0.0205, -0.0733, 0.0363, -0.0356, -0.0278, -0.0227, 0.0008, 0.0147, -0.0068, -0.0001, -0.0198, 0.0176, -0.0105, -0.0384, -0.0073, 0.0082, -0.0169, -0.0210, 0.0422, 0.0089, -0.0136, -0.0158, 0.0446, -0.0020, -0.0053, -0.0325, 0.0303, -0.0413, -0.0599, -0.0270, 0.0191, -0.0116, 0.0013, -0.0194, 0.0331, 0.0163, 0.0184, 0.0274, -0.0318, -0.0042, 0.0439, 0.0040, -0.0078, 0.0122, -0.0018, -0.0018, 0.0234, -0.0112, 0.0143, 0.0188, -0.0182, -0.0106, 0.0305, 0.0272, -0.0414, 0.0340, 0.0010, -0.0156, 0.0034, 0.0167, 0.0267, -0.0258, 0.0328, -0.0127, 0.0147, -0.0003, -0.0097, 0.0006, 0.0098, 0.0262, 0.0291, -0.0054, -0.0190, 0.0234, 0.0296, 0.0485, -0.0170, 0.0071, -0.0016, 0.0081, 0.0261, -0.0043, -0.0051, -0.0028, -0.0054, -0.0032, -0.0245, -0.0267, -0.0734]\n",
            "Token 3(  or): [-0.0137, -0.0020, 0.0047, -0.0162, 0.0106, -0.0227, -0.0202, 0.0163, 0.0060, 0.0019, 0.2367, 0.0060, 0.0058, -0.0125, -0.0277, 0.0065, -0.0235, -0.0062, -0.0007, -0.0093, 0.0298, 0.0093, 0.0114, -0.0165, 0.0178, 0.0058, 0.0355, -0.0031, -0.0250, -0.0124, 0.0066, -0.0057, -0.0154, -0.0091, 0.0283, -0.0368, 0.0010, 0.0029, -0.0094, 0.0115, -0.0197, -0.0031, -0.0163, -0.0035, -0.0395, -0.0055, 0.0098, -0.0021, 0.0044, -0.0014, 0.0105, -0.0191, -0.0210, -0.0022, 0.0078, -0.0189, 0.0018, 0.0031, 0.0167, -0.0015, -0.0184, -0.0037, 0.0071, -0.0148, -0.0176, -0.0019, -0.0092, -0.0459, -0.0115, 0.0478, 0.0037, 0.0025, -0.0333, 0.0282, -0.0280, 0.0329, 0.0252, -0.0353, -0.0105, 0.0405, -0.0083, -0.0311, 0.0422, 0.0033, 0.0008, 0.0139, -0.0066, 0.0440, 0.0105, -0.0012, -0.0186, 0.0123, 0.0397, -0.0361, 0.0109, 0.0139, -0.0207, -0.0089, 0.0085, 0.0214, -0.0252, 0.0208, 0.0071, 0.0136, -0.0066, -0.0177, 0.0191, 0.0019, -0.0055, 0.0189, 0.0122, -0.0294, 0.0188, 0.0022, 0.0159, -0.0105, -0.0046, -0.0094, 0.0102, 0.0198, -0.0117, 0.0063, 0.0181, 0.0053, -0.0141, 0.0026, -0.0235, -0.0688]\n",
            "Token 4( ...): [-0.0139, 0.0179, 0.0108, -0.0402, 0.0173, -0.0145, -0.0288, 0.0183, -0.0028, -0.0172, 0.2367, -0.0224, 0.0027, -0.0224, 0.0074, -0.0066, -0.0143, 0.0223, -0.0106, -0.0296, 0.0249, 0.0060, -0.0260, 0.0017, 0.0100, -0.0109, 0.0472, -0.0059, -0.0276, -0.0064, 0.0013, 0.0170, -0.0001, 0.0102, 0.0109, -0.0124, -0.0097, -0.0113, -0.0302, 0.0085, 0.0204, 0.0282, -0.0058, -0.0098, -0.0406, 0.0061, -0.0252, 0.0074, -0.0261, 0.0360, -0.0175, 0.0217, -0.0161, -0.0111, 0.0094, -0.0001, 0.0082, -0.0059, 0.0316, 0.0113, -0.0186, -0.0025, 0.0362, 0.0139, -0.0056, -0.0371, 0.0351, -0.0383, 0.0211, 0.0460, -0.0068, -0.0111, -0.0193, 0.0436, -0.0175, 0.0198, 0.0099, -0.0261, 0.0050, 0.0279, -0.0506, -0.0365, 0.0666, -0.0341, -0.0014, 0.0285, -0.0125, 0.0565, 0.0127, 0.0021, -0.0051, -0.0050, 0.0581, -0.0166, -0.0156, 0.0230, -0.0045, 0.0121, 0.0163, -0.0102, -0.0321, 0.0095, 0.0199, 0.0356, -0.0365, -0.0186, 0.0205, -0.0159, -0.0512, -0.0003, 0.0186, -0.0254, -0.0344, 0.0078, 0.0309, -0.0112, -0.0220, -0.0145, 0.0001, 0.0022, -0.0218, -0.0027, -0.0019, 0.0283, -0.0141, 0.0152, 0.0003, -0.1020]\n",
            "\n",
            "HEAD 14  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [0.0047, -0.0257, 0.0017, -0.0183, 0.0019, 0.0056, -0.0100, -0.0132, 0.1737, 0.0199, 0.0034, -0.0206, -0.0048, -0.0170, 0.0134, -0.0037, 0.0539, 0.0126, -0.0030, 0.0112, 0.0023, -0.0160, -0.0135, -0.0081, 0.0034, 0.0137, -0.0313, -0.0036, 0.0004, 0.0018, -0.0093, 0.0033, 0.0707, 0.0157, -0.0020, -0.0057, 0.0237, 0.0178, -0.0091, -0.0050, -0.0054, 0.0295, -0.0015, -0.0073, -0.0123, -0.0233, -0.0199, -0.0040, 0.0128, -0.0164, 0.0014, 0.0244, 0.0009, 0.0141, 0.0015, 0.0273, -0.0145, 0.0269, -0.0152, 0.0184, -0.0153, 0.0031, 0.0141, 0.0237, 0.0006, -0.0062, 0.0159, -0.0139, -0.0013, -0.0235, 0.0055, -0.0081, -0.0150, -0.0120, -0.0027, 0.1849, 0.0579, -0.0015, -0.0084, -0.0140, 0.0085, -0.0086, -0.0061, -0.0093, 0.0316, 0.0459, 0.0138, -0.0074, 0.0038, -0.0230, -0.1040, -0.0241, 0.0048, 0.0354, -0.0063, -0.0017, 0.0075, -0.0067, 0.0426, -0.0177, 0.0050, -0.0179, 0.0019, -0.0027, -0.0206, 0.0091, -0.0078, -0.0297, 0.0069, -0.0174, -0.0084, 0.0168, 0.0218, -0.0191, -0.0087, 0.0003, 0.0153, 0.0221, -0.0013, -0.0060, -0.0136, 0.0040, 0.0135, -0.0218, -0.0255, -0.0075, 0.0050, -0.0041]\n",
            "Token 1( açò): [-0.0016, -0.0269, 0.0002, -0.0202, 0.0034, 0.0010, -0.0053, -0.0104, 0.0936, 0.0170, 0.0078, -0.0142, -0.0098, -0.0136, 0.0104, -0.0014, 0.0449, 0.0114, 0.0031, 0.0058, 0.0069, -0.0204, -0.0077, -0.0057, 0.0034, 0.0066, -0.0268, -0.0069, 0.0013, 0.0049, -0.0083, 0.0021, 0.0375, 0.0152, -0.0039, -0.0033, 0.0268, 0.0133, -0.0077, -0.0035, -0.0034, 0.0199, 0.0020, -0.0068, -0.0094, -0.0198, -0.0137, -0.0027, 0.0052, -0.0143, 0.0029, 0.0196, 0.0010, 0.0142, 0.0029, 0.0251, -0.0138, 0.0172, -0.0112, 0.0141, -0.0124, 0.0010, 0.0159, 0.0182, 0.0032, -0.0086, 0.0162, -0.0084, 0.0044, -0.0268, 0.0059, -0.0078, -0.0115, -0.0073, 0.0026, 0.1106, 0.0595, 0.0022, -0.0044, -0.0124, 0.0093, -0.0079, -0.0052, -0.0035, 0.0336, 0.0461, 0.0100, -0.0103, -0.0061, -0.0211, -0.0868, -0.0193, 0.0002, 0.0303, -0.0029, -0.0002, 0.0024, -0.0043, 0.0372, -0.0175, 0.0047, -0.0148, -0.0066, -0.0018, -0.0194, 0.0111, -0.0071, -0.0265, -0.0007, -0.0141, -0.0097, 0.0098, 0.0199, -0.0189, -0.0053, -0.0021, 0.0126, 0.0244, -0.0011, 0.0029, -0.0172, 0.0051, 0.0102, -0.0260, -0.0209, -0.0114, -0.0005, -0.0100]\n",
            "Token 2(  es): [0.0019, -0.0251, 0.0010, -0.0185, 0.0021, 0.0040, -0.0079, -0.0130, 0.1372, 0.0190, 0.0041, -0.0188, -0.0062, -0.0160, 0.0125, -0.0023, 0.0496, 0.0118, -0.0016, 0.0093, 0.0049, -0.0173, -0.0117, -0.0069, 0.0034, 0.0116, -0.0287, -0.0049, 0.0001, 0.0042, -0.0094, 0.0028, 0.0535, 0.0153, -0.0025, -0.0047, 0.0245, 0.0161, -0.0092, -0.0037, -0.0049, 0.0259, -0.0005, -0.0066, -0.0107, -0.0221, -0.0175, -0.0022, 0.0095, -0.0146, 0.0021, 0.0217, 0.0004, 0.0143, 0.0016, 0.0259, -0.0139, 0.0243, -0.0137, 0.0164, -0.0136, 0.0031, 0.0146, 0.0217, 0.0017, -0.0072, 0.0151, -0.0129, 0.0004, -0.0250, 0.0054, -0.0073, -0.0134, -0.0111, -0.0009, 0.1490, 0.0569, 0.0006, -0.0069, -0.0136, 0.0080, -0.0073, -0.0046, -0.0076, 0.0318, 0.0445, 0.0124, -0.0080, 0.0003, -0.0224, -0.0965, -0.0224, 0.0030, 0.0330, -0.0053, -0.0005, 0.0065, -0.0055, 0.0390, -0.0170, 0.0051, -0.0164, -0.0005, -0.0026, -0.0190, 0.0098, -0.0078, -0.0284, 0.0045, -0.0154, -0.0086, 0.0140, 0.0201, -0.0179, -0.0070, -0.0009, 0.0133, 0.0222, -0.0017, -0.0031, -0.0146, 0.0043, 0.0121, -0.0220, -0.0238, -0.0087, 0.0037, -0.0062]\n",
            "Token 3(  or): [0.0001, -0.0231, 0.0005, -0.0177, 0.0023, 0.0031, -0.0065, -0.0135, 0.1081, 0.0186, 0.0037, -0.0182, -0.0065, -0.0157, 0.0121, -0.0011, 0.0460, 0.0110, -0.0016, 0.0084, 0.0071, -0.0174, -0.0113, -0.0064, 0.0028, 0.0108, -0.0260, -0.0055, -0.0005, 0.0059, -0.0098, 0.0024, 0.0357, 0.0148, -0.0026, -0.0041, 0.0245, 0.0154, -0.0099, -0.0026, -0.0046, 0.0239, -0.0004, -0.0056, -0.0098, -0.0215, -0.0160, 0.0001, 0.0076, -0.0127, 0.0026, 0.0189, -0.0004, 0.0144, 0.0017, 0.0246, -0.0136, 0.0244, -0.0126, 0.0151, -0.0119, 0.0038, 0.0145, 0.0206, 0.0024, -0.0075, 0.0138, -0.0134, 0.0007, -0.0257, 0.0052, -0.0061, -0.0124, -0.0115, -0.0001, 0.1190, 0.0532, 0.0019, -0.0059, -0.0136, 0.0071, -0.0055, -0.0028, -0.0070, 0.0312, 0.0422, 0.0114, -0.0078, -0.0013, -0.0216, -0.0912, -0.0214, 0.0016, 0.0309, -0.0050, 0.0007, 0.0067, -0.0044, 0.0352, -0.0160, 0.0054, -0.0148, -0.0008, -0.0030, -0.0168, 0.0097, -0.0079, -0.0276, 0.0037, -0.0140, -0.0085, 0.0128, 0.0182, -0.0163, -0.0055, -0.0015, 0.0113, 0.0211, -0.0025, -0.0018, -0.0151, 0.0044, 0.0111, -0.0209, -0.0225, -0.0087, 0.0038, -0.0073]\n",
            "Token 4( ...): [0.0017, -0.0230, 0.0002, -0.0172, 0.0029, 0.0039, -0.0076, -0.0131, 0.1284, 0.0188, 0.0030, -0.0170, -0.0050, -0.0162, 0.0128, -0.0019, 0.0480, 0.0115, -0.0017, 0.0081, 0.0034, -0.0162, -0.0124, -0.0080, 0.0025, 0.0125, -0.0280, -0.0052, 0.0002, 0.0041, -0.0089, 0.0026, 0.0487, 0.0143, -0.0021, -0.0045, 0.0227, 0.0165, -0.0087, -0.0041, -0.0052, 0.0265, -0.0013, -0.0073, -0.0111, -0.0220, -0.0165, -0.0025, 0.0099, -0.0144, 0.0020, 0.0213, 0.0014, 0.0135, 0.0025, 0.0257, -0.0142, 0.0250, -0.0130, 0.0161, -0.0130, 0.0028, 0.0145, 0.0214, 0.0011, -0.0061, 0.0151, -0.0133, 0.0006, -0.0231, 0.0044, -0.0078, -0.0137, -0.0106, -0.0019, 0.1418, 0.0549, 0.0003, -0.0067, -0.0142, 0.0079, -0.0074, -0.0045, -0.0070, 0.0308, 0.0439, 0.0132, -0.0074, 0.0015, -0.0213, -0.0948, -0.0230, 0.0029, 0.0318, -0.0058, 0.0002, 0.0064, -0.0051, 0.0383, -0.0163, 0.0049, -0.0157, 0.0007, -0.0014, -0.0183, 0.0082, -0.0078, -0.0275, 0.0044, -0.0162, -0.0084, 0.0155, 0.0207, -0.0164, -0.0067, 0.0003, 0.0140, 0.0219, -0.0010, -0.0031, -0.0141, 0.0055, 0.0117, -0.0224, -0.0229, -0.0069, 0.0042, -0.0059]\n",
            "\n",
            "HEAD 15  Scores Matrix Masked - Shape: (5, 5)\n",
            "================================================================================\n",
            "Token 0( <s>): [-0.0001, 0.0307, 0.0271, -0.0246, -0.0394, 0.0076, -0.0123, -0.0422, 0.0018, -0.0177, 0.0159, -0.1882, -0.0059, -0.0860, 0.0484, 0.0425, -0.0093, 0.0199, -0.0861, -0.0626, 0.0114, 0.0836, 0.0464, 0.0303, -0.0357, -0.0059, -0.0544, -0.0463, 0.0213, -0.0427, -0.0057, -0.0707, 0.0545, 0.0537, 0.0455, -0.0080, 0.0390, 0.0064, 0.0354, 0.0331, 0.0277, -0.0118, -0.0700, -0.0105, -0.0450, -0.0372, 0.0022, 0.0125, 0.0657, -0.0320, -0.0387, -0.0384, 0.0522, -0.0265, -0.0388, -0.0121, -0.0267, -0.0129, -0.0442, 0.0392, -0.0763, -0.0208, -0.0551, 0.0279, 0.0169, 0.0234, 0.0365, -0.0378, -0.0244, 0.0053, 0.0434, 0.0782, -0.0278, 0.0360, 0.0731, -0.0018, -0.0129, 0.0435, -0.0001, 0.0153, 0.0565, 0.0466, 0.0478, 0.0126, 0.0878, 0.0205, -0.0379, 0.0196, -0.0189, -0.0570, -0.0158, -0.0292, 0.0518, -0.0603, -0.0417, 0.0241, 0.0282, -0.0203, 0.0455, 0.0216, -0.0056, -0.0294, 0.0469, 0.0629, 0.0501, 0.0504, -0.0487, 0.0207, 0.0416, 0.0635, -0.0154, -0.0286, 0.0226, -0.1066, 0.0062, -0.0501, -0.0181, -0.0159, 0.0404, 0.0115, 0.0482, 0.0203, 0.0467, 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]\n",
            "Token 1( açò): [-0.0067, 0.0323, 0.0234, -0.0177, -0.0282, 0.0080, -0.0107, -0.0308, 0.0056, -0.0156, 0.0136, -0.1082, -0.0074, -0.0759, 0.0418, 0.0341, -0.0111, 0.0146, -0.0739, -0.0499, 0.0110, 0.0755, 0.0393, 0.0265, -0.0247, -0.0090, -0.0419, -0.0413, 0.0213, -0.0318, -0.0021, -0.0581, 0.0458, 0.0467, 0.0438, -0.0098, 0.0371, -0.0002, 0.0344, 0.0230, 0.0215, -0.0090, -0.0534, -0.0083, -0.0397, -0.0286, -0.0020, 0.0101, 0.0573, -0.0257, -0.0353, -0.0375, 0.0478, -0.0226, -0.0335, -0.0135, -0.0226, -0.0079, -0.0392, 0.0324, -0.0656, -0.0205, -0.0438, 0.0277, 0.0212, 0.0210, 0.0329, -0.0267, -0.0222, 0.0063, 0.0330, 0.0703, -0.0272, 0.0281, 0.0701, -0.0048, -0.0089, 0.0369, 0.0024, 0.0133, 0.0518, 0.0360, 0.0377, 0.0120, 0.0732, 0.0162, -0.0353, 0.0207, -0.0151, -0.0478, -0.0238, -0.0273, 0.0478, -0.0518, -0.0364, 0.0172, 0.0262, -0.0115, 0.0375, 0.0125, -0.0014, -0.0315, 0.0416, 0.0542, 0.0443, 0.0412, -0.0465, 0.0148, 0.0323, 0.0555, -0.0158, -0.0216, 0.0174, -0.0928, 0.0039, -0.0405, -0.0147, -0.0046, 0.0317, 0.0103, 0.0326, 0.0129, 0.0420, 0.0443, -0.0060, -0.0240, -0.0088, -0.0050]\n",
            "Token 2(  es): [-0.0116, 0.0287, 0.0205, -0.0178, -0.0205, 0.0089, -0.0091, -0.0224, 0.0063, -0.0128, 0.0083, -0.0407, -0.0050, -0.0667, 0.0295, 0.0282, -0.0106, 0.0123, -0.0622, -0.0422, 0.0095, 0.0620, 0.0347, 0.0226, -0.0180, -0.0087, -0.0330, -0.0375, 0.0231, -0.0229, 0.0010, -0.0485, 0.0349, 0.0397, 0.0375, -0.0055, 0.0313, -0.0014, 0.0283, 0.0156, 0.0173, -0.0046, -0.0391, -0.0083, -0.0350, -0.0251, -0.0026, 0.0081, 0.0505, -0.0230, -0.0288, -0.0329, 0.0360, -0.0208, -0.0293, -0.0154, -0.0164, -0.0032, -0.0298, 0.0271, -0.0531, -0.0216, -0.0385, 0.0187, 0.0226, 0.0164, 0.0296, -0.0204, -0.0214, 0.0058, 0.0274, 0.0582, -0.0208, 0.0229, 0.0582, -0.0029, -0.0027, 0.0295, 0.0067, 0.0151, 0.0447, 0.0315, 0.0288, 0.0122, 0.0606, 0.0138, -0.0310, 0.0197, -0.0185, -0.0410, -0.0151, -0.0219, 0.0437, -0.0444, -0.0308, 0.0110, 0.0252, -0.0110, 0.0293, 0.0065, -0.0000, -0.0264, 0.0338, 0.0445, 0.0371, 0.0310, -0.0430, 0.0101, 0.0271, 0.0451, -0.0131, -0.0187, 0.0128, -0.0791, 0.0028, -0.0366, -0.0066, 0.0011, 0.0231, 0.0042, 0.0228, 0.0076, 0.0388, 0.0302, -0.0064, -0.0227, -0.0093, -0.0039]\n",
            "Token 3(  or): [-0.0119, 0.0257, 0.0196, -0.0160, -0.0189, 0.0087, -0.0087, -0.0177, 0.0056, -0.0133, 0.0064, 0.0020, -0.0065, -0.0584, 0.0224, 0.0254, -0.0076, 0.0111, -0.0548, -0.0392, 0.0092, 0.0531, 0.0314, 0.0206, -0.0141, -0.0084, -0.0288, -0.0324, 0.0206, -0.0220, 0.0020, -0.0453, 0.0287, 0.0337, 0.0327, -0.0039, 0.0271, -0.0003, 0.0239, 0.0129, 0.0169, -0.0044, -0.0304, -0.0081, -0.0314, -0.0231, -0.0027, 0.0048, 0.0445, -0.0209, -0.0240, -0.0312, 0.0292, -0.0208, -0.0268, -0.0156, -0.0135, -0.0016, -0.0239, 0.0236, -0.0474, -0.0199, -0.0354, 0.0154, 0.0219, 0.0141, 0.0271, -0.0187, -0.0219, 0.0049, 0.0245, 0.0511, -0.0164, 0.0210, 0.0502, -0.0023, -0.0013, 0.0261, 0.0096, 0.0139, 0.0404, 0.0279, 0.0239, 0.0099, 0.0545, 0.0125, -0.0265, 0.0176, -0.0166, -0.0366, -0.0078, -0.0191, 0.0389, -0.0392, -0.0279, 0.0102, 0.0220, -0.0103, 0.0257, 0.0060, 0.0015, -0.0231, 0.0304, 0.0371, 0.0312, 0.0251, -0.0383, 0.0089, 0.0247, 0.0391, -0.0124, -0.0143, 0.0135, -0.0720, 0.0042, -0.0340, -0.0047, 0.0034, 0.0184, 0.0044, 0.0202, 0.0042, 0.0325, 0.0243, -0.0061, -0.0224, -0.0086, -0.0020]\n",
            "Token 4( ...): [-0.0110, 0.0258, 0.0208, -0.0197, -0.0239, 0.0069, -0.0082, -0.0236, 0.0030, -0.0125, 0.0068, -0.0439, -0.0025, -0.0640, 0.0289, 0.0301, -0.0075, 0.0150, -0.0603, -0.0460, 0.0107, 0.0598, 0.0325, 0.0232, -0.0179, -0.0056, -0.0363, -0.0370, 0.0182, -0.0273, 0.0016, -0.0520, 0.0346, 0.0400, 0.0340, -0.0044, 0.0277, 0.0021, 0.0256, 0.0215, 0.0206, -0.0044, -0.0437, -0.0063, -0.0307, -0.0265, -0.0008, 0.0076, 0.0467, -0.0207, -0.0273, -0.0317, 0.0347, -0.0201, -0.0257, -0.0143, -0.0152, -0.0037, -0.0287, 0.0271, -0.0528, -0.0197, -0.0401, 0.0198, 0.0193, 0.0158, 0.0285, -0.0219, -0.0206, 0.0085, 0.0280, 0.0536, -0.0189, 0.0282, 0.0571, -0.0000, -0.0037, 0.0333, 0.0085, 0.0126, 0.0438, 0.0341, 0.0307, 0.0127, 0.0605, 0.0138, -0.0272, 0.0161, -0.0155, -0.0454, -0.0109, -0.0209, 0.0417, -0.0425, -0.0315, 0.0137, 0.0208, -0.0131, 0.0299, 0.0090, -0.0023, -0.0248, 0.0330, 0.0428, 0.0339, 0.0315, -0.0387, 0.0137, 0.0280, 0.0438, -0.0115, -0.0188, 0.0126, -0.0782, 0.0059, -0.0362, -0.0051, -0.0018, 0.0236, 0.0054, 0.0291, 0.0100, 0.0361, 0.0338, -0.0068, -0.0198, -0.0092, -0.0009]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora concatenamos todos los heads de vuelta para volver a nuestra matriz (5, 2048). Y además hacemos otra trnasformación lineal (o_proj (2048 → 2048)) para mezclar la información de todos los heads."
      ],
      "metadata": {
        "id": "CuGj-8SaKQyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paso 1: Transpose - volver a poner tokens primero\n",
        "attention_output_transposed = attention_output.transpose(0, 1)  # (16, 5, 128) → (5, 16, 128)\n",
        "\n",
        "print(f\"Después del transpose: {attention_output_transposed.shape}\")\n",
        "\n",
        "# Paso 2: Reshape/Concatenar - unir los 16 heads de 128 → 2048\n",
        "attention_output_concat = attention_output_transposed.reshape(5, 2048)\n",
        "\n",
        "print(f\"Después de concatenar: {attention_output_concat.shape}  # (5, 2048)\")\n",
        "\n",
        "\n",
        "for token_idx in range(5):\n",
        "    token_vec = attention_output_concat[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Us0x-K-UJVQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a3ed574-8c5c-41ef-bcc2-f6a7c98d182d"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después del transpose: torch.Size([5, 16, 128])\n",
            "Después de concatenar: torch.Size([5, 2048])  # (5, 2048)\n",
            "Token 0 ( <s>): [0.0145, -0.0015, 0.1039, 0.0482, -0.0125, ..., 0.0558, -0.0058, -0.0225, -0.0111, -0.0018]  (2048 dims)\n",
            "Token 1 ( açò): [0.0142, -0.0011, 0.1035, 0.0477, -0.0123, ..., 0.0443, -0.0060, -0.0240, -0.0088, -0.0050]  (2048 dims)\n",
            "Token 2 (  es): [0.0075, 0.0072, 0.0931, 0.0378, -0.0060, ..., 0.0302, -0.0064, -0.0227, -0.0093, -0.0039]  (2048 dims)\n",
            "Token 3 (  or): [0.0131, 0.0029, 0.0935, 0.0424, -0.0084, ..., 0.0243, -0.0061, -0.0224, -0.0086, -0.0020]  (2048 dims)\n",
            "Token 4 ( ...): [0.0101, 0.0089, 0.0834, 0.0348, -0.0041, ..., 0.0338, -0.0068, -0.0198, -0.0092, -0.0009]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pesos de la operación o_proj (proyección de salida)"
      ],
      "metadata": {
        "id": "YF_UBAwrL7-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los pesos de o_proj\n",
        "W_o = model.model.layers[0].self_attn.o_proj.weight\n",
        "\n",
        "print(f\"Matriz de pesos W_o: {W_o.shape}  # (2048, 2048)\")\n",
        "print(f\"Shape de los pesos W_o: {W_o.shape}\")  # (2048, 2048)\n",
        "for i in range(5):\n",
        "    row = W_o[i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_o[-5+i]  # Fila i con 2048 valores\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  (2048 dims)\")\n",
        "print(\"\\n\",\"(2048 d) \"*11)\n",
        "\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "id": "2zVvqO9pNo-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab782181-46e3-461f-9ca8-3703178346d9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz de pesos W_o: torch.Size([2048, 2048])  # (2048, 2048)\n",
            "Shape de los pesos W_o: torch.Size([2048, 2048])\n",
            "[0.0012, 0.0014, -0.0007, -0.0010, 0.0011, ..., -0.0024, -0.0012, -0.0012, -0.0013, 0.0005]  (2048 dims)\n",
            "[0.0008, -0.0016, 0.0043, -0.0023, -0.0038, ..., -0.0037, 0.0012, 0.0015, 0.0006, -0.0003]  (2048 dims)\n",
            "[0.0007, -0.0027, -0.0009, -0.0022, 0.0019, ..., 0.0019, 0.0023, -0.0003, -0.0003, -0.0005]  (2048 dims)\n",
            "[0.0013, -0.0008, 0.0036, 0.0007, -0.0025, ..., -0.0006, -0.0008, -0.0015, -0.0016, -0.0009]  (2048 dims)\n",
            "[0.0009, -0.0025, -0.0022, 0.0001, -0.0022, ..., 0.0014, 0.0008, -0.0026, 0.0009, -0.0012]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0004, -0.0004, 0.0010, -0.0007, -0.0006, ..., 0.0009, 0.0031, -0.0010, 0.0004, 0.0016]  (2048 dims)\n",
            "[0.0006, -0.0015, 0.0029, -0.0005, 0.0012, ..., -0.0007, -0.0000, 0.0009, -0.0008, -0.0015]  (2048 dims)\n",
            "[-0.0010, -0.0024, 0.0042, -0.0010, 0.0010, ..., 0.0013, -0.0008, 0.0007, 0.0015, -0.0008]  (2048 dims)\n",
            "[-0.0013, 0.0008, -0.0028, 0.0007, 0.0008, ..., 0.0014, -0.0003, -0.0019, -0.0009, -0.0010]  (2048 dims)\n",
            "[0.0001, 0.0009, 0.0004, -0.0019, -0.0010, ..., 0.0021, -0.0006, 0.0006, -0.0004, -0.0007]  (2048 dims)\n",
            "\n",
            " (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) (2048 d) \n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hacemos la última transformación para tener la atención final."
      ],
      "metadata": {
        "id": "jnnwQgL8NpeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_final = attention_output_concat @ W_o.T\n",
        "\n",
        "print(f\"Después de o_proj: {attention_final.shape}  # (5, 2048)\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = attention_final[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "4AgyjtrJJVOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3436703d-e0ef-41de-d8a9-d6c660d4a614"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de o_proj: torch.Size([5, 2048])  # (5, 2048)\n",
            "Token 0 ( <s>): [-0.0077, -0.0031, -0.0027, 0.0026, 0.0045, ..., 0.0116, -0.0028, 0.0030, 0.0061, -0.0100]  (2048 dims)\n",
            "Token 1 ( açò): [-0.0063, -0.0025, -0.0040, 0.0027, 0.0059, ..., 0.0139, 0.0028, 0.0009, 0.0039, -0.0104]  (2048 dims)\n",
            "Token 2 (  es): [-0.0010, -0.0038, -0.0030, -0.0040, 0.0006, ..., 0.0069, -0.0047, -0.0037, 0.0027, -0.0016]  (2048 dims)\n",
            "Token 3 (  or): [-0.0013, -0.0022, -0.0019, -0.0015, 0.0011, ..., 0.0047, -0.0027, -0.0022, 0.0012, -0.0022]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0033, 0.0013, -0.0017, -0.0023, -0.0005, ..., 0.0031, -0.0041, -0.0023, 0.0023, 0.0007]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto hemos completado la primera etapa del plan en 5 puntos para el bloque de atención:\n",
        "Siguiendo el plan, lo siguiente es otr normalización en la capa:\\\n",
        "✓ Atención\\\n",
        "→ Residual connection\\\n",
        "→ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "4Am8-CSbUxAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Residual connection (sumar con la entrada original antes de normalizar). Esto ayuda al flujo de gradientes durante el entrenamiento (evita vanishing gradients) y permite entrenar redes muy profundas."
      ],
      "metadata": {
        "id": "NmoiZVEIOecS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual connection\n",
        "hidden_states = normalized[0] + attention_final\n",
        "\n",
        "print(f\"Después de residual connection: {hidden_states.shape}\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = hidden_states[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "dlQiYGEjJVLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e25aeac-bf44-4a30-d7a8-61542501568a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de residual connection: torch.Size([5, 2048])\n",
            "Token 0 ( <s>): [-0.0154, -0.0082, -0.0190, -0.0304, -0.0047, ..., -0.0151, 0.0109, -0.0555, 0.0108, -0.0262]  (2048 dims)\n",
            "Token 1 ( açò): [-0.1924, -0.0908, -0.0451, 0.2988, 0.5665, ..., -0.2851, -0.2904, -0.0315, -0.1305, 0.1600]  (2048 dims)\n",
            "Token 2 (  es): [-0.1281, 0.1214, 0.5040, 0.1014, 0.1807, ..., -0.0882, 0.4653, 0.3474, -0.3646, -0.3552]  (2048 dims)\n",
            "Token 3 (  or): [-0.2119, 0.1852, -0.3783, 0.0389, 0.0998, ..., 0.3018, 0.1228, 0.2199, 0.0525, 0.0938]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0361, 0.0432, 0.0609, 0.2337, 0.2819, ..., -0.7602, 0.1177, -0.0850, 0.0645, 0.4271]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiendo el plan, lo siguiente es otr normalización en la capa:\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "→ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "UC3gNePWUh8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# post_attention_layernorm\n",
        "residual = hidden_states  # Guardamos para la próxima residual (celda 67)\n",
        "hidden_states_normalized = model.model.layers[0].post_attention_layernorm(hidden_states)\n",
        "\n",
        "print(f\"Después de post_attention_layernorm: {hidden_states_normalized.shape}\")\n",
        "for token_idx in range(5):\n",
        "    token_vec = hidden_states[token_idx, :]  # (2048,)\n",
        "\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[:5]])\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in token_vec[-5:]])\n",
        "\n",
        "    print(f\"Token {token_idx} ({tokens_names[token_idx]:>4}): [{first_vals}, ..., {last_vals}]  (2048 dims)\")"
      ],
      "metadata": {
        "id": "TLHV8Q_7UhOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd28328-e78d-46d6-f7c1-810c158f7ddd"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Después de post_attention_layernorm: torch.Size([5, 2048])\n",
            "Token 0 ( <s>): [-0.0154, -0.0082, -0.0190, -0.0304, -0.0047, ..., -0.0151, 0.0109, -0.0555, 0.0108, -0.0262]  (2048 dims)\n",
            "Token 1 ( açò): [-0.1924, -0.0908, -0.0451, 0.2988, 0.5665, ..., -0.2851, -0.2904, -0.0315, -0.1305, 0.1600]  (2048 dims)\n",
            "Token 2 (  es): [-0.1281, 0.1214, 0.5040, 0.1014, 0.1807, ..., -0.0882, 0.4653, 0.3474, -0.3646, -0.3552]  (2048 dims)\n",
            "Token 3 (  or): [-0.2119, 0.1852, -0.3783, 0.0389, 0.0998, ..., 0.3018, 0.1228, 0.2199, 0.0525, 0.0938]  (2048 dims)\n",
            "Token 4 ( ...): [-0.0361, 0.0432, 0.0609, 0.2337, 0.2819, ..., -0.7602, 0.1177, -0.0850, 0.0645, 0.4271]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "02SiVxlDOwqY"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamoms ahora a una capa Multilayer Perceptron(MLP):\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "✓ post_attention_layernorm (RMSNorm)\\\n",
        "→ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "tHHNZKAhV-J5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto es una combinación de un aumento de la dimensión y una reducción, en este caso subimos a 5440. La elección de este número es un poco técnica pero la explico para no dejar cabos sueltos. Tiene que ver con optimización de hardware y conocimiento ya adquirido de entrenar otras estructuras.\n",
        "Este MLP usa la activación SwiGLU, esto hace que tengamos 3 matrices lineales aprendidas (gate_proj, up_proj, down_proj) en lugar de las dos tradicionales.\n",
        "Se intenta entonces mantener el mismo número de parámetros que se haría con dos capas (que es hacer un x4 en las dims), por tanto:\\\n",
        "\n",
        "MLP estándar (2 capas):  2 × (hidden × 4×hidden) = 8 × hidden²\\\n",
        "SwiGLU (3 capas):        3 × (hidden × intermediate) \\\n",
        "\n",
        "\n",
        "Para igualar FLOPs:\\\n",
        "3 × intermediate ≈ 8 × hidden\\\n",
        "intermediate ≈ (8/3) × hidden ≈ 2.67 × hidden\n",
        "\n",
        "\n",
        "Para nuestro modelo:\n",
        "hidden_size = 2048\\\n",
        "8/3 × 2048 ≈ 5461\\\n",
        "Redondeado a múltiplo cercano de potencia de 2: 5440 = 85 × 64"
      ],
      "metadata": {
        "id": "jtRst2V9XYAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a mostrar a continuación las tres matrices:\\\n",
        "gate_proj: (2048 → 5440)\\\n",
        "up_proj: (2048 → 5440)\\\n",
        "down_proj: (5440 → 2048)"
      ],
      "metadata": {
        "id": "ofezBwJ8Zmn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Por si no conoces esta activación te dejo la estructura aquí:\n",
        "# Input: x (2048 dims)\n",
        "#     ↓\n",
        "# gate_proj(x) → (5440 dims)  ← rama 1\n",
        "# up_proj(x)   → (5440 dims)  ← rama 2\n",
        "#     ↓\n",
        "# Combinar: Swish(gate_proj(x)) ⊙ up_proj(x)  ← multiplicación elemento a elemento\n",
        "#     ↓\n",
        "# down_proj → (2048 dims)\n",
        "# ```\n",
        "\n",
        "# **Fórmula:**\n",
        "# ```\n",
        "# output = down_proj( Swish(gate_proj(x)) ⊙ up_proj(x) )"
      ],
      "metadata": {
        "id": "Sz-yzOEhaUHp"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entender gate_proj. Vemos la matriz de pesos W_proj y multiplicamos pot la salida de la normalized layer:"
      ],
      "metadata": {
        "id": "S5DPTgCcanPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_gate = model.model.layers[0].mlp.gate_proj.weight  # (5440, 2048)\n",
        "\n",
        "\n",
        "print(f\"\\nW_gate (gate_proj): {W_gate.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_gate[i]\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_gate.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_gate[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_gate.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_gate.shape[0]} dims) \"*11)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k5M1SSPxaett",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d634c881-82ca-417b-eb47-dcd3dbb7a92d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_gate (gate_proj): torch.Size([5440, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0063, 0.0026, -0.0066, -0.0099, -0.0037, ..., 0.0129, 0.0176, -0.0011, 0.0125, 0.0138]  (2048 dims)\n",
            "[0.0028, 0.0225, 0.0190, 0.0093, 0.0201, ..., -0.0320, 0.0240, -0.0173, -0.0046, 0.0269]  (2048 dims)\n",
            "[-0.0094, -0.0106, -0.0076, -0.0159, 0.0009, ..., -0.0025, 0.0008, -0.0161, 0.0039, -0.0182]  (2048 dims)\n",
            "[0.0092, -0.0143, 0.0014, -0.0058, 0.0054, ..., 0.0057, 0.0374, 0.0008, -0.0106, 0.0025]  (2048 dims)\n",
            "[-0.0273, -0.0114, -0.0006, 0.0206, 0.0198, ..., -0.0175, 0.0070, 0.0041, 0.0091, -0.0037]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0070, 0.0062, 0.0032, 0.0325, 0.0063, ..., 0.0020, 0.0121, 0.0074, -0.0342, 0.0081]  (2048 dims)\n",
            "[0.0034, 0.0271, -0.0147, 0.0008, -0.0264, ..., 0.0096, -0.0101, 0.0068, -0.0051, -0.0123]  (2048 dims)\n",
            "[0.0017, -0.0031, -0.0096, 0.0013, 0.0049, ..., -0.0179, 0.0154, -0.0045, 0.0156, -0.0029]  (2048 dims)\n",
            "[0.0276, -0.0107, 0.0160, -0.0171, 0.0325, ..., -0.0156, -0.0010, -0.0188, -0.0013, 0.0083]  (2048 dims)\n",
            "[-0.0149, -0.0161, -0.0122, -0.0052, -0.0116, ..., 0.0209, -0.0244, 0.0132, 0.0134, -0.0036]  (2048 dims)\n",
            "\n",
            " (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos `hidden_states_normalized` (la salida del post_attention_layernorm) por W_gate:\n"
      ],
      "metadata": {
        "id": "RAdCyZAjHAFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gate_output = hidden_states_normalized @ W_gate.T  # (5, 2048) @ (2048, 5440) = (5, 5440)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {gate_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = gate_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({gate_output.shape[1]} dims)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea6ngsqsHDZ_",
        "outputId": "353120e1-22f9-424f-e966-60c2580804f0"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.2301, -0.0203, -0.1844, -0.1309, 0.0579, ..., 0.0858, 0.0993, 0.0085, 0.0286, -0.0227]  (5440 dims)\n",
            "[-0.1139, 0.2137, -0.0860, 0.1257, -0.0316, ..., -0.0032, -0.0201, -0.1213, 0.1930, 0.0240]  (5440 dims)\n",
            "[-0.4383, 0.0238, -0.4579, -0.2830, 0.1297, ..., 0.0193, 0.0685, -0.2809, -0.1731, -0.3664]  (5440 dims)\n",
            "[-0.1594, -0.2865, -0.3713, -0.5836, -0.0132, ..., -0.0055, -0.0447, -0.2622, -0.3264, -0.1687]  (5440 dims)\n",
            "[-0.5516, -0.4088, -0.5492, -0.3127, -0.0123, ..., -0.1521, -0.1967, -0.4992, -0.1397, -0.3461]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo mismo con up_proj:"
      ],
      "metadata": {
        "id": "Q3SeaAZzIAGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_up = model.model.layers[0].mlp.up_proj.weight      # (5440, 2048)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {W_up.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_up[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_up.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_up[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_up.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_up.shape[0]} dims) \"*11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftYXIcUUFUAi",
        "outputId": "dadec4b9-bbed-4988-8233-d0cbbbd0659b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5440, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0232, 0.0148, 0.0086, 0.0011, 0.0030, ..., 0.0020, 0.0105, -0.0013, -0.0025, 0.0005]  (2048 dims)\n",
            "[0.0148, 0.0039, 0.0054, 0.0132, 0.0262, ..., 0.0110, 0.0024, -0.0229, 0.0049, -0.0170]  (2048 dims)\n",
            "[0.0113, -0.0081, -0.0240, 0.0106, -0.0188, ..., -0.0240, 0.0028, -0.0022, 0.0034, 0.0170]  (2048 dims)\n",
            "[0.0031, 0.0095, 0.0381, 0.0109, 0.0171, ..., 0.0097, 0.0276, 0.0023, 0.0041, -0.0010]  (2048 dims)\n",
            "[-0.0170, 0.0145, -0.0059, -0.0110, 0.0039, ..., 0.0023, -0.0135, -0.0214, 0.0107, -0.0214]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0051, 0.0002, -0.0075, 0.0070, 0.0297, ..., 0.0135, 0.0195, 0.0223, -0.0352, -0.0012]  (2048 dims)\n",
            "[-0.0073, -0.0028, 0.0122, 0.0074, 0.0155, ..., -0.0057, -0.0052, 0.0003, 0.0029, -0.0143]  (2048 dims)\n",
            "[-0.0072, -0.0215, -0.0074, 0.0048, 0.0058, ..., -0.0117, -0.0055, -0.0156, -0.0135, -0.0033]  (2048 dims)\n",
            "[0.0131, -0.0057, -0.0150, -0.0152, 0.0078, ..., 0.0064, -0.0057, -0.0030, 0.0167, 0.0065]  (2048 dims)\n",
            "[-0.0016, -0.0009, 0.0037, 0.0195, -0.0217, ..., 0.0156, -0.0009, 0.0066, 0.0010, 0.0114]  (2048 dims)\n",
            "\n",
            " (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) (5440 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiplicamos `hidden_states_normalized` (la salida del post_attention_layernorm) por W_up:"
      ],
      "metadata": {
        "id": "cEeDmIftHjD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "up_output = hidden_states_normalized @ W_up.T  # (5, 2048) @ (2048, 5440) = (5, 5440)\n",
        "\n",
        "print(f\"\\nW_up (up_proj): {up_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = up_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({up_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyJnyPF8HjiJ",
        "outputId": "c4b10437-607b-455f-b070-f6c48039e56c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_up (up_proj): torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[0.1364, 0.0267, -0.0092, -0.0883, 0.0097, ..., 0.0462, 0.0049, 0.1218, 0.0402, 0.0662]  (5440 dims)\n",
            "[0.1067, -0.0558, 0.0570, 0.0145, -0.0729, ..., 0.0251, 0.1501, -0.0627, -0.0286, 0.0424]  (5440 dims)\n",
            "[-0.0672, -0.0715, 0.0134, -0.1234, 0.0517, ..., 0.1935, -0.0726, 0.0223, 0.0219, -0.0975]  (5440 dims)\n",
            "[-0.0379, 0.0599, -0.1041, 0.0295, 0.2339, ..., -0.0847, -0.0018, 0.1348, -0.0553, -0.1734]  (5440 dims)\n",
            "[-0.1869, 0.0460, -0.0892, -0.0483, 0.2186, ..., 0.2333, 0.1010, 0.0931, 0.1295, 0.0160]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo mismo con down_proj:"
      ],
      "metadata": {
        "id": "9kHhLMkSIZGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_down = model.model.layers[0].mlp.down_proj.weight  # (2048, 5440)\n",
        "\n",
        "print(f\"\\nW_down (down_proj): {W_down.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_down[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_down.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_down[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_down.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_down.shape[0]} dims) \"*11)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttf46xatFYKL",
        "outputId": "cd17eab5-2dad-4164-c504-ee4e661829ca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "W_down (down_proj): torch.Size([2048, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.0015, 0.0040, -0.0049, 0.0017, 0.0132, ..., -0.0155, 0.0054, -0.0222, 0.0164, -0.0004]  (5440 dims)\n",
            "[0.0014, 0.0139, 0.0009, -0.0155, -0.0161, ..., -0.0167, 0.0065, -0.0045, 0.0020, 0.0063]  (5440 dims)\n",
            "[-0.0137, -0.0206, 0.0031, -0.0031, -0.0002, ..., -0.0089, -0.0005, -0.0096, 0.0023, 0.0277]  (5440 dims)\n",
            "[0.0050, -0.0073, -0.0092, 0.0161, 0.0065, ..., -0.0079, -0.0076, -0.0031, -0.0034, -0.0153]  (5440 dims)\n",
            "[0.0121, 0.0129, -0.0260, -0.0178, -0.0025, ..., -0.0320, 0.0132, 0.0035, 0.0077, -0.0117]  (5440 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[0.0089, -0.0063, 0.0014, -0.0131, -0.0269, ..., -0.0125, -0.0049, 0.0173, -0.0060, 0.0042]  (5440 dims)\n",
            "[0.0197, 0.0010, 0.0157, -0.0162, -0.0189, ..., 0.0214, -0.0049, -0.0090, -0.0078, -0.0042]  (5440 dims)\n",
            "[-0.0166, 0.0064, -0.0103, 0.0146, -0.0066, ..., 0.0162, -0.0039, 0.0270, 0.0017, -0.0081]  (5440 dims)\n",
            "[-0.0125, -0.0085, 0.0013, 0.0019, -0.0078, ..., -0.0204, 0.0259, 0.0010, -0.0176, 0.0004]  (5440 dims)\n",
            "[0.0018, -0.0126, 0.0271, 0.0208, 0.0014, ..., 0.0029, 0.0081, -0.0108, 0.0092, 0.0010]  (5440 dims)\n",
            "\n",
            " (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) (2048 dims) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para combinarlas hemos de hacer la activación SwiGLU:\n",
        "\n",
        "Dimensiones en cada paso:\\\n",
        "\n",
        "Input: (5, 2048)\\\n",
        "gate_proj: (5, 2048) @ (2048, 5440) → (5, 5440)\\\n",
        "up_proj: (5, 2048) @ (2048, 5440) → (5, 5440)\\\n",
        "SwiGLU: Swish + ⊙: (5, 5440) (no cambia dimensión)\\\n",
        "down_proj: (5, 5440) @ (5440, 2048) → (5, 2048)\\\n",
        "\n",
        "Recordamos que Swish es: Swish(x) = x * sigmoid(x)\n"
      ],
      "metadata": {
        "id": "DqkYDCEyI4Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hidden_states (5, 2048)\n",
        "#     ↓\n",
        "# ├→ × W_gate^T → gate_output (5, 5440)\n",
        "# │                    ↓\n",
        "# │               Swish(gate_output) (5, 5440)\n",
        "# │                    ↓\n",
        "# └→ × W_up^T   → up_output (5, 5440)\n",
        "#                      ↓\n",
        "#         Swish(gate_output) ⊙ up_output  (5, 5440)\n",
        "#         (multiplicación elemento a elemento)\n",
        "#                      ↓\n",
        "#               × W_down^T → output (5, 2048)"
      ],
      "metadata": {
        "id": "0EePnfDDJaJr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Opción 1: Manual\n",
        "swish_gate = gate_output * torch.sigmoid(gate_output)\n",
        "\n",
        "# Opción 2: Usando SiLU (que es exactamente Swish)\n",
        "# swish_gate = torch.nn.functional.silu(gate_output)\n",
        "\n",
        "print(f\"swish_gate shape: {swish_gate.shape}  # (5, 5440)\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = swish_gate[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({swish_gate.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY60VsHfKJzj",
        "outputId": "bf54dee8-ff6b-4c7d-ad2e-982cddf5c2c7"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swish_gate shape: torch.Size([5, 5440])  # (5, 5440)\n",
            "\n",
            "================================================================================\n",
            "[-0.1019, -0.0101, -0.0837, -0.0612, 0.0298, ..., 0.0448, 0.0521, 0.0043, 0.0145, -0.0112]  (5440 dims)\n",
            "[-0.0537, 0.1182, -0.0412, 0.0668, -0.0156, ..., -0.0016, -0.0099, -0.0570, 0.1058, 0.0121]  (5440 dims)\n",
            "[-0.1719, 0.0120, -0.1774, -0.1216, 0.0691, ..., 0.0097, 0.0354, -0.1209, -0.0791, -0.1500]  (5440 dims)\n",
            "[-0.0734, -0.1229, -0.1516, -0.2090, -0.0066, ..., -0.0027, -0.0218, -0.1140, -0.1368, -0.0772]  (5440 dims)\n",
            "[-0.2016, -0.1632, -0.2010, -0.1321, -0.0061, ..., -0.0703, -0.0887, -0.1886, -0.0650, -0.1434]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Multiplicar swish_gate ⊙ up_output (elemento a elemento)."
      ],
      "metadata": {
        "id": "FiV0Rg6HLAnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_intermediate = swish_gate * up_output  # (5, 5440)\n",
        "\n",
        "print(f\"swish_gate shape: {mlp_intermediate.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = mlp_intermediate[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({mlp_intermediate.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9vN3Np_Fo_h",
        "outputId": "909c0326-b1ca-4756-cd82-21a91539f780"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "swish_gate shape: torch.Size([5, 5440])\n",
            "\n",
            "================================================================================\n",
            "[-0.0139, -0.0003, 0.0008, 0.0054, 0.0003, ..., 0.0021, 0.0003, 0.0005, 0.0006, -0.0007]  (5440 dims)\n",
            "[-0.0057, -0.0066, -0.0023, 0.0010, 0.0011, ..., -0.0000, -0.0015, 0.0036, -0.0030, 0.0005]  (5440 dims)\n",
            "[0.0115, -0.0009, -0.0024, 0.0150, 0.0036, ..., 0.0019, -0.0026, -0.0027, -0.0017, 0.0146]  (5440 dims)\n",
            "[0.0028, -0.0074, 0.0158, -0.0062, -0.0015, ..., 0.0002, 0.0000, -0.0154, 0.0076, 0.0134]  (5440 dims)\n",
            "[0.0377, -0.0075, 0.0179, 0.0064, -0.0013, ..., -0.0164, -0.0090, -0.0175, -0.0084, -0.0023]  (5440 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Siguiente paso: Multiplicar por down_proj (transpuesta) para volver a 2048 dimensiones."
      ],
      "metadata": {
        "id": "haHcwjUFL6Kk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Proyección final del MLP\n",
        "mlp_output = mlp_intermediate @ W_down.T  # (5, 5440) @ (5440, 2048) = (5, 2048)\n",
        "\n",
        "print(f\"mlp_output shape: {mlp_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = mlp_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({mlp_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Goo91hSVFYG3",
        "outputId": "fe4f623a-7095-4a5c-89f1-a85fe2e0d35e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlp_output shape: torch.Size([5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0039, -0.0090, 0.0049, -0.0112, 0.0105, ..., 0.0098, 0.0073, 0.0010, 0.0006, -0.0037]  (2048 dims)\n",
            "[-0.0005, -0.0053, 0.0008, 0.0018, 0.0028, ..., 0.0037, -0.0050, -0.0073, 0.0027, -0.0076]  (2048 dims)\n",
            "[-0.0034, -0.0182, 0.0097, -0.0115, 0.0040, ..., 0.0182, -0.0286, 0.0155, 0.0082, 0.0119]  (2048 dims)\n",
            "[-0.0086, -0.0098, 0.0256, 0.0002, 0.0256, ..., 0.0057, -0.0241, -0.0103, -0.0293, -0.0040]  (2048 dims)\n",
            "[-0.0014, 0.0067, 0.0151, -0.0110, 0.0102, ..., -0.0004, -0.0201, 0.0140, -0.0120, -0.0187]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamoms ahora a una capa Multilayer Perceptron(MLP):\\\n",
        "✓ Atención\\\n",
        "✓ Residual connection\\\n",
        "✓ post_attention_layernorm (RMSNorm)\\\n",
        "✓ MLP\\\n",
        "→ Residual connection"
      ],
      "metadata": {
        "id": "G82zsI9-U_dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Añadimos ahora otra residual connection, y así damos tick al último elemento de la lista:"
      ],
      "metadata": {
        "id": "E9pV2qWmMSOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sumar con el residual guardado antes del MLP\n",
        "final_output = residual + mlp_output  # (5, 2048)\n",
        "\n",
        "print(f\"Salida final del decoder layer 0: {final_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = final_output[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({final_output.shape[1]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6WGt6w3FYEi",
        "outputId": "c3728d48-3905-4147-c7c9-1354166139cf"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida final del decoder layer 0: torch.Size([5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-0.0194, -0.0172, -0.0141, -0.0416, 0.0058, ..., -0.0054, 0.0182, -0.0545, 0.0113, -0.0299]  (2048 dims)\n",
            "[-0.1928, -0.0961, -0.0443, 0.3006, 0.5693, ..., -0.2814, -0.2954, -0.0389, -0.1278, 0.1524]  (2048 dims)\n",
            "[-0.1315, 0.1032, 0.5137, 0.0899, 0.1847, ..., -0.0700, 0.4367, 0.3629, -0.3564, -0.3433]  (2048 dims)\n",
            "[-0.2205, 0.1754, -0.3527, 0.0390, 0.1255, ..., 0.3075, 0.0988, 0.2096, 0.0233, 0.0898]  (2048 dims)\n",
            "[-0.0375, 0.0499, 0.0760, 0.2227, 0.2921, ..., -0.7607, 0.0975, -0.0710, 0.0525, 0.4084]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vale por fin hemos terminado con 1 bloque de attention ¡pero hay 23 más!..."
      ],
      "metadata": {
        "id": "Qoh49IgsP0Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Número de capas: {model.config.num_hidden_layers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHDBWdqyPqV8",
        "outputId": "87cba06a-54b7-49d4-b0ec-5fc188905940"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de capas: 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La salida de cada capa se convierte en la entrada de la siguiente. Cada capa refina y transforma la representación de los tokens.\n",
        "\n",
        "Las 24 capas procesan secuencialmente, manteniendo siempre la dimensión (5, 2048)."
      ],
      "metadata": {
        "id": "-rtThY4ONadd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"APILAMIENTO DE LAS 24 CAPAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Usar el modelo completo para obtener las salidas de todas las capas\n",
        "with torch.no_grad():\n",
        "    outputs = model(\n",
        "        input_ids=tokens,\n",
        "        output_hidden_states=True,  # Esto nos da la salida de cada capa\n",
        "        return_dict=True\n",
        "    )\n",
        "\n",
        "# outputs.hidden_states contiene:\n",
        "# [0] = embeddings\n",
        "# [1] = salida capa 0\n",
        "# [2] = salida capa 1\n",
        "# ...\n",
        "# [24] = salida capa 23\n",
        "\n",
        "print(f\"Número de hidden states (embeddings + 24 capas): {len(outputs.hidden_states)}\")\n",
        "\n",
        "# Mostrar evolución de cada capa\n",
        "for layer_idx in range(25):  # 0=embeddings, 1-24=capas\n",
        "    hidden = outputs.hidden_states[layer_idx]\n",
        "\n",
        "    if layer_idx == 0:\n",
        "        print(f\"\\nEmbeddings iniciales:\")\n",
        "    else:\n",
        "        print(f\"\\nCapa {layer_idx-1:2d} completada:\")\n",
        "\n",
        "    for i in range(5):\n",
        "        row = hidden[0,i]\n",
        "\n",
        "        # Primeros 5 valores\n",
        "        first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "        # Últimos 5 valores\n",
        "        last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "        print(f\"[{first_vals}, ..., {last_vals}]  ({hidden.shape[2]} dims)\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Shape final: {outputs.hidden_states[-1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PP3vo-pFYBe",
        "outputId": "b447d4b5-f89d-4f5b-ee0e-d21c15e1c23a"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "APILAMIENTO DE LAS 24 CAPAS\n",
            "================================================================================\n",
            "Número de hidden states (embeddings + 24 capas): 25\n",
            "\n",
            "Embeddings iniciales:\n",
            "[-0.0004, -0.0002, -0.0007, -0.0016, -0.0005, ..., -0.0009, 0.0008, -0.0027, 0.0003, -0.0008]  (2048 dims)\n",
            "[-0.0081, -0.0030, -0.0015, 0.0126, 0.0275, ..., -0.0092, -0.0146, -0.0013, -0.0075, 0.0078]  (2048 dims)\n",
            "[-0.0039, 0.0030, 0.0129, 0.0031, 0.0062, ..., -0.0020, 0.0165, 0.0102, -0.0143, -0.0113]  (2048 dims)\n",
            "[-0.0063, 0.0043, -0.0093, 0.0012, 0.0033, ..., 0.0062, 0.0043, 0.0063, 0.0019, 0.0030]  (2048 dims)\n",
            "[-0.0011, 0.0011, 0.0017, 0.0077, 0.0106, ..., -0.0179, 0.0047, -0.0026, 0.0027, 0.0149]  (2048 dims)\n",
            "\n",
            "Capa  0 completada:\n",
            "[0.0061, -0.0134, 0.0077, 0.0040, -0.0027, ..., 0.0107, -0.0186, 0.0021, 0.0059, -0.0128]  (2048 dims)\n",
            "[-0.0184, -0.0122, -0.0208, 0.0246, 0.0348, ..., 0.0054, -0.0165, 0.0060, -0.0049, -0.0173]  (2048 dims)\n",
            "[-0.0019, -0.0045, 0.0096, 0.0011, 0.0045, ..., 0.0052, 0.0139, 0.0134, -0.0110, -0.0154]  (2048 dims)\n",
            "[-0.0023, 0.0120, -0.0126, 0.0009, 0.0031, ..., 0.0065, 0.0018, 0.0031, 0.0043, 0.0011]  (2048 dims)\n",
            "[0.0021, 0.0041, 0.0005, 0.0012, 0.0077, ..., -0.0100, 0.0070, -0.0078, 0.0018, 0.0150]  (2048 dims)\n",
            "\n",
            "Capa  1 completada:\n",
            "[0.0080, -0.0083, -0.0598, -0.0063, -0.0015, ..., 0.0208, -0.0131, 0.0059, -0.0048, -0.0163]  (2048 dims)\n",
            "[0.0006, -0.0059, -0.0024, 0.0364, 0.0169, ..., 0.0193, 0.0002, 0.0152, -0.0031, -0.0176]  (2048 dims)\n",
            "[-0.0050, -0.0088, 0.0306, 0.0166, -0.0046, ..., 0.0239, 0.0409, 0.0256, -0.0221, -0.0294]  (2048 dims)\n",
            "[-0.0151, -0.0002, -0.0091, -0.0064, -0.0003, ..., 0.0067, 0.0228, -0.0028, 0.0030, 0.0107]  (2048 dims)\n",
            "[0.0054, 0.0044, 0.0151, -0.0013, 0.0087, ..., -0.0205, 0.0104, -0.0055, -0.0028, 0.0105]  (2048 dims)\n",
            "\n",
            "Capa  2 completada:\n",
            "[-0.0503, -0.0942, -0.2990, 0.0324, 0.1797, ..., 0.1450, -0.1362, -0.0488, -0.0426, 0.2972]  (2048 dims)\n",
            "[0.0028, 0.0053, -0.0369, 0.0404, 0.0148, ..., 0.0185, -0.0028, -0.0031, -0.0092, 0.0076]  (2048 dims)\n",
            "[-0.0016, -0.0066, 0.0321, 0.0127, -0.0069, ..., 0.0317, 0.0347, 0.0129, -0.0101, -0.0330]  (2048 dims)\n",
            "[-0.0219, 0.0020, -0.0351, -0.0084, -0.0122, ..., -0.0049, 0.0171, -0.0090, -0.0118, 0.0137]  (2048 dims)\n",
            "[0.0105, 0.0078, 0.0156, -0.0001, 0.0047, ..., -0.0228, 0.0164, -0.0197, -0.0032, -0.0006]  (2048 dims)\n",
            "\n",
            "Capa  3 completada:\n",
            "[-0.0510, -0.0951, -0.2940, 0.0412, 0.1759, ..., 0.1598, -0.1284, -0.0665, -0.0438, 0.2779]  (2048 dims)\n",
            "[0.0104, -0.0015, -0.0492, 0.0501, 0.0319, ..., 0.0166, -0.0260, 0.0107, -0.0134, 0.0031]  (2048 dims)\n",
            "[-0.0134, -0.0002, 0.0399, 0.0091, -0.0158, ..., 0.0176, 0.0343, 0.0386, -0.0102, -0.0270]  (2048 dims)\n",
            "[-0.0171, -0.0128, -0.0446, 0.0048, -0.0140, ..., -0.0157, 0.0258, -0.0149, 0.0045, 0.0087]  (2048 dims)\n",
            "[0.0136, -0.0055, 0.0188, 0.0015, -0.0109, ..., -0.0379, 0.0082, -0.0012, 0.0053, -0.0033]  (2048 dims)\n",
            "\n",
            "Capa  4 completada:\n",
            "[-0.0871, -0.0718, -0.2653, 0.2059, 0.1697, ..., 0.2432, -0.1823, -0.0043, -0.1436, 0.3555]  (2048 dims)\n",
            "[0.0069, 0.0176, -0.0524, 0.0525, 0.0280, ..., 0.0172, -0.0328, 0.0241, 0.0073, -0.0038]  (2048 dims)\n",
            "[-0.0166, 0.0106, 0.0144, 0.0160, -0.0229, ..., 0.0151, 0.0335, 0.0214, -0.0074, -0.0252]  (2048 dims)\n",
            "[-0.0185, -0.0130, -0.0438, 0.0146, -0.0122, ..., -0.0216, 0.0212, -0.0133, -0.0002, -0.0082]  (2048 dims)\n",
            "[-0.0101, 0.0096, 0.0012, 0.0201, -0.0301, ..., -0.0527, 0.0247, 0.0088, 0.0001, -0.0021]  (2048 dims)\n",
            "\n",
            "Capa  5 completada:\n",
            "[-0.0673, -0.0137, -0.2893, 0.2206, 0.1803, ..., 0.2666, -0.1903, 0.0131, -0.1543, 0.3765]  (2048 dims)\n",
            "[0.0095, -0.0018, -0.0796, 0.0367, 0.0425, ..., 0.0229, -0.0345, 0.0069, 0.0129, 0.0055]  (2048 dims)\n",
            "[-0.0108, 0.0282, 0.0437, 0.0074, -0.0209, ..., 0.0309, 0.0374, 0.0026, -0.0065, -0.0406]  (2048 dims)\n",
            "[-0.0253, -0.0205, -0.0055, 0.0159, 0.0138, ..., 0.0051, 0.0061, -0.0533, 0.0007, -0.0438]  (2048 dims)\n",
            "[-0.0441, -0.0256, 0.0500, 0.0276, -0.0088, ..., -0.0271, 0.0217, 0.0270, -0.0282, 0.0036]  (2048 dims)\n",
            "\n",
            "Capa  6 completada:\n",
            "[-0.0812, -0.0109, -0.3070, 0.2436, 0.2062, ..., 0.2559, -0.1881, 0.0263, -0.1697, 0.3873]  (2048 dims)\n",
            "[0.0378, 0.0057, -0.0681, 0.0406, 0.0361, ..., -0.0075, -0.0360, -0.0075, 0.0402, -0.0092]  (2048 dims)\n",
            "[-0.0101, 0.0243, 0.0492, 0.0595, -0.0046, ..., -0.0044, 0.0661, -0.0028, 0.0318, -0.0233]  (2048 dims)\n",
            "[-0.0056, -0.0206, -0.0070, -0.0047, 0.0008, ..., -0.0038, 0.0404, -0.0694, 0.0087, -0.0509]  (2048 dims)\n",
            "[-0.0500, 0.0123, 0.0392, -0.0421, 0.0030, ..., 0.0138, 0.0505, -0.0133, -0.0343, 0.0133]  (2048 dims)\n",
            "\n",
            "Capa  7 completada:\n",
            "[-0.0462, -0.0272, -0.2010, 0.2290, 0.2099, ..., 0.2054, -0.1714, 0.0008, -0.1601, 0.3828]  (2048 dims)\n",
            "[0.0167, 0.0029, -0.0728, 0.0660, 0.0501, ..., 0.0286, -0.0110, -0.0167, -0.0027, -0.0062]  (2048 dims)\n",
            "[0.0231, 0.0349, 0.0355, 0.0328, 0.0221, ..., -0.0137, 0.0726, -0.0193, 0.0191, -0.0041]  (2048 dims)\n",
            "[-0.0573, -0.0031, -0.0168, -0.0363, -0.0092, ..., -0.0050, 0.0449, -0.0732, -0.0016, -0.0784]  (2048 dims)\n",
            "[-0.0934, 0.0205, 0.0331, -0.1103, 0.0050, ..., 0.0804, -0.0034, -0.0176, -0.0527, 0.0111]  (2048 dims)\n",
            "\n",
            "Capa  8 completada:\n",
            "[-0.0453, -0.0306, -0.1601, 0.2141, 0.1713, ..., 0.1829, -0.1890, -0.0084, -0.1404, 0.3771]  (2048 dims)\n",
            "[-0.0081, -0.0099, -0.0761, 0.0494, 0.0227, ..., 0.0288, 0.0355, 0.0450, 0.0002, -0.0050]  (2048 dims)\n",
            "[0.0341, 0.0133, 0.0087, 0.0424, -0.0055, ..., -0.0033, 0.0847, 0.0378, 0.0355, 0.0218]  (2048 dims)\n",
            "[-0.0134, -0.0261, -0.0278, -0.0538, -0.0011, ..., 0.0281, 0.0372, -0.0616, 0.0089, -0.0550]  (2048 dims)\n",
            "[-0.0823, 0.0392, -0.0021, -0.0832, -0.0346, ..., 0.0679, -0.0144, 0.0073, -0.0377, 0.0090]  (2048 dims)\n",
            "\n",
            "Capa  9 completada:\n",
            "[0.0011, -0.0609, -0.1237, 0.2069, 0.1589, ..., 0.1948, -0.1965, -0.0130, -0.1161, 0.3894]  (2048 dims)\n",
            "[0.0153, -0.0162, -0.0854, 0.0417, 0.0358, ..., 0.0395, 0.0213, 0.0590, -0.0176, 0.0125]  (2048 dims)\n",
            "[0.0350, 0.0355, -0.0246, 0.0399, 0.0274, ..., -0.0189, 0.0667, 0.0355, 0.0022, 0.0082]  (2048 dims)\n",
            "[-0.0350, -0.0088, -0.0262, -0.0287, 0.0003, ..., 0.0458, 0.0550, -0.0780, -0.0007, -0.0399]  (2048 dims)\n",
            "[-0.0543, 0.0488, 0.0050, 0.0165, -0.0239, ..., 0.0818, -0.0528, -0.0700, -0.0364, -0.0137]  (2048 dims)\n",
            "\n",
            "Capa 10 completada:\n",
            "[-0.0011, -0.0519, -0.1007, 0.1981, 0.1603, ..., 0.1631, -0.1749, -0.0114, -0.0925, 0.3941]  (2048 dims)\n",
            "[0.0522, -0.0793, -0.1073, 0.0569, 0.0727, ..., 0.0095, 0.0366, 0.0167, -0.0221, 0.0033]  (2048 dims)\n",
            "[0.0426, 0.0009, -0.0617, 0.0518, 0.0730, ..., -0.0230, 0.0780, -0.0145, 0.0121, -0.0250]  (2048 dims)\n",
            "[-0.0351, -0.0368, -0.0406, 0.0114, 0.0526, ..., 0.0319, 0.0687, -0.1145, 0.0089, -0.0461]  (2048 dims)\n",
            "[-0.0720, 0.0583, 0.0403, 0.0393, 0.0001, ..., 0.0788, -0.0216, -0.0778, -0.0396, -0.0402]  (2048 dims)\n",
            "\n",
            "Capa 11 completada:\n",
            "[0.0022, -0.0411, -0.1030, 0.1885, 0.1484, ..., 0.1392, -0.1442, -0.0022, -0.0653, 0.4037]  (2048 dims)\n",
            "[0.0710, -0.0818, -0.1154, 0.0538, 0.0745, ..., -0.0074, 0.0050, -0.0044, -0.0260, -0.0156]  (2048 dims)\n",
            "[0.0726, 0.0220, -0.0609, 0.0398, 0.0526, ..., -0.0137, 0.0502, 0.0048, -0.0140, -0.0309]  (2048 dims)\n",
            "[-0.0137, -0.0105, -0.0585, -0.0072, 0.0272, ..., 0.0054, 0.0790, -0.1347, -0.0288, -0.0360]  (2048 dims)\n",
            "[-0.1376, 0.0280, 0.0541, 0.0430, 0.0080, ..., 0.0529, 0.0328, -0.1104, -0.0124, -0.0269]  (2048 dims)\n",
            "\n",
            "Capa 12 completada:\n",
            "[0.0097, -0.0098, -0.0636, 0.1444, 0.1026, ..., 0.1329, -0.1288, 0.0402, -0.0167, 0.3945]  (2048 dims)\n",
            "[0.0615, -0.0782, -0.0912, 0.0716, 0.0261, ..., -0.0116, 0.0330, 0.0257, -0.0618, -0.0370]  (2048 dims)\n",
            "[0.0539, 0.0164, -0.0189, 0.0162, 0.0088, ..., 0.0255, 0.0458, 0.0153, 0.0070, -0.0567]  (2048 dims)\n",
            "[0.0068, -0.0410, -0.0553, 0.0002, -0.0167, ..., 0.0463, 0.1167, -0.1185, -0.0078, -0.0572]  (2048 dims)\n",
            "[-0.1127, -0.0310, 0.0645, 0.0763, 0.0552, ..., 0.0599, 0.0852, -0.1056, -0.0681, -0.0016]  (2048 dims)\n",
            "\n",
            "Capa 13 completada:\n",
            "[0.0072, -0.0149, -0.0629, 0.1233, 0.0996, ..., 0.1322, -0.1242, 0.0684, -0.0124, 0.3958]  (2048 dims)\n",
            "[0.0635, -0.0553, -0.1304, 0.0678, 0.0653, ..., 0.0263, 0.0248, -0.0205, -0.0532, -0.0503]  (2048 dims)\n",
            "[0.0542, -0.0006, -0.0765, 0.0168, -0.0030, ..., 0.0268, 0.0746, -0.0063, 0.0104, -0.0630]  (2048 dims)\n",
            "[0.0126, -0.0652, -0.0203, -0.0127, -0.0224, ..., 0.0624, 0.0665, -0.1828, 0.0191, -0.0600]  (2048 dims)\n",
            "[-0.0575, -0.0227, 0.1040, 0.0808, 0.0451, ..., 0.0664, 0.1149, -0.0976, -0.0436, -0.0224]  (2048 dims)\n",
            "\n",
            "Capa 14 completada:\n",
            "[0.0160, -0.0111, -0.0505, 0.1153, 0.0980, ..., 0.1370, -0.1623, 0.1032, -0.0072, 0.3889]  (2048 dims)\n",
            "[0.0848, -0.0617, -0.1216, 0.1024, 0.0798, ..., 0.0182, 0.0789, -0.1109, -0.0640, -0.0504]  (2048 dims)\n",
            "[0.0396, 0.0336, -0.0840, 0.0524, 0.0134, ..., 0.0407, 0.1319, -0.0459, 0.0235, -0.0814]  (2048 dims)\n",
            "[-0.0472, -0.0187, -0.1156, -0.0087, -0.0277, ..., 0.0208, 0.0665, -0.3155, -0.0688, -0.1461]  (2048 dims)\n",
            "[-0.0114, -0.0204, 0.1165, 0.0441, 0.0487, ..., 0.0523, 0.1299, -0.0862, -0.0708, 0.0268]  (2048 dims)\n",
            "\n",
            "Capa 15 completada:\n",
            "[0.0606, 0.0399, -0.0828, 0.1002, 0.0552, ..., 0.1800, -0.1963, 0.0938, 0.0517, 0.3584]  (2048 dims)\n",
            "[0.0560, -0.0703, -0.1377, 0.0915, 0.0524, ..., 0.0164, 0.0742, -0.0831, -0.0710, 0.0127]  (2048 dims)\n",
            "[-0.0026, 0.0865, -0.1657, 0.0031, 0.0111, ..., 0.0935, 0.1854, 0.0272, 0.0611, -0.0948]  (2048 dims)\n",
            "[-0.0058, 0.0155, -0.1627, 0.0265, -0.0830, ..., 0.1075, 0.0679, -0.3015, -0.0951, -0.1244]  (2048 dims)\n",
            "[-0.1019, -0.0195, 0.0597, -0.0324, 0.0345, ..., 0.0743, 0.1103, -0.0001, -0.0777, -0.0129]  (2048 dims)\n",
            "\n",
            "Capa 16 completada:\n",
            "[0.0778, 0.0440, -0.0947, 0.0904, 0.0411, ..., 0.2016, -0.1886, 0.1133, 0.0932, 0.3416]  (2048 dims)\n",
            "[0.0386, -0.1048, -0.1434, 0.0679, 0.0334, ..., 0.0358, -0.0110, 0.0041, -0.0750, 0.0089]  (2048 dims)\n",
            "[-0.0275, 0.0706, -0.1718, 0.0111, -0.0104, ..., 0.0143, 0.2161, 0.0821, 0.0344, -0.1370]  (2048 dims)\n",
            "[0.0064, -0.0575, -0.1466, 0.0855, -0.0828, ..., 0.1047, 0.0328, -0.2946, -0.1338, -0.0321]  (2048 dims)\n",
            "[-0.0613, -0.0646, -0.0055, -0.0508, 0.0905, ..., 0.0358, 0.0913, 0.0122, -0.1272, 0.0347]  (2048 dims)\n",
            "\n",
            "Capa 17 completada:\n",
            "[0.0567, 0.0654, -0.0855, 0.0558, 0.0220, ..., 0.2130, -0.2015, 0.1323, 0.1083, 0.3621]  (2048 dims)\n",
            "[0.0413, -0.0901, -0.1593, 0.0448, 0.0689, ..., 0.0469, 0.0129, -0.0278, 0.0459, 0.0167]  (2048 dims)\n",
            "[-0.0392, 0.1248, -0.1760, 0.0597, 0.0355, ..., -0.0484, 0.3018, 0.0811, 0.0709, -0.1576]  (2048 dims)\n",
            "[0.0948, -0.0229, -0.1334, 0.0541, -0.0156, ..., 0.0489, 0.0159, -0.3553, -0.1496, -0.0190]  (2048 dims)\n",
            "[-0.0564, -0.0841, 0.0195, -0.0490, 0.1585, ..., 0.0700, 0.1028, -0.0020, -0.0766, 0.0563]  (2048 dims)\n",
            "\n",
            "Capa 18 completada:\n",
            "[0.0838, 0.0908, -0.1030, 0.0543, 0.0123, ..., 0.2252, -0.2244, 0.1165, 0.1193, 0.3705]  (2048 dims)\n",
            "[0.0948, -0.0855, -0.1890, -0.0017, 0.0286, ..., 0.0878, -0.0021, -0.0280, 0.0365, 0.0034]  (2048 dims)\n",
            "[0.0060, 0.0994, -0.2177, 0.0596, 0.0773, ..., -0.0340, 0.2875, 0.0197, 0.0722, -0.2175]  (2048 dims)\n",
            "[0.0118, -0.1045, -0.1325, 0.0698, -0.1072, ..., 0.0415, 0.0758, -0.3826, -0.1207, -0.0166]  (2048 dims)\n",
            "[-0.0198, -0.0568, -0.0390, -0.0500, 0.1325, ..., 0.0271, 0.0077, 0.0146, -0.0998, 0.0423]  (2048 dims)\n",
            "\n",
            "Capa 19 completada:\n",
            "[0.0886, 0.1037, -0.0838, 0.0390, 0.0123, ..., 0.2412, -0.2007, 0.1305, 0.1314, 0.3922]  (2048 dims)\n",
            "[0.1587, -0.0381, -0.2047, 0.0685, 0.0584, ..., 0.1121, 0.0200, -0.0288, 0.0170, 0.0256]  (2048 dims)\n",
            "[0.0480, 0.1764, -0.2791, 0.1301, 0.0767, ..., 0.1243, 0.4207, 0.0420, 0.0858, -0.3156]  (2048 dims)\n",
            "[-0.0357, -0.0724, -0.1461, 0.1223, -0.1265, ..., 0.1019, 0.0802, -0.2475, -0.1614, 0.0971]  (2048 dims)\n",
            "[0.0453, -0.0612, -0.0851, -0.1055, 0.1984, ..., 0.0796, -0.0408, -0.0037, -0.1432, 0.0424]  (2048 dims)\n",
            "\n",
            "Capa 20 completada:\n",
            "[0.0665, 0.1316, -0.0929, 0.0730, -0.0000, ..., 0.2776, -0.1847, 0.1415, 0.1510, 0.3898]  (2048 dims)\n",
            "[0.1399, -0.0108, -0.2143, 0.0496, 0.1082, ..., -0.0122, -0.0000, 0.0189, 0.0077, 0.0685]  (2048 dims)\n",
            "[-0.1018, 0.2152, -0.2880, 0.1107, 0.3133, ..., -0.0139, 0.4936, 0.0829, 0.1370, -0.3670]  (2048 dims)\n",
            "[0.0761, -0.0459, -0.0729, 0.0618, -0.0876, ..., 0.0122, 0.1240, -0.1662, -0.0518, 0.1404]  (2048 dims)\n",
            "[-0.0108, -0.0909, -0.0808, -0.2261, 0.2689, ..., 0.0471, 0.0222, -0.0204, -0.2124, -0.0236]  (2048 dims)\n",
            "\n",
            "Capa 21 completada:\n",
            "[0.0670, 0.1537, -0.1140, 0.0464, -0.0291, ..., 0.2745, -0.1954, 0.1596, 0.1353, 0.3859]  (2048 dims)\n",
            "[0.0880, -0.0325, -0.2440, 0.0433, 0.0660, ..., 0.0614, 0.1126, 0.1613, 0.0632, 0.0702]  (2048 dims)\n",
            "[-0.1179, 0.1774, -0.3481, 0.2139, 0.0670, ..., -0.0120, 0.9118, 0.0417, 0.0695, -0.4823]  (2048 dims)\n",
            "[0.0804, 0.0074, -0.0112, 0.0602, -0.0904, ..., -0.1117, 0.0461, -0.1517, 0.0411, 0.1542]  (2048 dims)\n",
            "[-0.1843, -0.1273, -0.1203, -0.2852, 0.2254, ..., 0.0091, 0.1873, -0.0453, -0.0738, -0.1713]  (2048 dims)\n",
            "\n",
            "Capa 22 completada:\n",
            "[-0.0869, 0.1795, 0.0161, -0.0475, 0.0100, ..., 0.1999, -0.1835, 0.3044, 0.2806, 0.4190]  (2048 dims)\n",
            "[0.2347, 0.0500, -0.4671, -0.0244, 0.0110, ..., 0.0119, 0.0758, 0.4019, 0.0425, 0.1599]  (2048 dims)\n",
            "[-0.1365, 0.3261, -0.3708, 0.1781, 0.0475, ..., -0.1386, 1.2014, 0.1080, 0.0285, -0.5826]  (2048 dims)\n",
            "[-0.0275, 0.0349, 0.1099, 0.0627, -0.1189, ..., -0.0727, -0.0790, -0.0424, -0.1420, 0.1890]  (2048 dims)\n",
            "[-0.2142, -0.1054, -0.3552, -0.4569, 0.2439, ..., -0.0674, 0.3793, -0.1261, 0.0174, -0.1954]  (2048 dims)\n",
            "\n",
            "Capa 23 completada:\n",
            "[-1.5909, -1.7375, -4.4629, -0.4475, -1.5282, ..., 1.0973, -2.6448, 0.6250, -0.4054, 0.6383]  (2048 dims)\n",
            "[1.6819, -0.9818, -3.6779, 0.2479, -1.1596, ..., 2.7471, -3.4384, 4.5534, 2.1092, -0.6639]  (2048 dims)\n",
            "[0.7666, 3.5930, -1.8386, 1.6267, -0.0512, ..., 1.3732, 2.2846, 0.4330, 1.0839, -3.3412]  (2048 dims)\n",
            "[0.7188, 2.1451, 1.3267, -0.2884, -1.1240, ..., 0.2166, -3.2900, 2.8703, -0.9791, 2.8026]  (2048 dims)\n",
            "[-1.5753, -3.9831, -3.6078, -1.3246, -2.2641, ..., 2.2254, -0.3861, -0.7341, -0.4248, -1.9969]  (2048 dims)\n",
            "\n",
            "================================================================================\n",
            "Shape final: torch.Size([1, 5, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez salimos de las 24 capas de atención nos quedan solamente unas pocas capas para llegar al final:\n",
        "-RMSNorm final: Normaliza la salida de la última capa decoder.\n",
        "-lm_head: Proyecta de 2048 dimensiones al vocabulario completo (256000). Cada posición ahora tiene un score para cada token posible."
      ],
      "metadata": {
        "id": "A6nYo95xQMXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last_layer_output = outputs.hidden_states[-1]\n",
        "# Paso 1: Final RMSNorm\n",
        "with torch.no_grad():\n",
        "    normalized_output = model.model.norm(last_layer_output)\n",
        "\n",
        "print(f\"\\nDespués de RMSNorm final: {normalized_output.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = normalized_output[0,i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({normalized_output.shape[2]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFzBGOzVNxbb",
        "outputId": "c76e4d29-a7a9-428f-c877-928c445b6e4b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de RMSNorm final: torch.Size([1, 5, 2048])\n",
            "\n",
            "================================================================================\n",
            "[-1.4676, -1.7436, -4.2174, -0.4108, -1.4097, ..., 1.1949, -2.1066, 0.5878, -0.3940, 0.6377]  (2048 dims)\n",
            "[1.5520, -0.9856, -3.4767, 0.2277, -1.0700, ..., 2.9925, -2.7396, 4.2838, 2.0507, -0.6635]  (2048 dims)\n",
            "[0.7146, 3.6436, -1.7558, 1.5091, -0.0477, ..., 1.5112, 1.8388, 0.4115, 1.0647, -3.3730]  (2048 dims)\n",
            "[0.6832, 2.2181, 1.2919, -0.2728, -1.0684, ..., 0.2430, -2.7002, 2.7816, -0.9807, 2.8849]  (2048 dims)\n",
            "[-1.4453, -3.9753, -3.3908, -1.2094, -2.0772, ..., 2.4102, -0.3059, -0.6866, -0.4106, -1.9840]  (2048 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pasamos ahora a 256000 dims para obtener los logits (valores asignados que corresponden 1 a 1 con los tokens del vocabulario). para ello tenemos la gigantesca matriz de pesos W_lm_head (256000, 2048)"
      ],
      "metadata": {
        "id": "ar2Rif01REp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W_lm_head = model.lm_head.weight  # (256000, 2048)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"MATRIZ LM_HEAD\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nW_lm_head shape: {W_lm_head.shape} \")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = W_lm_head[i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_lm_head.shape[1]} dims)\")\n",
        "print(\"[\", \"...     \"*12, \"]\")\n",
        "for i in range(5):\n",
        "    row = W_lm_head[-5+i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({W_lm_head.shape[1]} dims)\")\n",
        "print(\"\\n\",f\"({W_lm_head.shape[0]} d) \"*11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZbIkArnRlm7",
        "outputId": "92d03044-52dd-475a-8593-0d6e7a981ff1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "MATRIZ LM_HEAD\n",
            "================================================================================\n",
            "\n",
            "W_lm_head shape: torch.Size([256000, 2048]) \n",
            "\n",
            "================================================================================\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[0.0557, 0.0265, -0.0060, 0.0295, -0.0415, ..., 0.0098, 0.0605, 0.0481, -0.0206, 0.0449]  (2048 dims)\n",
            "[-0.0036, 0.0272, 0.0037, 0.0021, 0.0128, ..., -0.0069, -0.0334, -0.0135, 0.0273, -0.0228]  (2048 dims)\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[0.0172, 0.0022, 0.0003, 0.0131, -0.0266, ..., 0.0090, 0.0391, 0.0223, -0.0244, 0.0115]  (2048 dims)\n",
            "[ ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ]\n",
            "[-0.0007, 0.0010, 0.0060, 0.0011, 0.0112, ..., 0.0182, -0.0277, 0.0097, 0.0069, -0.0031]  (2048 dims)\n",
            "[0.0170, -0.0066, 0.0315, 0.0150, -0.0103, ..., 0.0146, 0.0405, -0.0033, -0.0137, 0.0142]  (2048 dims)\n",
            "[0.0211, 0.0054, 0.0060, -0.0106, -0.0054, ..., -0.0142, 0.0086, 0.0028, -0.0019, 0.0110]  (2048 dims)\n",
            "[0.0124, -0.0084, -0.0201, -0.0234, -0.0270, ..., -0.0132, -0.0018, 0.0110, -0.0072, 0.0114]  (2048 dims)\n",
            "[0.0032, -0.0082, 0.0248, -0.0067, 0.0065, ..., -0.0073, -0.0193, 0.0037, 0.0248, -0.0008]  (2048 dims)\n",
            "\n",
            " (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) (256000 d) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits = normalized_output @ W_lm_head.T  # (1, 5, 2048) @ (2048, 256000) = (1, 5, 256000)\n",
        "\n",
        "# # alternativa: lm_head - proyección al vocabulario\n",
        "# with torch.no_grad():\n",
        "#     logits = model.lm_head(normalized_output)\n",
        "\n",
        "print(f\"\\nDespués de lm_head: {logits.shape}\")\n",
        "print(f\"\\n{'='*80}\")\n",
        "for i in range(5):\n",
        "    row = logits[0,i]\n",
        "\n",
        "    # Primeros 5 valores\n",
        "    first_vals = \", \".join([f\"{v:.4f}\" for v in row[:5]])\n",
        "\n",
        "    # Últimos 5 valores\n",
        "    last_vals = \", \".join([f\"{v:.4f}\" for v in row[-5:]])\n",
        "\n",
        "    print(f\"[{first_vals}, ..., {last_vals}]  ({logits.shape[2]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjZxyRriQu2y",
        "outputId": "5422aff7-3b64-4485-d5d5-067b7cb9e223"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Después de lm_head: torch.Size([1, 5, 256000])\n",
            "\n",
            "================================================================================\n",
            "[-5.1194, -5.1734, 3.0396, -5.1196, -5.1196, ..., 5.0793, -1.8981, -0.2701, -0.4054, 1.1557]  (256000 dims)\n",
            "[-5.4080, -7.6178, 4.5850, -5.4066, -5.4083, ..., 2.2102, -3.9177, -1.2109, -0.0522, 1.6760]  (256000 dims)\n",
            "[-4.1653, -4.7880, 2.2707, -4.1642, -4.1657, ..., 2.8799, -2.3822, 1.6910, -0.4971, 2.6660]  (256000 dims)\n",
            "[-3.5495, -4.8614, 4.8744, -3.5508, -3.5503, ..., 4.7410, -2.1277, 0.0223, 4.7902, 1.4817]  (256000 dims)\n",
            "[-6.1523, -9.2047, 6.6189, -6.1515, -6.1524, ..., 4.2811, -2.5536, -0.5159, -0.7494, 3.2771]  (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estamos muy cerquita ya, para predecir el siguiente token usamos solamente la última fila de esta matriz que corresponde a la predicción para el último token de la secuencia ('...')"
      ],
      "metadata": {
        "id": "DR4aY--eTHf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PREDICCIÓN DEL SIGUIENTE TOKEN\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Para predecir el siguiente token, usamos el ÚLTIMO token de la secuencia (posición 4)\n",
        "last_token_logits = logits[0, -1, :]  # (256000,)\n",
        "\n",
        "print(f\"\\nLogits del último token: {last_token_logits.shape}\")\n",
        "\n",
        "# Primeros 5 valores\n",
        "first_vals = \", \".join([f\"{v:.4f}\" for v in last_token_logits[:5]])\n",
        "\n",
        "# Últimos 5 valores\n",
        "last_vals = \", \".join([f\"{v:.4f}\" for v in last_token_logits[-5:]])\n",
        "\n",
        "print(f\"[{first_vals}, ..., {last_vals}] ({last_token_logits.shape[0]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMnW-VQ6TG4-",
        "outputId": "05cda12f-250f-4247-d257-f2f943ff4e07"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PREDICCIÓN DEL SIGUIENTE TOKEN\n",
            "================================================================================\n",
            "\n",
            "Logits del último token: torch.Size([256000])\n",
            "[-6.1523, -9.2047, 6.6189, -6.1515, -6.1524, ..., 4.2811, -2.5536, -0.5159, -0.7494, 3.2771] (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto ya sacamos las probabilidades con una softmax.\n",
        "Hemos de decir que el modelo llega hasta aquí, al usar el modelo este nos devuelve los logits. Con eso nosotros ya aplicamos softmax y la estrategia de sampleado (elección del token) que querramos. Se usan los siguiente parámetros normalmente:"
      ],
      "metadata": {
        "id": "HjuMIwZPULM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output_tokens = model.generate(\n",
        "#     tokens,\n",
        "#     max_length=50,\n",
        "#     temperature=1.0,      # ← AQUÍ se especifica\n",
        "#     top_p=0.9,           # ← AQUÍ se especifica\n",
        "#     top_k=50,            # ← AQUÍ se especifica\n",
        "#     do_sample=True       # ← True = sampling, False = greedy\n",
        "# )"
      ],
      "metadata": {
        "id": "7rhhjsdWXCkY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explico estos valores al final del notebook como apéndice."
      ],
      "metadata": {
        "id": "kGBHLrNYX4U1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar softmax para obtener probabilidades\n",
        "probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "print(f\"\\nProbabilidades: {probabilities.shape}\")\n",
        "print(f\"Suma de probabilidades: {probabilities.sum():.6f}  # Debe ser 1.0\")\n",
        "# Primeros 5 valores\n",
        "first_vals = \", \".join([f\"{v:.4f}\" for v in probabilities[:5]])\n",
        "\n",
        "# Últimos 5 valores\n",
        "last_vals = \", \".join([f\"{v:.4f}\" for v in probabilities[-5:]])\n",
        "\n",
        "print(f\"[{first_vals}, ..., {last_vals}] ({probabilities.shape[0]} dims)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxRrtUJGQuz-",
        "outputId": "1b146489-4af0-4016-8c68-2f4f90f78f7e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Probabilidades: torch.Size([256000])\n",
            "Suma de probabilidades: 1.000019  # Debe ser 1.0\n",
            "[0.0000, 0.0000, 0.0004, 0.0000, 0.0000, ..., 0.0000, 0.0000, 0.0000, 0.0000, 0.0000] (256000 dims)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el token con mayor probabilidad (greedy decoding)\n",
        "predicted_token_id = torch.argmax(probabilities).item()\n",
        "predicted_probability = probabilities[predicted_token_id].item()\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"TOKEN PREDICHO:\")\n",
        "print(f\"  ID: {predicted_token_id}\")\n",
        "print(f\"  Texto: {tokenizer.decode([predicted_token_id])}\")\n",
        "print(f\"  Probabilidad: {predicted_probability:.4f} ({predicted_probability*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzqdbSXDQuw7",
        "outputId": "8a9656a5-c611-4803-d0bb-41d855a3b11c"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TOKEN PREDICHO:\n",
            "  ID: 594\n",
            "  Texto: es\n",
            "  Probabilidad: 0.0599 (5.99%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-5 predicciones\n",
        "top5_probs, top5_ids = torch.topk(probabilities, 5)\n",
        "print(f\"\\nTop 5 predicciones:\")\n",
        "for i in range(5):\n",
        "    token_id = top5_ids[i].item()\n",
        "    prob = top5_probs[i].item()\n",
        "    text = tokenizer.decode([token_id])\n",
        "    print(f\"  {i+1}. [{token_id}] '{text}' - {prob:.4f} ({prob*100:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mwojcbCQuul",
        "outputId": "dacd4698-34a0-4574-bfc4-a0056e60b19a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 5 predicciones:\n",
            "  1. [594] 'es' - 0.0599 (5.99%)\n",
            "  2. [503] 'se' - 0.0427 (4.27%)\n",
            "  3. [1161] 'és' - 0.0409 (4.09%)\n",
            "  4. [704] 'no' - 0.0340 (3.40%)\n",
            "  5. [394] 's' - 0.0315 (3.15%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡HEMOS COMPLETADO EL RECORRIDO COMPLETO! 🎉"
      ],
      "metadata": {
        "id": "LmxOlLHhVXGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input tokens\n",
        "#     ↓\n",
        "# Embeddings (256000 → 2048)\n",
        "#     ↓\n",
        "# ┌─ CAPA 0 (y se repite 24 veces) ────┐\n",
        "# │  input_layernorm (RMSNorm)         │\n",
        "# │  Multi-Head Attention              │\n",
        "# │  Residual connection               │\n",
        "# │  post_attention_layernorm (RMSNorm)│\n",
        "# │  MLP con SwiGLU                    │\n",
        "# │  Residual connection               │\n",
        "# └────────────────────────────────────┘\n",
        "#     ↓\n",
        "# ... (capas 1-23 igual) ...\n",
        "#     ↓\n",
        "# RMSNorm FINAL (después de las 24 capas)\n",
        "#     ↓\n",
        "# lm_head (2048 → 256000)\n",
        "#     ↓\n",
        "# Softmax\n",
        "#     ↓\n",
        "# Predicción"
      ],
      "metadata": {
        "id": "mQevSwrQQuri"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de irte recordamos que esto ha sido solo para predecir el siguiente de ..., pero para predecir el siguiente hay que volver a pasar por todo el modelo incluyendo el nuevo token predicho."
      ],
      "metadata": {
        "id": "803AKrgFaF16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input: [<s>, açò, es, or, ...]\n",
        "#        ↓ (TODO el modelo)\n",
        "# Output: predice \"es\" como siguiente token\n",
        "\n",
        "# Para seguir generando:\n",
        "# Paso 1: Input [<s>, açò, es, or, ...] → Predice token es\n",
        "# Paso 2: Input [<s>, açò, es, or, ..., es] → Volver a pasar por TODO el modelo → Predice siguiente token\n",
        "# Paso 3: Input [<s>, açò, es, or, ..., es, nuevo_token] → Volver a pasar por TODO → Predice siguiente\n",
        "# Y así sucesivamente..."
      ],
      "metadata": {
        "id": "3fIkK62uVkDW"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada vez:\n",
        "\n",
        "Se añade el token predicho al final de la secuencia\\\n",
        "Se pasa toda la secuencia por TODO el modelo de nuevo\n",
        "\n",
        "  Embeddings\\\n",
        "  24 capas completas (attention + MLP)\\\n",
        "  RMSNorm final\\\n",
        "  lm_head\\\n",
        "  Softmax\n",
        "\n",
        "\n",
        "Se toma el último logit para predecir el siguiente\n",
        "\n",
        "Por eso la generación de texto es costosa computacionalmente: cada token nuevo requiere un forward pass completo por las 24 capas.\n",
        "(Aquí es donde técnicas como KV-cache ayudan a optimizar, pero eso es otro tema)"
      ],
      "metadata": {
        "id": "I4h8C6e4aUKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## APÉNDICE"
      ],
      "metadata": {
        "id": "b_-qKvnNX-ZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación parámetros de model.generate():"
      ],
      "metadata": {
        "id": "Cb8dsZm3YG51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "do_sample (True/False):"
      ],
      "metadata": {
        "id": "OwBR215QYMzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do_sample=False  # Greedy decoding (siempre el token con mayor probabilidad)\n",
        "# do_sample=True   # Sampling probabilístico (introduce aleatoriedad)"
      ],
      "metadata": {
        "id": "NfwcMtRKYAXI"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "temperature (float, 0.1-2.0):"
      ],
      "metadata": {
        "id": "i7ZfHp2OYVRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# logits_ajustados = logits / temperature\n",
        "# probabilities = softmax(logits_ajustados)\n",
        "\n",
        "# temperature=0.5   # Más conservador, menos aleatorio\n",
        "# temperature=1.0   # Distribución original\n",
        "# temperature=1.5   # Más creativo, más aleatorio\n"
      ],
      "metadata": {
        "id": "JcUi-dD3YZV4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_k (int):"
      ],
      "metadata": {
        "id": "z2NJiB5YYm2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k=50  # Solo considera los 50 tokens más probables\n",
        "\n",
        "# Ordena tokens por probabilidad\n",
        "# Toma solo los top-k\n",
        "# Pone probabilidad 0 al resto\n",
        "# Renormaliza y samplea de esos k\n",
        "\n",
        "# Efecto: Evita tokens muy improbables/raros"
      ],
      "metadata": {
        "id": "TA8JA3rAYpkU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_p (float entre 0 y 1, típicamente 0.9-0.95):"
      ],
      "metadata": {
        "id": "Mu3rfGWvY1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_p=0.9  # Considera el conjunto mínimo de tokens que suman 90% de probabilidad\n",
        "\n",
        "# Ordena tokens por probabilidad descendente\n",
        "# Suma probabilidades acumuladas hasta llegar a p\n",
        "# Solo samplea de ese conjunto \"núcleo\"\n",
        "\n",
        "# Ejemplo:\n",
        "\n",
        "# Token A: 40%, Token B: 30%, Token C: 15%, Token D: 10%, otros: 5%\n",
        "# Con top_p=0.9 → considera A, B, C, D (suman 95% > 90%)\n",
        "# Con top_p=0.7 → considera solo A, B (suman 70%)\n",
        "\n",
        "# Ventaja sobre top_k: Es dinámico. A veces incluye 10 tokens, a veces 100, dependiendo de qué tan concentrada esté la distribución."
      ],
      "metadata": {
        "id": "UYin269RY33Z"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "y por último aunque no tiene que ver con el sampleado tenemos"
      ],
      "metadata": {
        "id": "UWyjZPXnZVtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "max_length o max_new_tokens, eos_token_id y min_length para decidir cuándo acabar."
      ],
      "metadata": {
        "id": "DR6P2Z_DZJcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=100        # Máxima longitud TOTAL (input + generado)\n",
        "max_new_tokens=50     # Máximo número de tokens NUEVOS a generar\n",
        "eos_token_id=None         # para cuando encuentra el token de fin (ej: </s>)\n",
        "min_length=25         # longitud mínima antes de poder parar"
      ],
      "metadata": {
        "id": "VJXU6czhZAoI"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E5rL0tXIQBt9"
      },
      "execution_count": 81,
      "outputs": []
    }
  ]
}