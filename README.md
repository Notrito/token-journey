# token-journey
Transformations level micro to the tokens throughout a transformer

# üîç An√°lisis Paso a Paso de la Arquitectura Aitana-2B-S

## üìã Descripci√≥n General

Este notebook realiza un **an√°lisis exhaustivo y detallado** de la arquitectura interna del modelo de lenguaje **Aitana-2B-S** (modelo de 2 mil millones de par√°metros desarrollado por GPLSI). El objetivo es **visualizar y comprender cada transformaci√≥n matricial** que sufre un texto desde que entra como tokens hasta que se genera la predicci√≥n del siguiente token. As√≠ podr√°s ver c√≥mo los tokens se van transformando en el nivel micro, n√∫mero a n√∫mero. 

### ¬øQu√© hace este notebook?

**A nivel macro**, el notebook:
- Carga el modelo Aitana-2B-S desde HuggingFace
- Toma una frase de entrada en valenciano
- **Sigue paso a paso** todas las transformaciones matem√°ticas que ocurren internamente
- Muestra las dimensiones de cada matriz en cada etapa
- Visualiza los valores num√©ricos (primeros y √∫ltimos elementos) de cada transformaci√≥n
- Culmina prediciendo el siguiente token con probabilidades

**A nivel micro**, desglosa:
1. **Tokenizaci√≥n**: C√≥mo se convierte el texto en IDs num√©ricos
2. **Embeddings**: Proyecci√≥n de tokens a espacio vectorial (256K vocabulario ‚Üí 2048 dimensiones)
3. **24 Capas Transformer**: Cada una con:
   - RMSNorm (normalizaci√≥n)
   - Multi-Head Attention (16 heads, 4 KV heads - Grouped Query Attention)
   - Residual connections
   - MLP con activaci√≥n SwiGLU
4. **Proyecci√≥n final**: De 2048 dimensiones ‚Üí 256K vocabulario (logits)
5. **Softmax**: Conversi√≥n de logits a probabilidades
6. **Predicci√≥n**: Selecci√≥n del siguiente token

---

## üèóÔ∏è Arquitectura del Modelo Aitana-2B-S

### Especificaciones T√©cnicas

| Par√°metro | Valor |
|-----------|-------|
| **Par√°metros totales** | ~2 mil millones |
| **Vocabulario** | 256,000 tokens |
| **Dimensi√≥n oculta** | 2048 |
| **N√∫mero de capas** | 24 |
| **Attention heads** | 16 |
| **KV heads** | 4 (Grouped Query Attention) |
| **Dimensi√≥n intermedia (MLP)** | 5440 |
| **Embeddings rotacionales** | RoPE (theta=10000) |
| **Normalizaci√≥n** | RMSNorm |
| **Activaci√≥n MLP** | SwiGLU |

# üöÄ C√≥mo Usar Este Notebook
El notebook ha sido creado en [Google Colab](https://colab.research.google.com/) y funciona bien siempre que tengas acceso a la TPU

### Personalizaci√≥n

**Cambiar texto de entrada:**
```python
text = "La teua frase en valenci√† ac√≠"
```

**Cambiar par√°metros de generaci√≥n:**
```python
generation = generator(
    input_text,
    do_sample=True,
    temperature=1.2,    # M√°s creativo
    top_k=50,
    top_p=0.95,
    max_new_tokens=100
)
```

---

## üìö Conceptos Explicados

### RMSNorm
### Grouped Query Attention (GQA)
### RoPE (Rotary Position Embeddings)
### SwiGLU
### Residual Connections

---

## üéØ Prop√≥sito Educativo

Este notebook es ideal para:
- ‚úÖ **Estudiantes** aprendiendo arquitecturas transformer
- ‚úÖ **Investigadores** que necesitan entender implementaciones espec√≠ficas
- ‚úÖ **Ingenieros ML** debuggeando modelos o implementando desde cero
- ‚úÖ **Curiosos** que quieren ver "dentro" de un LLM

**Lo que aprender√°s:**
1. C√≥mo fluyen los datos a trav√©s de un transformer
2. Dimensiones exactas de cada matriz
3. Por qu√© ciertos dise√±os (GQA, RoPE, SwiGLU) se eligen
4. D√≥nde est√° el costo computacional
5. C√≥mo se generan predicciones realmente

---

## ‚ö†Ô∏è Advertencias

1. **Requiere GPU con >8GB VRAM** para cargar el modelo completo
2. **No incluye optimizaciones** (KV-cache, quantization) - es did√°ctico
3. **Los valores mostrados** pueden cambiar ligeramente entre ejecuciones (inicializaci√≥n de weights)
4. **No es c√≥digo de producci√≥n** - prioriza claridad sobre eficiencia

---

## üìñ Referencias

- **Modelo:** [gplsi/Aitana-2B-S en HuggingFace](https://huggingface.co/gplsi/Aitana-2B-S)
- **Arquitectura base:** LLaMA 2 (Meta AI)
---

## üìù Licencia

Este notebook es material educativo. Respeta las licencias del modelo Aitana-2B-S y las bibliotecas utilizadas.

---
