# token-journey
Transformations level micro to the tokens throughout a transformer

# ğŸ” AnÃ¡lisis Paso a Paso de la Arquitectura Aitana-2B-S

## ğŸ“‹ DescripciÃ³n General

Este notebook realiza un **anÃ¡lisis exhaustivo y detallado** de la arquitectura interna del modelo de lenguaje **Aitana-2B-S** (modelo de 2 mil millones de parÃ¡metros desarrollado por GPLSI). El objetivo es **visualizar y comprender cada transformaciÃ³n matricial** que sufre un texto desde que entra como tokens hasta que se genera la predicciÃ³n del siguiente token.

### Â¿QuÃ© hace este notebook?

**A nivel macro**, el notebook:
- Carga el modelo Aitana-2B-S desde HuggingFace
- Toma una frase de entrada en valenciano
- **Sigue paso a paso** todas las transformaciones matemÃ¡ticas que ocurren internamente
- Muestra las dimensiones de cada matriz en cada etapa
- Visualiza los valores numÃ©ricos (primeros y Ãºltimos elementos) de cada transformaciÃ³n
- Culmina prediciendo el siguiente token con probabilidades

**A nivel micro**, desglosa:
1. **TokenizaciÃ³n**: CÃ³mo se convierte el texto en IDs numÃ©ricos
2. **Embeddings**: ProyecciÃ³n de tokens a espacio vectorial (256K vocabulario â†’ 2048 dimensiones)
3. **24 Capas Transformer**: Cada una con:
   - RMSNorm (normalizaciÃ³n)
   - Multi-Head Attention (16 heads, 4 KV heads - Grouped Query Attention)
   - Residual connections
   - MLP con activaciÃ³n SwiGLU
4. **ProyecciÃ³n final**: De 2048 dimensiones â†’ 256K vocabulario (logits)
5. **Softmax**: ConversiÃ³n de logits a probabilidades
6. **PredicciÃ³n**: SelecciÃ³n del siguiente token

---

## ğŸ—ï¸ Arquitectura del Modelo Aitana-2B-S

### Especificaciones TÃ©cnicas

| ParÃ¡metro | Valor |
|-----------|-------|
| **ParÃ¡metros totales** | ~2 mil millones |
| **Vocabulario** | 256,000 tokens |
| **DimensiÃ³n oculta** | 2048 |
| **NÃºmero de capas** | 24 |
| **Attention heads** | 16 |
| **KV heads** | 4 (Grouped Query Attention) |
| **DimensiÃ³n intermedia (MLP)** | 5440 |
| **Embeddings rotacionales** | RoPE (theta=10000) |
| **NormalizaciÃ³n** | RMSNorm |
| **ActivaciÃ³n MLP** | SwiGLU |

# ğŸš€ CÃ³mo Usar Este Notebook
El notebook ha sido creado en [Google Colab](https://colab.research.google.com/) y funciona bien siempre que tengas acceso a la TPU

### PersonalizaciÃ³n

**Cambiar texto de entrada:**
```python
text = "Tu propia frase en valenciano o castellano"
```

**Cambiar parÃ¡metros de generaciÃ³n:**
```python
generation = generator(
    input_text,
    do_sample=True,
    temperature=1.2,    # MÃ¡s creativo
    top_k=50,
    top_p=0.95,
    max_new_tokens=100
)
```

---

## ğŸ“š Conceptos Explicados

### RMSNorm
### Grouped Query Attention (GQA)
### RoPE (Rotary Position Embeddings)
### SwiGLU
### Residual Connections

---

## ğŸ¯ PropÃ³sito Educativo

Este notebook es ideal para:
- âœ… **Estudiantes** aprendiendo arquitecturas transformer
- âœ… **Investigadores** que necesitan entender implementaciones especÃ­ficas
- âœ… **Ingenieros ML** debuggeando modelos o implementando desde cero
- âœ… **Curiosos** que quieren ver "dentro" de un LLM

**Lo que aprenderÃ¡s:**
1. CÃ³mo fluyen los datos a travÃ©s de un transformer
2. Dimensiones exactas de cada matriz
3. Por quÃ© ciertos diseÃ±os (GQA, RoPE, SwiGLU) se eligen
4. DÃ³nde estÃ¡ el costo computacional
5. CÃ³mo se generan predicciones realmente

---

## âš ï¸ Advertencias

1. **Requiere GPU con >8GB VRAM** para cargar el modelo completo
2. **No incluye optimizaciones** (KV-cache, quantization) - es didÃ¡ctico
3. **Los valores mostrados** pueden cambiar ligeramente entre ejecuciones (inicializaciÃ³n de weights)
4. **No es cÃ³digo de producciÃ³n** - prioriza claridad sobre eficiencia

---

## ğŸ“– Referencias

- **Modelo:** [gplsi/Aitana-2B-S en HuggingFace](https://huggingface.co/gplsi/Aitana-2B-S)
- **Arquitectura base:** LLaMA 2 (Meta AI)

---

## ğŸ“ Licencia

Este notebook es material educativo. Respeta las licencias del modelo Aitana-2B-S y las bibliotecas utilizadas.

---

## ğŸ‰ ConclusiÃ³n

Has completado un viaje desde **texto â†’ tokens â†’ embeddings â†’ 24 capas transformer â†’ logits â†’ predicciÃ³n**. Â¡Ahora entiendes cada multiplicaciÃ³n matricial que ocurre en un LLM moderno!

**Siguiente paso:** Implementa tu propio transformer desde cero con estos conocimientos. ğŸš€
